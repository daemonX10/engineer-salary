{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67611af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting max CPU count to: 20\n"
     ]
    }
   ],
   "source": [
    "# Fix for wmic error in Windows\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = str(os.cpu_count())\n",
    "print(f\"Setting max CPU count to: {os.environ['LOKY_MAX_CPU_COUNT']}\")\n",
    "\n",
    "# For older joblib versions, you might also need:\n",
    "os.environ[\"JOBLIB_TEMP_FOLDER\"] = os.path.join(os.path.expanduser(\"~\"), \"temp_joblib\")\n",
    "if not os.path.exists(os.environ[\"JOBLIB_TEMP_FOLDER\"]):\n",
    "    os.makedirs(os.environ[\"JOBLIB_TEMP_FOLDER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8feb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torch-2.7.0%2Bcu126-cp313-cp313-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\damod\\anaconda3\\envs\\nova\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\damod\\anaconda3\\envs\\nova\\lib\\site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\damod\\anaconda3\\envs\\nova\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\damod\\anaconda3\\envs\\nova\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu126/torch-2.7.0%2Bcu126-cp313-cp313-win_amd64.whl (2770.9 MB)\n",
      "   ---------------------------------------- 0.0/2.8 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.9 MB/s eta 0:09:30\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.9 MB/s eta 0:09:25\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.9 MB/s eta 0:09:26\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.9 MB/s eta 0:09:25\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.9 MB/s eta 0:09:23\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:41\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:35\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:35\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:34\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:38\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:31\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:30\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.9 MB/s eta 0:09:29\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.9 MB/s eta 0:09:28\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:30\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:29\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:29\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:33\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:34\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:33\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:32\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:32\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:30\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:29\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:32\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:28\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:29\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:28\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:28\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:29\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:27\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:26\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:26\n",
      "   ---------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:26\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:25\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:25\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:24\n",
      "    --------------------------------------- 0.0/2.8 GB 4.9 MB/s eta 0:09:24\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:25\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:25\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:24\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:24\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:24\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:23\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:23\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:22\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:22\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:23\n",
      "    --------------------------------------- 0.0/2.8 GB 4.8 MB/s eta 0:09:24\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:23\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:23\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:22\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:23\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:22\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:22\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:22\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:21\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:23\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:22\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:21\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:21\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:19\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:19\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:18\n",
      "    --------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:18\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:21\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:21\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:22\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:19\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:19\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:19\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:19\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:18\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:18\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:17\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:17\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:17\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:17\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:17\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:16\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:16\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:16\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:16\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:16\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:15\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:15\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:15\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:15\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:15\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:14\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:14\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:14\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:21\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:20\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:19\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:19\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:19\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:18\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:18\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:18\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:17\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:18\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:18\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:17\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:17\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:16\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:16\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:16\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:16\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:15\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:15\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:15\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:15\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.8 MB/s eta 0:09:15\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:26\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:25\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:25\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:25\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:25\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:24\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:24\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:24\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:23\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:23\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:22\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:22\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:22\n",
      "   - -------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:22\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.1/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:19\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:19\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:19\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:19\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:19\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:18\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:18\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:18\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:18\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:17\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:17\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:17\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:17\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.7 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:20\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:19\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:19\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:19\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:25\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:24\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:24\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:24\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:24\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:24\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:23\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:23\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:23\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:23\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:23\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:23\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:22\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:22\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:22\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:22\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:21\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:22\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.5 MB/s eta 0:09:25\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.5 MB/s eta 0:09:25\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.5 MB/s eta 0:09:25\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.5 MB/s eta 0:09:25\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:23\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:23\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.6 MB/s eta 0:09:23\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.5 MB/s eta 0:09:31\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.5 MB/s eta 0:09:31\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.5 MB/s eta 0:09:31\n",
      "   -- ------------------------------------- 0.2/2.8 GB 4.4 MB/s eta 0:09:41\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.4 MB/s eta 0:09:41\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.4 MB/s eta 0:09:41\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.4 MB/s eta 0:09:41\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:09:52\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:09:54\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:09:59\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:09:59\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:10:00\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:10:00\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:10:00\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:10:00\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:10:00\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:10:00\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:10:00\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.3 MB/s eta 0:10:00\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.1 MB/s eta 0:10:24\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.1 MB/s eta 0:10:23\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.1 MB/s eta 0:10:23\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:34\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:33\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:34\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:37\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:41\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:39\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:40\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:41\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:41\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:41\n",
      "   --- ------------------------------------ 0.2/2.8 GB 4.0 MB/s eta 0:10:41\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.9 MB/s eta 0:11:02\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.9 MB/s eta 0:11:02\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.8 MB/s eta 0:11:04\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.8 MB/s eta 0:11:11\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.8 MB/s eta 0:11:11\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.8 MB/s eta 0:11:11\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.8 MB/s eta 0:11:12\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.8 MB/s eta 0:11:12\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.8 MB/s eta 0:11:17\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.8 MB/s eta 0:11:16\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.8 MB/s eta 0:11:17\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.8 MB/s eta 0:11:18\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:19\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:24\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:29\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:30\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:30\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:31\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:30\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:20\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:19\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:22\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:20\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:21\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:20\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:19\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:19\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:19\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:18\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:18\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:17\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:18\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:17\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:17\n",
      "   --- ------------------------------------ 0.2/2.8 GB 3.7 MB/s eta 0:11:17\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:16\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:16\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:17\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:16\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:16\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:12\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:12\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:17\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:21\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:21\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:20\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:20\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:19\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:19\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:19\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:18\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:18\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:18\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:18\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:17\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:17\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:16\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:16\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:16\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:16\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:15\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:15\n",
      "   --- ------------------------------------ 0.3/2.8 GB 3.7 MB/s eta 0:11:15\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:10\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:09\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:09\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:09\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:07\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:06\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:06\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:05\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:05\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:04\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:04\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:04\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:04\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:03\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:03\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.7 MB/s eta 0:11:03\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:56\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:54\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:54\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:54\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:53\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:52\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:53\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:58\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:58\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:58\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:58\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:58\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:57\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:57\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:57\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:56\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:55\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:55\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:55\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:54\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:54\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:48\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:48\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:49\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:48\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:48\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:48\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.8 MB/s eta 0:10:38\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.9 MB/s eta 0:10:36\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.9 MB/s eta 0:10:26\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.9 MB/s eta 0:10:25\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.9 MB/s eta 0:10:23\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.9 MB/s eta 0:10:22\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 3.9 MB/s eta 0:10:22\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.0 MB/s eta 0:10:11\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.0 MB/s eta 0:10:10\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.0 MB/s eta 0:10:06\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.1 MB/s eta 0:10:02\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.1 MB/s eta 0:10:01\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.1 MB/s eta 0:10:01\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.1 MB/s eta 0:10:00\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.1 MB/s eta 0:09:59\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.1 MB/s eta 0:09:59\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.2 MB/s eta 0:09:38\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.2 MB/s eta 0:09:37\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.2 MB/s eta 0:09:36\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.2 MB/s eta 0:09:35\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.2 MB/s eta 0:09:34\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.2 MB/s eta 0:09:34\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.2 MB/s eta 0:09:33\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.3 MB/s eta 0:09:25\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.3 MB/s eta 0:09:24\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 4.3 MB/s eta 0:09:23\n",
      "   ----- ---------------------------------- 0.3/2.8 GB 4.3 MB/s eta 0:09:23\n",
      "   ----- ---------------------------------- 0.3/2.8 GB 4.3 MB/s eta 0:09:19\n",
      "   ----- ---------------------------------- 0.3/2.8 GB 4.4 MB/s eta 0:09:16\n",
      "   ----- ---------------------------------- 0.3/2.8 GB 4.4 MB/s eta 0:09:17\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:09:16\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:09:11\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:56\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:55\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:55\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:56\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:54\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:53\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:49\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:48\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:47\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:47\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:46\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:42\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:42\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:42\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:42\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:40\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.7 MB/s eta 0:08:37\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.7 MB/s eta 0:08:37\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.7 MB/s eta 0:08:37\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.7 MB/s eta 0:08:37\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.7 MB/s eta 0:08:37\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:47\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:45\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:43\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:44\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:41\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:42\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:41\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:42\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:42\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:42\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:41\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:41\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:41\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:41\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:40\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:40\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:40\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:44\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:44\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:45\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:45\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:44\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:44\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:44\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:41\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:41\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:38\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.6 MB/s eta 0:08:38\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:43\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:43\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:44\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:44\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:51\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:51\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:51\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:51\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:50\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:50\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:51\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:52\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:51\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:51\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:50\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:50\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:49\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:49\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:49\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:49\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:49\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:49\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:49\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.5 MB/s eta 0:08:49\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:04\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:04\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:03\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:03\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:04\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:04\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:03\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:03\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:02\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:02\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:02\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:02\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:02\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:01\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:01\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:02\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:00\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:08:56\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:08:56\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:08:56\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:08:55\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:08:55\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:08:55\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:08:54\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:08:54\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.4 MB/s eta 0:08:54\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:02\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:02\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:02\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:02\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:01\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:01\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:01\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:09:00\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:08:59\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:08:59\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:08:59\n",
      "   ------ --------------------------------- 0.4/2.8 GB 4.3 MB/s eta 0:08:59\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:58\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:58\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:58\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:58\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:58\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:58\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:57\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:57\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:56\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:56\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:56\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:56\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:56\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:55\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:55\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:54\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:55\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:54\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:54\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:54\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:58\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:08:59\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:09:00\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:09:00\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:09:01\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:09:01\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:09:01\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:09:01\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.3 MB/s eta 0:09:01\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:16\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:16\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:17\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:18\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:18\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:19\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:19\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:20\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:21\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:22\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:22\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:22\n",
      "   ------ --------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:24\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:25\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:25\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:17\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:16\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:16\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:18\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:19\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:20\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:21\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.1 MB/s eta 0:09:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:25\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:26\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:27\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:28\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:29\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:30\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:32\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:33\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:34\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:30\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:35\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:34\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:36\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:36\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:38\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:38\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:38\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:39\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:39\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:34\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:34\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:36\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:35\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:28\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:29\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:30\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:31\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:28\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:35\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:36\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:35\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:37\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:37\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:39\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:38\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:38\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:37\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:38\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:36\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:36\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:36\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:21\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:20\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:19\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:17\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:18\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:17\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:21\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:20\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:20\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:22\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:24\n",
      "   ------- -------------------------------- 0.5/2.8 GB 4.0 MB/s eta 0:09:25\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:26\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:25\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:25\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:25\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:25\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:24\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:24\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:24\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:24\n",
      "   ------- -------------------------------- 0.5/2.8 GB 3.9 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:23\n",
      "   ------- -------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:34\n",
      "   ------- -------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:34\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:35\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:36\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:35\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:35\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:34\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.9 MB/s eta 0:09:34\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:35\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:35\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:35\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:35\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:35\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:53\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:53\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:52\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:52\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:10:01\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:10:00\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:10:00\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:10:00\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:10:01\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:10:01\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:06\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:04\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:04\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:13\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:13\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:12\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:12\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.5 MB/s eta 0:10:23\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.5 MB/s eta 0:10:24\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.5 MB/s eta 0:10:19\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.5 MB/s eta 0:10:22\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.5 MB/s eta 0:10:20\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.5 MB/s eta 0:10:19\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.5 MB/s eta 0:10:19\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.5 MB/s eta 0:10:18\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:01\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:59\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:59\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:58\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:05\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:06\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:10\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:09\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:07\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:04\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:03\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:09:58\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:10:00\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:09:58\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:09:58\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:57\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:56\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:56\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:55\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:53\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:53\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:51\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:50\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:48\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:41\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:39\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:39\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:35\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:34\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:33\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:33\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:33\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:33\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.8 MB/s eta 0:09:33\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:48\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:48\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:48\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:46\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:45\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:47\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:46\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:46\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:44\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:44\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:44\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:44\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:44\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:09:57\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:09:56\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:09:53\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:09:52\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:09:52\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.6 MB/s eta 0:09:51\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:49\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:48\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:47\n",
      "   -------- ------------------------------- 0.6/2.8 GB 3.7 MB/s eta 0:09:43\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:42\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:42\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:40\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:38\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:36\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:35\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.8 MB/s eta 0:09:31\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.8 MB/s eta 0:09:29\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.8 MB/s eta 0:09:28\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.8 MB/s eta 0:09:25\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.8 MB/s eta 0:09:24\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.8 MB/s eta 0:09:22\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.8 MB/s eta 0:09:22\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.8 MB/s eta 0:09:21\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.8 MB/s eta 0:09:21\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.8 MB/s eta 0:09:21\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:35\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:36\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:37\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:37\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:37\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:37\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.7 MB/s eta 0:09:37\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.5 MB/s eta 0:10:01\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.5 MB/s eta 0:10:02\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.6 MB/s eta 0:09:58\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.6 MB/s eta 0:09:58\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.6 MB/s eta 0:09:58\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.6 MB/s eta 0:09:58\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.4 MB/s eta 0:10:18\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.5 MB/s eta 0:10:16\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.5 MB/s eta 0:10:13\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.5 MB/s eta 0:10:13\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.5 MB/s eta 0:10:13\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.5 MB/s eta 0:10:14\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.5 MB/s eta 0:10:13\n",
      "   --------- ------------------------------ 0.6/2.8 GB 3.5 MB/s eta 0:10:13\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:10:13\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:10:12\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:10:12\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.4 MB/s eta 0:10:15\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.4 MB/s eta 0:10:15\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.4 MB/s eta 0:10:14\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:57\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:55\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:54\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:52\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:51\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:53\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:51\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:53\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:54\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:55\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:57\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:57\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:55\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:55\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:55\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:55\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:55\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:09:55\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:10:00\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.5 MB/s eta 0:10:00\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:52\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:50\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:50\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:50\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:50\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:48\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:44\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:43\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:43\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:44\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:42\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:42\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:42\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.6 MB/s eta 0:09:33\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:32\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:30\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:29\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:25\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:26\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:24\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:23\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:22\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:23\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:22\n",
      "   --------- ------------------------------ 0.7/2.8 GB 3.7 MB/s eta 0:09:21\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.7 MB/s eta 0:09:15\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:13\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:09\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:07\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:07\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:07\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:06\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:07\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:06\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:06\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:06\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:06\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:06\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:04\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:04\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.8 MB/s eta 0:09:05\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:48\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:47\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:45\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:44\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:44\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:43\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:43\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:44\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:45\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:44\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:43\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:42\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 3.9 MB/s eta 0:08:40\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:25\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:24\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.1 MB/s eta 0:08:23\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.1 MB/s eta 0:08:22\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.1 MB/s eta 0:08:21\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.1 MB/s eta 0:08:21\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.1 MB/s eta 0:08:21\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.1 MB/s eta 0:08:21\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.1 MB/s eta 0:08:22\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.1 MB/s eta 0:08:22\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:22\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:23\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:23\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:23\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:23\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:22\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:27\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:27\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:26\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:26\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 4.0 MB/s eta 0:08:25\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.0 MB/s eta 0:08:25\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.0 MB/s eta 0:08:25\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.0 MB/s eta 0:08:24\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.0 MB/s eta 0:08:24\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.0 MB/s eta 0:08:24\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.0 MB/s eta 0:08:24\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.0 MB/s eta 0:08:24\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.0 MB/s eta 0:08:24\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:08:14\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:08:16\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:08:15\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:08:14\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:08:13\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:59\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:59\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:59\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:08:01\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:08:02\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:08:02\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:08:02\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:58\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:57\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:58\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:57\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:57\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:57\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:55\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:55\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:55\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:56\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:53\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:53\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:52\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:51\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:51\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:51\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:51\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:51\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:52\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:35\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:35\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:35\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:35\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:35\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.4 MB/s eta 0:07:31\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.4 MB/s eta 0:07:32\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:33\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:33\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:33\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:36\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:37\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:37\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:38\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:37\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:38\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:38\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:38\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:39\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:39\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.3 MB/s eta 0:07:40\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:40\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:40\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:41\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:41\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:42\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:42\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.2 MB/s eta 0:07:42\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:50\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:50\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:51\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:52\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:52\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:53\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:54\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:54\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:54\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:54\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:53\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:53\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:53\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:53\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:52\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:52\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:52\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:51\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:51\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:51\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:51\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:51\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:50\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:50\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:50\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:49\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:49\n",
      "   ------------ --------------------------- 0.8/2.8 GB 4.1 MB/s eta 0:07:47\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:45\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:44\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:47\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:46\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:46\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:50\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:50\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:50\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:49\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:49\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:49\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:48\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:48\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:47\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:46\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:46\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:45\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:44\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:43\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:43\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:43\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:40\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:39\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:39\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:38\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:38\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:38\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:38\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:38\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:37\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:37\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:37\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:36\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:36\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.1 MB/s eta 0:07:36\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.2 MB/s eta 0:07:33\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.2 MB/s eta 0:07:33\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.2 MB/s eta 0:07:33\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.2 MB/s eta 0:07:32\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.2 MB/s eta 0:07:32\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.2 MB/s eta 0:07:30\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.2 MB/s eta 0:07:29\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.2 MB/s eta 0:07:27\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.2 MB/s eta 0:07:26\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.2 MB/s eta 0:07:25\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:16\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:15\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:14\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:14\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:13\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:13\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:13\n",
      "   ------------ --------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:13\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:12\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:12\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:11\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:11\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:10\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:09\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:09\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:08\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:08\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:07\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:07\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:06\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:06\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:08\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:08\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:08\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:10\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:09\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:09\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:08\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.3 MB/s eta 0:07:07\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:06\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:05\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:04\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:03\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:03\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:03\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:02\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:02\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:01\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:01\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:00\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:07:00\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:06:59\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:06:58\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:06:58\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:06:57\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:06:56\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:06:56\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:06:56\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:06:55\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:06:54\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.4 MB/s eta 0:06:53\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.5 MB/s eta 0:06:50\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.5 MB/s eta 0:06:49\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.5 MB/s eta 0:06:49\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.5 MB/s eta 0:06:48\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.5 MB/s eta 0:06:47\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.5 MB/s eta 0:06:47\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.5 MB/s eta 0:06:47\n",
      "   ------------- -------------------------- 0.9/2.8 GB 4.5 MB/s eta 0:06:46\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.5 MB/s eta 0:06:45\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.5 MB/s eta 0:06:45\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.5 MB/s eta 0:06:44\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.5 MB/s eta 0:06:43\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.5 MB/s eta 0:06:42\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.5 MB/s eta 0:06:42\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.5 MB/s eta 0:06:41\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.5 MB/s eta 0:06:40\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.5 MB/s eta 0:06:39\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.6 MB/s eta 0:06:32\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.6 MB/s eta 0:06:32\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.6 MB/s eta 0:06:32\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.6 MB/s eta 0:06:31\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.6 MB/s eta 0:06:30\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.6 MB/s eta 0:06:30\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.6 MB/s eta 0:06:29\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:28\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:28\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:27\n",
      "   ------------- -------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:26\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:26\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:26\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:25\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:25\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:24\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:24\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:24\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:24\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:24\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:23\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:23\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:23\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:22\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:22\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:22\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:21\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:21\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:21\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:21\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:20\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:20\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:18\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:17\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.8 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.8 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:15\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:15\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:15\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:15\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:13\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:13\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:13\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:13\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:12\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:12\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:12\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:12\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:12\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:14\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:13\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:13\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:13\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:13\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:13\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:12\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:12\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:12\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:12\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:11\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:11\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:11\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:11\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:10\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:10\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:10\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:10\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:09\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:09\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:09\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:09\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:09\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:08\n",
      "   -------------- ------------------------- 1.0/2.8 GB 4.7 MB/s eta 0:06:08\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:07\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:07\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:06\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:06\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:06\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:06\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:05\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:05\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:06\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:06\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:09\n",
      "   --------------- ------------------------ 1.0/2.8 GB 4.7 MB/s eta 0:06:09\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:09\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:09\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:08\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:06\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:06\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:05\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:05\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:02\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:02\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:02\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:02\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:02\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:02\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:01\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:01\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:01\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:01\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:01\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:01\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:01\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:01\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:01\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:00\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:00\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:00\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:00\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:00\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:06:00\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:59\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:59\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:59\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:59\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:58\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:58\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:58\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:58\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:57\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:57\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:57\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:57\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:56\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:56\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:56\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:56\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:54\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:54\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:54\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:54\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:54\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:53\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:53\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:53\n",
      "   --------------- ------------------------ 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.7 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:53\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:54\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:53\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:53\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:50\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 4.6 MB/s eta 0:05:50\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:50\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:50\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:50\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:49\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:49\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:49\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:49\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:49\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:48\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:48\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:48\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:48\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:48\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:48\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.6 MB/s eta 0:05:48\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:57\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:56\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:55\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:55\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:55\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:54\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:54\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:54\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:55\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:54\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:53\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:53\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:53\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:53\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:53\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:51\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:51\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:51\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:50\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:51\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:51\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:51\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:51\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:51\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:50\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:50\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:50\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.5 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:52\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:55\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:55\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:55\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:55\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:57\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:58\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:57\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:57\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:57\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.4 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:59\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:59\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:59\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:59\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:58\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:58\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:58\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:57\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:58\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:58\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:59\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:57\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:57\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:55\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:56\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:54\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:53\n",
      "   ------------------ --------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:53\n",
      "   ------------------ --------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:53\n",
      "   ------------------ --------------------- 1.2/2.8 GB 4.3 MB/s eta 0:05:53\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:53\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:52\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:52\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:52\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:52\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:52\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:53\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:51\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:51\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:52\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:51\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:52\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:52\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:51\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:52\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:51\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:51\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:51\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:51\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:51\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:53\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:45\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:44\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:44\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:44\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:50\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:49\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:44\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:44\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.3 MB/s eta 0:05:44\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:45\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:45\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:45\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:45\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:46\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:46\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:47\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------ --------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.2 MB/s eta 0:05:48\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:54\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:54\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:54\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:54\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:54\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:54\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:54\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:50\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:51\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:51\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:50\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:50\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:51\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:52\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.0 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:50\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:50\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.1 MB/s eta 0:05:50\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.0 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.0 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.0 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.3/2.8 GB 4.0 MB/s eta 0:05:53\n",
      "   ------------------- -------------------- 1.4/2.8 GB 4.0 MB/s eta 0:06:00\n",
      "   ------------------- -------------------- 1.4/2.8 GB 4.0 MB/s eta 0:06:00\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:00\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:00\n",
      "   ------------------- -------------------- 1.4/2.8 GB 4.0 MB/s eta 0:05:59\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:05:59\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:05:59\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:04\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:04\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:03\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:03\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:03\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:02\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:01\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:01\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:01\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:00\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:01\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:02\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:01\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.9 MB/s eta 0:06:01\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:05\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:07\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:07\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:07\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:07\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:05\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:05\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:05\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:05\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   ------------------- -------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:07\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:08\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:09\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:07\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:07\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:05\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:04\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:05\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:05\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:06\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:07\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:07\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:11\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:11\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:11\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:09\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:08\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:08\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:08\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:01\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:01\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:02\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:01\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:00\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:01\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:02\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:02\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:02\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:02\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:02\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:05\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:04\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:04\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:03\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:03\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:01\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:01\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:00\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:00\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:59\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:59\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:59\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:59\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:06:00\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:59\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:59\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:59\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:01\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:01\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:01\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:06:01\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:56\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:55\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:55\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:54\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:54\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:55\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:56\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:56\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:58\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:56\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:56\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:56\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:55\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.8 MB/s eta 0:05:55\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:58\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:58\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:59\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:58\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:58\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:58\n",
      "   -------------------- ------------------- 1.4/2.8 GB 3.7 MB/s eta 0:05:58\n",
      "   -------------------- ------------------- 1.5/2.8 GB 3.7 MB/s eta 0:05:59\n",
      "   -------------------- ------------------- 1.5/2.8 GB 3.7 MB/s eta 0:05:59\n",
      "   -------------------- ------------------- 1.5/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   -------------------- ------------------- 1.5/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   -------------------- ------------------- 1.5/2.8 GB 3.7 MB/s eta 0:06:00\n",
      "   -------------------- ------------------- 1.5/2.8 GB 3.7 MB/s eta 0:06:00\n",
      "   -------------------- ------------------- 1.5/2.8 GB 3.7 MB/s eta 0:06:00\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.7 MB/s eta 0:05:59\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.7 MB/s eta 0:05:59\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.7 MB/s eta 0:05:58\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.7 MB/s eta 0:05:57\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.7 MB/s eta 0:05:53\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.7 MB/s eta 0:05:52\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.7 MB/s eta 0:05:51\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:46\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:45\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:44\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:43\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:42\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:41\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:40\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:40\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:40\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:40\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:39\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:40\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:40\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:39\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:39\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:39\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:39\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:39\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:38\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:37\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:36\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.8 MB/s eta 0:05:38\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:35\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:34\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:32\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:32\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:31\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:30\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:29\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:28\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:27\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:26\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:26\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:26\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:26\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:26\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:26\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:25\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:23\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:23\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:22\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:21\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:21\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:21\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:21\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:24\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:23\n",
      "   --------------------- ------------------ 1.5/2.8 GB 3.9 MB/s eta 0:05:22\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:21\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:20\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:19\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:19\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:18\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:17\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:16\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:16\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:15\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:15\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:15\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:13\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:12\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:11\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.1 MB/s eta 0:05:09\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:10\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.0 MB/s eta 0:05:09\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.1 MB/s eta 0:05:09\n",
      "   --------------------- ------------------ 1.5/2.8 GB 4.1 MB/s eta 0:05:08\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:08\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:07\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:06\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:06\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:05\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:05\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:05\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.0 MB/s eta 0:05:11\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.0 MB/s eta 0:05:10\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:06\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:06\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:06\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:05\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:05\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:05\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:04\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:04\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:04\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:04\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:04\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:03\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:03\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:02\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:01\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:00\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:05:00\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:04:58\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:04:58\n",
      "   ---------------------- ----------------- 1.5/2.8 GB 4.1 MB/s eta 0:04:57\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.1 MB/s eta 0:04:57\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.1 MB/s eta 0:04:56\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.1 MB/s eta 0:04:55\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.1 MB/s eta 0:04:55\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.1 MB/s eta 0:04:54\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.1 MB/s eta 0:04:54\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:53\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:52\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:51\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:48\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:47\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:47\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:46\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:46\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:46\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:45\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:44\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:44\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:43\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:43\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:43\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:45\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:45\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.2 MB/s eta 0:04:45\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:41\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:40\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:40\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:40\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:40\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:41\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:40\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:39\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:37\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:36\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:36\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:35\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:34\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:34\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:34\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:34\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:33\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:33\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:33\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:32\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:32\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:32\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:32\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:32\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:32\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:31\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:31\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:31\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:31\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:27\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:27\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:27\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:27\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:26\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:26\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:26\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:25\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:25\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:25\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:25\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:24\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:24\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:21\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:23\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:21\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:21\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:20\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:20\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:20\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:20\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:20\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:24\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:21\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:20\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:20\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:20\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:19\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:19\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:19\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.4 MB/s eta 0:04:21\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:21\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:21\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:21\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:21\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:22\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:21\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:20\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:19\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:19\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:19\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 4.3 MB/s eta 0:04:19\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:18\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:18\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:19\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:18\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:18\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:18\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:18\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:18\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:17\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:12\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:12\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:12\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:12\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:15\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:15\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:15\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:13\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:13\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:13\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:13\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:13\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:11\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:11\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:11\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:11\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:09\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:09\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:09\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:09\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:12\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:12\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:12\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:12\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:07\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:07\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:06\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:06\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:05\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:05\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:05\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:04\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:03\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:03\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:03\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.4 MB/s eta 0:04:03\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:07\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:07\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:07\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:07\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:09\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:09\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:09\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.3 MB/s eta 0:04:09\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:15\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:15\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:15\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:14\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:13\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:13\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:13\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:13\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:12\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:12\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:11\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:11\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:10\n",
      "   ------------------------ --------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:10\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:10\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:09\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:09\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:09\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:09\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:08\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:08\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:10\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:09\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:08\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:08\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.2 MB/s eta 0:04:08\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:14\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:13\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:13\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:12\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:12\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:12\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:12\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:10\n",
      "   ------------------------- -------------- 1.7/2.8 GB 4.1 MB/s eta 0:04:10\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.1 MB/s eta 0:04:09\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.1 MB/s eta 0:04:07\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.1 MB/s eta 0:04:07\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.1 MB/s eta 0:04:06\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:04\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:04\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:04\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:03\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:03\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:03\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:03\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:02\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:02\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:02\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:02\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:01\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:01\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:04:00\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:59\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:59\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:59\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:59\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:59\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:58\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:58\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:58\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:58\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:57\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:56\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:56\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:56\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:55\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:55\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:55\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:55\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:54\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.2 MB/s eta 0:03:54\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:50\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:50\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:50\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:49\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:49\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:49\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:49\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:49\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:49\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:49\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:48\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:48\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:47\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:45\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:46\n",
      "   ------------------------- -------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:47\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:47\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:47\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:47\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:46\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:46\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:46\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:45\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:45\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:45\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:45\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:44\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:44\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:44\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:44\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:43\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:42\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:41\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:41\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:41\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:40\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:41\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:41\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:39\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:38\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:39\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:39\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:39\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:39\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:39\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:38\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:38\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:38\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:37\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:37\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:37\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.3 MB/s eta 0:03:37\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.4 MB/s eta 0:03:33\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.4 MB/s eta 0:03:33\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.4 MB/s eta 0:03:33\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.4 MB/s eta 0:03:33\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.4 MB/s eta 0:03:33\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.4 MB/s eta 0:03:30\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.4 MB/s eta 0:03:30\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.4 MB/s eta 0:03:31\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.4 MB/s eta 0:03:31\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.5 MB/s eta 0:03:27\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.5 MB/s eta 0:03:26\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.5 MB/s eta 0:03:26\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.5 MB/s eta 0:03:25\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.5 MB/s eta 0:03:25\n",
      "   -------------------------- ------------- 1.8/2.8 GB 4.5 MB/s eta 0:03:24\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:24\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:27\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:27\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:27\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:26\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:26\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:26\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:25\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:25\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:25\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:25\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:25\n",
      "   -------------------------- ------------- 1.9/2.8 GB 4.4 MB/s eta 0:03:24\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:23\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:22\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:22\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:22\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:22\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:16\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:16\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:16\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:16\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:16\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.5 MB/s eta 0:03:16\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:21\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:21\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:20\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:20\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:20\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:20\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:19\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:19\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:19\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.4 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:20\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:21\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:20\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:20\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:20\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:19\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:19\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:19\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:19\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:19\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:18\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:17\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:15\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:15\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:14\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:14\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:13\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:14\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:14\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:14\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:14\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:14\n",
      "   --------------------------- ------------ 1.9/2.8 GB 4.3 MB/s eta 0:03:14\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 1.9/2.8 GB 4.3 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:10\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:09\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:09\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:09\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:18\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:18\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:18\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:17\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:17\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:17\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:16\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:15\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:15\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:14\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:13\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:12\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.1 MB/s eta 0:03:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:11\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:07\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:06\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:06\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:05\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:05\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.3 MB/s eta 0:03:05\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:05\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:05\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:05\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:05\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:04\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:04\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:04\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:04\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:03\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:03\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:03\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:03\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:02\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:02\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:02\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:02\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:03\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:03\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:03\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:02\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 4.2 MB/s eta 0:03:02\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:03:01\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:03:01\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:03:01\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:03:01\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:03:01\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:03:02\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:58\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:57\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:57\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:56\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:56\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:56\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:56\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:56\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:55\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:55\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:55\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:56\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:57\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:57\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:57\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:57\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:57\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:56\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:56\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:55\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:55\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:55\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:54\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:54\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:53\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:54\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.2 MB/s eta 0:02:54\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:52\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:52\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:51\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:51\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:51\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:51\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:49\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.0/2.8 GB 4.3 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.3 MB/s eta 0:02:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.3 MB/s eta 0:02:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.3 MB/s eta 0:02:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.3 MB/s eta 0:02:48\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.3 MB/s eta 0:02:48\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.3 MB/s eta 0:02:48\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.3 MB/s eta 0:02:48\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.3 MB/s eta 0:02:48\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.3 MB/s eta 0:02:48\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:53\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:53\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:52\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.2 MB/s eta 0:02:52\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.2 MB/s eta 0:02:52\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.2 MB/s eta 0:02:52\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:53\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:53\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:53\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:52\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:52\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:51\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:51\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:51\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:51\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:51\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:51\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:49\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:50\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 4.1 MB/s eta 0:02:50\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:48\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:43\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:43\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:42\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:42\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:42\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:42\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:41\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:41\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:41\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:41\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:41\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.3 MB/s eta 0:02:40\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.2 MB/s eta 0:02:42\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.2 MB/s eta 0:02:42\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.2 MB/s eta 0:02:42\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.2 MB/s eta 0:02:42\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.2 MB/s eta 0:02:42\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:46\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:46\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:46\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:46\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:45\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:44\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:46\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:45\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:45\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.1 MB/s eta 0:02:45\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.0 MB/s eta 0:02:47\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.0 MB/s eta 0:02:48\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.0 MB/s eta 0:02:48\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.0 MB/s eta 0:02:49\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.0 MB/s eta 0:02:49\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.0 MB/s eta 0:02:49\n",
      "   ------------------------------ --------- 2.1/2.8 GB 4.0 MB/s eta 0:02:49\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:50\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:50\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:50\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:50\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:50\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:50\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:55\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:54\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:54\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:54\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:55\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:55\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:55\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:54\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:52\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:52\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:52\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:52\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:51\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:50\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:49\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:49\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:48\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:47\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:47\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:47\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:46\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:46\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:46\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.8 MB/s eta 0:02:46\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:45\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:45\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:45\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:44\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:43\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:42\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:41\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:40\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:40\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:40\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:40\n",
      "   ------------------------------ --------- 2.1/2.8 GB 3.9 MB/s eta 0:02:39\n",
      "   ------------------------------- -------- 2.1/2.8 GB 3.9 MB/s eta 0:02:39\n",
      "   ------------------------------- -------- 2.1/2.8 GB 4.0 MB/s eta 0:02:38\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:37\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:37\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:37\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:37\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:35\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:35\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:34\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:34\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:34\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:34\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:33\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:33\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:33\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:33\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:32\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:32\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:32\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:31\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:31\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:30\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:30\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:30\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:30\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:31\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:31\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:31\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:27\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:26\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:25\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:25\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:25\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:25\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:25\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:25\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:25\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:23\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:22\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:22\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:22\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:22\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:22\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:21\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:21\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:21\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:20\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:20\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:19\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:19\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:18\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:18\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:18\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:18\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:18\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:18\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:22\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:21\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:21\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:20\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:20\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:20\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:20\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:20\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:20\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.0 MB/s eta 0:02:20\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:19\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:19\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:19\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:18\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:17\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.1 MB/s eta 0:02:17\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:13\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:13\n",
      "   ------------------------------- -------- 2.2/2.8 GB 4.2 MB/s eta 0:02:12\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.2 MB/s eta 0:02:12\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.2 MB/s eta 0:02:12\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.2 MB/s eta 0:02:11\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.2 MB/s eta 0:02:11\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.2 MB/s eta 0:02:10\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.3 MB/s eta 0:02:09\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.3 MB/s eta 0:02:09\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.3 MB/s eta 0:02:09\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.3 MB/s eta 0:02:08\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.3 MB/s eta 0:02:07\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.3 MB/s eta 0:02:06\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.3 MB/s eta 0:02:06\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.3 MB/s eta 0:02:06\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:04\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:04\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:04\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:04\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:04\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.4 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.5 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.5 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.5 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.5 MB/s eta 0:01:58\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.5 MB/s eta 0:01:58\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.5 MB/s eta 0:01:58\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.5 MB/s eta 0:01:57\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.5 MB/s eta 0:01:57\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.5 MB/s eta 0:01:57\n",
      "   -------------------------------- ------- 2.2/2.8 GB 4.5 MB/s eta 0:01:56\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.5 MB/s eta 0:01:56\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.5 MB/s eta 0:01:56\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.5 MB/s eta 0:01:56\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.5 MB/s eta 0:01:55\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.5 MB/s eta 0:01:55\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.5 MB/s eta 0:01:55\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.5 MB/s eta 0:01:55\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.5 MB/s eta 0:01:55\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.4 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.4 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.4 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.3 MB/s eta 0:02:01\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.3 MB/s eta 0:02:02\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:02\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:05\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:04\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:04\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:04\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:03\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:02\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:02\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.2 MB/s eta 0:02:02\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:02\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:02\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:01\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:01\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:01\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:01\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:01\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:01\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:01\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:00\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:00\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:00\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:02:00\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:01:59\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:01:58\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:01:58\n",
      "   -------------------------------- ------- 2.3/2.8 GB 4.1 MB/s eta 0:01:58\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:59\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:59\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:59\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:59\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:58\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:58\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:59\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:58\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:58\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:57\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:57\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:57\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:57\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:56\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:56\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:56\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:56\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:56\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:55\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:55\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:55\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:55\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:55\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:55\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:55\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:54\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:54\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:54\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:54\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:53\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:53\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:53\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.0 MB/s eta 0:01:53\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.0 MB/s eta 0:01:54\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.0 MB/s eta 0:01:54\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.0 MB/s eta 0:01:53\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.0 MB/s eta 0:01:53\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:50\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:50\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:49\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:49\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:49\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:48\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:48\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:48\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:47\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:47\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:47\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:46\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:46\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:46\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:46\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:46\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:45\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:45\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:45\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:45\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:44\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.2 MB/s eta 0:01:44\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:45\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:44\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:44\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:44\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:44\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:43\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:44\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:44\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:44\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:44\n",
      "   --------------------------------- ------ 2.3/2.8 GB 4.1 MB/s eta 0:01:43\n",
      "   --------------------------------- ------ 2.4/2.8 GB 4.1 MB/s eta 0:01:43\n",
      "   --------------------------------- ------ 2.4/2.8 GB 4.1 MB/s eta 0:01:43\n",
      "   --------------------------------- ------ 2.4/2.8 GB 4.1 MB/s eta 0:01:42\n",
      "   --------------------------------- ------ 2.4/2.8 GB 4.1 MB/s eta 0:01:42\n",
      "   --------------------------------- ------ 2.4/2.8 GB 4.1 MB/s eta 0:01:42\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.1 MB/s eta 0:01:42\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:40\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:40\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.1 MB/s eta 0:01:40\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.1 MB/s eta 0:01:40\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:39\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:39\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:39\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:38\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:38\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:38\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:38\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:37\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:37\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:37\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:37\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:37\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:36\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:36\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:36\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:36\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:35\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:35\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.2 MB/s eta 0:01:35\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.3 MB/s eta 0:01:32\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.3 MB/s eta 0:01:32\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.3 MB/s eta 0:01:31\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.3 MB/s eta 0:01:31\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.4 MB/s eta 0:01:29\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.4 MB/s eta 0:01:29\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.4 MB/s eta 0:01:29\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.4 MB/s eta 0:01:28\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.4 MB/s eta 0:01:27\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:26\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:26\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:25\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:25\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:24\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:24\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:24\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:24\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:24\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:23\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:23\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:23\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:23\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:22\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:22\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:22\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:22\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:21\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:21\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:21\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:21\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:20\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:20\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:20\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:20\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:19\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:19\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:19\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:19\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:18\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:19\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:19\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:19\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:18\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.5 MB/s eta 0:01:18\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.6 MB/s eta 0:01:17\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.6 MB/s eta 0:01:17\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 4.6 MB/s eta 0:01:16\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:16\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:16\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:15\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:15\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:15\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:15\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:14\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:14\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:14\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:14\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:14\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:13\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:13\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:13\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:12\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:12\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:12\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:11\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:11\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:11\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:11\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:10\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:10\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:10\n",
      "   ----------------------------------- ---- 2.4/2.8 GB 4.6 MB/s eta 0:01:10\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.6 MB/s eta 0:01:10\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.6 MB/s eta 0:01:09\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.6 MB/s eta 0:01:09\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.6 MB/s eta 0:01:09\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:08\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:08\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:08\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:08\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:07\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:07\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:07\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:06\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:06\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:06\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:06\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:04\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:04\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:04\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:04\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.6 MB/s eta 0:01:04\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.7 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:04\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:04\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:04\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:04\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:03\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:02\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:02\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:02\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 4.5 MB/s eta 0:01:02\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.5 MB/s eta 0:01:02\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.5 MB/s eta 0:01:02\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.5 MB/s eta 0:01:02\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.5 MB/s eta 0:01:02\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.5 MB/s eta 0:01:02\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.5 MB/s eta 0:01:02\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:02\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:02\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:02\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:01\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:01\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:01\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:01\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:01\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:01\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:01\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:01\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:00\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:00\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:00\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:00\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:01:00\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:59\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:59\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:59\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:59\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:58\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:58\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:58\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:58\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:58\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.4 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:57\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:56\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:56\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:56\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:56\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:56\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:55\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:55\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:55\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:55\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:55\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.2 MB/s eta 0:00:55\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.2 MB/s eta 0:00:55\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.2 MB/s eta 0:00:55\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.2 MB/s eta 0:00:54\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:54\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:53\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:53\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:53\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:53\n",
      "   ------------------------------------ --- 2.5/2.8 GB 4.3 MB/s eta 0:00:53\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.3 MB/s eta 0:00:52\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.3 MB/s eta 0:00:52\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.3 MB/s eta 0:00:52\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.3 MB/s eta 0:00:52\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.2 MB/s eta 0:00:52\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.2 MB/s eta 0:00:52\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.2 MB/s eta 0:00:52\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.2 MB/s eta 0:00:51\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.2 MB/s eta 0:00:51\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.2 MB/s eta 0:00:51\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.2 MB/s eta 0:00:51\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.2 MB/s eta 0:00:50\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.2 MB/s eta 0:00:50\n",
      "   ------------------------------------ --- 2.6/2.8 GB 4.2 MB/s eta 0:00:50\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:50\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:49\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:49\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:49\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:49\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:48\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:48\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:48\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:48\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:48\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:47\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:47\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:47\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:47\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:46\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:46\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:46\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:46\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:45\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:45\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:45\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:45\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:45\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:44\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:44\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:44\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:44\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:43\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:43\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:43\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:43\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:42\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:42\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:42\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:42\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:41\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:41\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:41\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:41\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:40\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.2 MB/s eta 0:00:40\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.3 MB/s eta 0:00:39\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.3 MB/s eta 0:00:39\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.3 MB/s eta 0:00:39\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.3 MB/s eta 0:00:39\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:37\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:37\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:37\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:37\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:36\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:36\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:36\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:36\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:36\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.3 MB/s eta 0:00:36\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.3 MB/s eta 0:00:36\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:36\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:35\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:35\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:35\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:35\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:35\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:34\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:34\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:33\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:33\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:33\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:33\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.4 MB/s eta 0:00:33\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.5 MB/s eta 0:00:32\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.5 MB/s eta 0:00:32\n",
      "   ------------------------------------- -- 2.6/2.8 GB 4.5 MB/s eta 0:00:31\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.5 MB/s eta 0:00:31\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.5 MB/s eta 0:00:31\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.5 MB/s eta 0:00:31\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:31\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:31\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:31\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:31\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:30\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:30\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:30\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:30\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:30\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:30\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:30\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:29\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:29\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:29\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:29\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:29\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.4 MB/s eta 0:00:29\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.3 MB/s eta 0:00:29\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.3 MB/s eta 0:00:29\n",
      "   -------------------------------------- - 2.6/2.8 GB 4.3 MB/s eta 0:00:28\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:28\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:28\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:28\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:28\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:28\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:28\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:27\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:27\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:27\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:26\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:26\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:26\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:26\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:26\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:26\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:26\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:25\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:25\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:25\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:24\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:24\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:24\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:24\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:23\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:23\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:23\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:23\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:23\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:22\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:22\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:22\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:22\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.4 MB/s eta 0:00:22\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.3 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.2 MB/s eta 0:00:21\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.2 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.2 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.2 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.2 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.2 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.2 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.2 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.2 MB/s eta 0:00:20\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.2 MB/s eta 0:00:19\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:19\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:19\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:19\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:19\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:19\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:19\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:18\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:18\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:18\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:18\n",
      "   -------------------------------------- - 2.7/2.8 GB 4.1 MB/s eta 0:00:18\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:18\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:17\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:17\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:17\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:16\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:16\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:16\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:16\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:15\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:15\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:15\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:15\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:14\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:14\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:14\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:14\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:14\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:13\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:13\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:13\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:13\n",
      "   ---------------------------------------  2.7/2.8 GB 4.1 MB/s eta 0:00:12\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:12\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:12\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:12\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:12\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:11\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:11\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:11\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:11\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:11\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:10\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:09\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:09\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:09\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:09\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:09\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:09\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:08\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:08\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:08\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:08\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:07\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:07\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:07\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:07\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:06\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:06\n",
      "   ---------------------------------------  2.7/2.8 GB 4.0 MB/s eta 0:00:06\n",
      "   ---------------------------------------  2.8/2.8 GB 4.0 MB/s eta 0:00:06\n",
      "   ---------------------------------------  2.8/2.8 GB 4.0 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 4.0 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 4.0 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 4.0 MB/s eta 0:00:05\n",
      "   ---------------------------------------  2.8/2.8 GB 4.0 MB/s eta 0:00:04\n",
      "   ---------------------------------------  2.8/2.8 GB 4.0 MB/s eta 0:00:04\n",
      "   ---------------------------------------  2.8/2.8 GB 4.0 MB/s eta 0:00:04\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 GB 3.7 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp313-cp313-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/6.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.3 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.9/6.3 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp313-cp313-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.0/4.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.8/4.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.1/4.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.9/4.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.2/4.2 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.2 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.1/6.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.9/6.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.9/6.2 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 4.7 MB/s eta 0:00:00\n",
      "Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl (930 kB)\n",
      "Installing collected packages: mpmath, sympy, setuptools, networkx, jinja2, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 setuptools-70.2.0 sympy-1.13.3 torch-2.7.0+cu126 torchaudio-2.7.0+cu126 torchvision-0.22.0+cu126\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa585f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662774b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in .\\supernova\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in .\\supernova\\lib\\site-packages (2.1.3)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting category_encoders\n",
      "  Using cached category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-3.0.0-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting catboost\n",
      "  Using cached catboost-1.2.8-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting optuna\n",
      "  Using cached optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: joblib in .\\supernova\\lib\\site-packages (1.4.2)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting scikeras\n",
      "  Using cached scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\supernova\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\supernova\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\supernova\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\supernova\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\supernova\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\supernova\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\supernova\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\supernova\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in .\\supernova\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in .\\supernova\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in .\\supernova\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting patsy>=0.5.1 (from category_encoders)\n",
      "  Using cached patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting statsmodels>=0.9.0 (from category_encoders)\n",
      "  Using cached statsmodels-0.14.4-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: graphviz in .\\supernova\\lib\\site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: plotly in .\\supernova\\lib\\site-packages (from catboost) (6.0.1)\n",
      "Requirement already satisfied: six in .\\supernova\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Using cached alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Using cached colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in .\\supernova\\lib\\site-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in .\\supernova\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in .\\supernova\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in .\\supernova\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in .\\supernova\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in .\\supernova\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: libclang>=13.0.0 in .\\supernova\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in .\\supernova\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in .\\supernova\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in .\\supernova\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in .\\supernova\\lib\\site-packages (from tensorflow) (79.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in .\\supernova\\lib\\site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in .\\supernova\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in .\\supernova\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in .\\supernova\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.13.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in .\\supernova\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: namex in .\\supernova\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.15.0-cp312-cp312-win_amd64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\supernova\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\supernova\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\supernova\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\supernova\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in .\\supernova\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in .\\supernova\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in .\\supernova\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in .\\supernova\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: colorama in .\\supernova\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in .\\supernova\\lib\\site-packages (from plotly->catboost) (1.36.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in .\\supernova\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in .\\supernova\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in .\\supernova\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached matplotlib-3.10.1-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "Using cached category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
      "Using cached xgboost-3.0.0-py3-none-win_amd64.whl (150.0 MB)\n",
      "Using cached catboost-1.2.8-cp312-cp312-win_amd64.whl (102.4 MB)\n",
      "Using cached optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl (376.0 MB)\n",
      "Using cached scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Using cached alembic-1.15.2-py3-none-any.whl (231 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached contourpy-1.3.2-cp312-cp312-win_amd64.whl (223 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.13.0-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "Using cached keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
      "Using cached patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Using cached statsmodels-0.14.4-cp312-cp312-win_amd64.whl (9.8 MB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Using cached optree-0.15.0-cp312-cp312-win_amd64.whl (307 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Installing collected packages: patsy, optree, ml-dtypes, markdown-it-py, Mako, h5py, google-pasta, contourpy, colorlog, astunparse, xgboost, tensorboard, scikit-learn, rich, matplotlib, alembic, statsmodels, seaborn, optuna, keras, catboost, tensorflow, scikeras, category_encoders\n",
      "Successfully installed Mako-1.3.10 alembic-1.15.2 astunparse-1.6.3 catboost-1.2.8 category_encoders-2.8.1 colorlog-6.9.0 contourpy-1.3.2 google-pasta-0.2.0 h5py-3.13.0 keras-3.9.2 markdown-it-py-3.0.0 matplotlib-3.10.1 ml-dtypes-0.5.1 optree-0.15.0 optuna-4.3.0 patsy-1.0.1 rich-14.0.0 scikeras-0.13.0 scikit-learn-1.6.1 seaborn-0.13.2 statsmodels-0.14.4 tensorboard-2.19.0 tensorflow-2.19.0 xgboost-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas numpy matplotlib seaborn scikit-learn category_encoders xgboost catboost optuna joblib tensorflow scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112cb7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in .\\supernova\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in .\\supernova\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in .\\supernova\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in .\\supernova\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in .\\supernova\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: category_encoders in .\\supernova\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: xgboost in .\\supernova\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: catboost in .\\supernova\\lib\\site-packages (1.2.8)\n",
      "Requirement already satisfied: optuna in .\\supernova\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: joblib in .\\supernova\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scikeras in .\\supernova\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\supernova\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\supernova\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\supernova\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\supernova\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\supernova\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\supernova\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\supernova\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\supernova\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\supernova\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in .\\supernova\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in .\\supernova\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in .\\supernova\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: patsy>=0.5.1 in .\\supernova\\lib\\site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in .\\supernova\\lib\\site-packages (from category_encoders) (0.14.4)\n",
      "Requirement already satisfied: graphviz in .\\supernova\\lib\\site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: plotly in .\\supernova\\lib\\site-packages (from catboost) (6.0.1)\n",
      "Requirement already satisfied: six in .\\supernova\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in .\\supernova\\lib\\site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in .\\supernova\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in .\\supernova\\lib\\site-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in .\\supernova\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in .\\supernova\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: keras>=3.2.0 in .\\supernova\\lib\\site-packages (from scikeras) (3.9.2)\n",
      "Requirement already satisfied: Mako in .\\supernova\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in .\\supernova\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: absl-py in .\\supernova\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.2.2)\n",
      "Requirement already satisfied: rich in .\\supernova\\lib\\site-packages (from keras>=3.2.0->scikeras) (14.0.0)\n",
      "Requirement already satisfied: namex in .\\supernova\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.0.9)\n",
      "Requirement already satisfied: h5py in .\\supernova\\lib\\site-packages (from keras>=3.2.0->scikeras) (3.13.0)\n",
      "Requirement already satisfied: optree in .\\supernova\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.15.0)\n",
      "Requirement already satisfied: ml-dtypes in .\\supernova\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.5.1)\n",
      "Requirement already satisfied: greenlet>=1 in .\\supernova\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
      "Requirement already satisfied: colorama in .\\supernova\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in .\\supernova\\lib\\site-packages (from plotly->catboost) (1.36.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in .\\supernova\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in .\\supernova\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in .\\supernova\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in .\\supernova\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy matplotlib seaborn scikit-learn category_encoders xgboost catboost optuna joblib scikeras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48b3fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-24 12:19:03,809 - INFO - Created directory: models\n",
      "2025-04-24 12:19:03,811 - INFO - Created directory: features\n",
      "2025-04-24 12:19:03,814 - INFO - Created directory: results\n",
      "2025-04-24 12:19:03,816 - INFO - Created directory: submissions\n",
      "2025-04-24 12:19:03,819 - INFO - Created directory: logs\n",
      "2025-04-24 12:19:03,821 - INFO - Created directory: plots\n",
      "2025-04-24 12:19:03,824 - INFO - Created directory: optuna_trials\n",
      "2025-04-24 12:19:03,827 - INFO - Created directory: scalers\n",
      "2025-04-24 12:19:03,830 - INFO - --- Starting Complete Pipeline Run --- Timestamp: 20250424_121903 ---\n",
      "2025-04-24 12:19:03,833 - INFO - Pipeline Config: Combined FE, Scaling=True, FeatSelect=True (Thresh=mean), CV Thresh=0.72, n_jobs=18 used for Sklearn, Const Cols Kept in Preproc\n",
      "2025-04-24 12:19:03,835 - INFO - Logging detailed output to: logs/pipeline_run_20250424_121903.log\n",
      "2025-04-24 12:19:03,837 - INFO - Loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Complete Pipeline Run (Combined FE) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 12:19:04,321 - INFO - Train shape: (1280, 317), Test shape: (854, 316)\n",
      "2025-04-24 12:19:04,327 - INFO - Found 39 unique states and 5 unique feature_1 values across train/test.\n",
      "2025-04-24 12:19:04,329 - INFO - Preprocessing training data (using combined logic)...\n",
      "2025-04-24 12:19:04,336 - INFO - Starting preprocessing (Combined Logic). Is training: True\n",
      "2025-04-24 12:19:04,343 - INFO - Target variable 'salary_category' found and label encoded.\n",
      "2025-04-24 12:19:04,349 - INFO - Target distribution (Encoded): [501 419 360]\n",
      "2025-04-24 12:19:04,358 - INFO - Saved label encoder and target mapping: features/target_mapping_20250424_121903.json\n",
      "2025-04-24 12:19:04,361 - INFO - Initial cleaning: Numerical and Boolean Features...\n",
      "2025-04-24 12:19:04,848 - WARNING - Column 'feature_10' contains non-0/1 values (834 instances). Treating as numerical and imputing with median.\n",
      "2025-04-24 12:19:04,853 - INFO - Starting Feature Engineering (using logic from simpler model)...\n",
      "2025-04-24 12:19:04,897 - INFO - Applying Target Encoding to 'job_title_grouped'...\n",
      "2025-04-24 12:19:04,941 - INFO - Fit and saved TargetEncoder for job_title_grouped\n",
      "2025-04-24 12:19:04,980 - INFO - Processed 'job_title' (flags, grouping, target encoding).\n",
      "2025-04-24 12:19:05,030 - INFO - Processed 'job_posted_date' (cyclical, recency, norm year).\n",
      "2025-04-24 12:19:05,042 - INFO - Added binned feature for feature_9.\n",
      "2025-04-24 12:19:05,046 - INFO - Added interaction: feature_2_9_interaction\n",
      "2025-04-24 12:19:05,061 - INFO - Added squared, sqrt, and binned features for feature_2.\n",
      "2025-04-24 12:19:05,074 - INFO - Added boolean sum and squared sum features.\n",
      "2025-04-24 12:19:05,079 - INFO - Added interaction: feature_10_8_interaction\n",
      "2025-04-24 12:19:05,348 - INFO - Applying PCA (n=15) to job description features...\n",
      "2025-04-24 12:19:05,458 - INFO - Fit and saved PCA model for job description.\n",
      "2025-04-24 12:19:05,477 - INFO - Finished processing job description features (aggregates and PCA).\n",
      "2025-04-24 12:19:05,481 - INFO - Applying manual One-Hot Encoding for 'job_state' using 39 total unique states.\n",
      "2025-04-24 12:19:05,551 - INFO - Applying manual One-Hot Encoding for 'feature_1' using 5 total unique values.\n",
      "2025-04-24 12:19:05,572 - INFO - Final cleanup and column alignment...\n",
      "2025-04-24 12:19:05,641 - WARNING - Column 'is_senior' identified as constant (nunique=1) in training data. Engineered: True. Keeping column.\n",
      "2025-04-24 12:19:05,644 - WARNING - Column 'is_junior' identified as constant (nunique=1) in training data. Engineered: True. Keeping column.\n",
      "2025-04-24 12:19:05,646 - WARNING - Column 'is_developer' identified as constant (nunique=1) in training data. Engineered: True. Keeping column.\n",
      "2025-04-24 12:19:05,651 - WARNING - Column 'is_specialist' identified as constant (nunique=1) in training data. Engineered: True. Keeping column.\n",
      "2025-04-24 12:19:05,662 - INFO - Engineered column 'state_WA' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,665 - INFO - Engineered column 'state_FL' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,668 - INFO - Engineered column 'state_OK' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,670 - INFO - Engineered column 'state_PA' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,674 - INFO - Engineered column 'state_CT' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,677 - INFO - Engineered column 'state_IL' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,679 - INFO - Engineered column 'state_OH' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,682 - INFO - Engineered column 'state_TN' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,686 - INFO - Engineered column 'state_SD' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,688 - INFO - Engineered column 'state_DC' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,691 - INFO - Engineered column 'state_NJ' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,694 - INFO - Engineered column 'state_OR' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,696 - INFO - Engineered column 'state_IA' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,699 - INFO - Engineered column 'state_MN' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,701 - WARNING - Column 'state_RI' identified as constant (nunique=1) in training data. Engineered: True. Keeping column.\n",
      "2025-04-24 12:19:05,703 - INFO - Engineered column 'state_MD' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,705 - INFO - Engineered column 'state_NC' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,708 - INFO - Engineered column 'state_NY' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,711 - INFO - Engineered column 'state_VA' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,714 - INFO - Engineered column 'state_NV' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,716 - INFO - Engineered column 'state_AR' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,719 - INFO - Engineered column 'state_MI' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,723 - INFO - Engineered column 'state_GA' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,725 - INFO - Engineered column 'state_CA' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,727 - INFO - Engineered column 'state_SC' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,729 - INFO - Engineered column 'state_NM' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,732 - INFO - Engineered column 'state_TX' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,735 - INFO - Engineered column 'state_AL' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,737 - INFO - Engineered column 'state_UT' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,740 - INFO - Engineered column 'state_MO' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,743 - INFO - Engineered column 'state_AK' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,746 - WARNING - Column 'state_WY' identified as constant (nunique=1) in training data. Engineered: True. Keeping column.\n",
      "2025-04-24 12:19:05,749 - INFO - Engineered column 'state_KY' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,751 - INFO - Engineered column 'state_LA' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,754 - INFO - Engineered column 'state_AZ' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,757 - INFO - Engineered column 'state_CO' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,760 - WARNING - Column 'state_KS' identified as constant (nunique=1) in training data. Engineered: True. Keeping column.\n",
      "2025-04-24 12:19:05,762 - INFO - Engineered column 'state_IN' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,765 - INFO - Engineered column 'state_MA' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,768 - INFO - Engineered column 'feat1_C' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,771 - INFO - Engineered column 'feat1_E' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,773 - INFO - Engineered column 'feat1_D' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,776 - INFO - Engineered column 'feat1_B' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,779 - INFO - Engineered column 'feat1_A' has low cardinality (nunique=2) in training data.\n",
      "2025-04-24 12:19:05,787 - INFO - Saved 95 final feature column names (constant columns NOT dropped).\n",
      "2025-04-24 12:19:05,798 - INFO - Preprocessing train done. Shape: (1280, 95). Time: 1.46s\n",
      "2025-04-24 12:19:05,829 - INFO - Train preprocess done. Initial Feats: 95\n",
      "2025-04-24 12:19:05,832 - INFO - Splitting data (80/20)...\n",
      "2025-04-24 12:19:05,857 - INFO - Train (Pre-scale): (1024, 95), Val (Pre-scale): (256, 95)\n",
      "2025-04-24 12:19:05,860 - INFO - Applying StandardScaler...\n",
      "2025-04-24 12:19:05,916 - INFO - Scaler saved: scalers/scaler_20250424_121903.joblib\n",
      "2025-04-24 12:19:05,919 - INFO - Scaled Train shape: (1024, 95), Scaled Val shape: (256, 95)\n",
      "2025-04-24 12:19:05,921 - INFO - Preprocessing test data (using combined logic)...\n",
      "2025-04-24 12:19:05,923 - INFO - Starting preprocessing (Combined Logic). Is training: False\n",
      "2025-04-24 12:19:05,942 - INFO - Loaded latest label encoder: label_encoder_20250424_121903.joblib\n",
      "2025-04-24 12:19:05,944 - INFO - Initial cleaning: Numerical and Boolean Features...\n",
      "2025-04-24 12:19:06,359 - WARNING - Column 'feature_10' contains non-0/1 values (540 instances). Treating as numerical and imputing with median.\n",
      "2025-04-24 12:19:06,365 - INFO - Starting Feature Engineering (using logic from simpler model)...\n",
      "2025-04-24 12:19:06,414 - INFO - Loaded and applied TargetEncoder: features/job_title_encoder_20250424_121903.joblib\n",
      "2025-04-24 12:19:06,438 - INFO - Processed 'job_title' (flags, grouping, target encoding).\n",
      "2025-04-24 12:19:06,487 - INFO - Processed 'job_posted_date' (cyclical, recency, norm year).\n",
      "2025-04-24 12:19:06,495 - INFO - Added binned feature for feature_9.\n",
      "2025-04-24 12:19:06,499 - INFO - Added interaction: feature_2_9_interaction\n",
      "2025-04-24 12:19:06,512 - INFO - Added squared, sqrt, and binned features for feature_2.\n",
      "2025-04-24 12:19:06,524 - INFO - Added boolean sum and squared sum features.\n",
      "2025-04-24 12:19:06,529 - INFO - Added interaction: feature_10_8_interaction\n",
      "2025-04-24 12:19:06,749 - INFO - Applying PCA (n=15) to job description features...\n",
      "2025-04-24 12:19:06,768 - INFO - Loaded specific PCA model: features/job_desc_pca_20250424_121903.joblib\n",
      "2025-04-24 12:19:06,929 - INFO - Finished processing job description features (aggregates and PCA).\n",
      "2025-04-24 12:19:06,935 - INFO - Applying manual One-Hot Encoding for 'job_state' using 39 total unique states.\n",
      "2025-04-24 12:19:07,307 - INFO - Applying manual One-Hot Encoding for 'feature_1' using 5 total unique values.\n",
      "2025-04-24 12:19:07,321 - INFO - Final cleanup and column alignment...\n",
      "2025-04-24 12:19:07,412 - INFO - Preprocessing test done. Shape: (854, 95). Time: 1.49s\n",
      "2025-04-24 12:19:07,422 - INFO - Test columns aligned.\n",
      "2025-04-24 12:19:07,424 - INFO - Scaling test data...\n",
      "2025-04-24 12:19:07,439 - INFO - Test preprocess & scale done. Shape: (854, 95)\n",
      "2025-04-24 12:19:07,444 - INFO - Performing feature selection (Threshold: mean)...\n",
      "2025-04-24 12:19:07,447 - INFO - Fitting RF for feature selection...\n",
      "2025-04-24 12:19:08,286 - INFO - Feat selection removed 58 features. Selected 37.\n",
      "2025-04-24 12:19:08,298 - INFO - Selection applied to train/val/test.\n",
      "2025-04-24 12:19:08,303 - INFO - Data shapes post-scaling/selection: Train=(1024, 37), Val=(256, 37), Test=(854, 37)\n",
      "2025-04-24 12:19:08,305 - INFO - Number of features used in modeling: 37\n",
      "2025-04-24 12:19:08,308 - INFO - --- Optimizing models & Making Individual Predictions (Thresh: 0.72) ---\n",
      "2025-04-24 12:19:08,311 - INFO - --- Optimizing logistic ---\n",
      "2025-04-24 12:19:08,313 - INFO - Starting logistic optimization (30 trials)...\n",
      "2025-04-24 12:19:08,317 - INFO - Optuna timeout for logistic: 3600s.\n",
      "[I 2025-04-24 12:19:24,319] A new study created in RDB with name: logistic_opt_20250424_121903\n",
      "2025-04-24 12:19:24,337 - INFO - Setting Optuna timeout for logistic to 3600 seconds.\n",
      "[I 2025-04-24 12:19:24,657] Trial 0 finished with value: 0.6562697274031564 and parameters: {'C': 0.042000519717404865, 'penalty': 'l2', 'max_iter': 745}. Best is trial 0 with value: 0.6562697274031564.\n",
      "[I 2025-04-24 12:19:25,082] Trial 1 finished with value: 0.66506456241033 and parameters: {'C': 2.5629506191010494, 'penalty': 'l2', 'max_iter': 512}. Best is trial 1 with value: 0.66506456241033.\n",
      "[I 2025-04-24 12:19:25,393] Trial 2 finished with value: 0.66506456241033 and parameters: {'C': 0.4766104509058789, 'penalty': 'l2', 'max_iter': 515}. Best is trial 1 with value: 0.66506456241033.\n",
      "[I 2025-04-24 12:19:25,624] Trial 3 finished with value: 0.6289191774270684 and parameters: {'C': 0.0007674581453758628, 'penalty': 'l2', 'max_iter': 859}. Best is trial 1 with value: 0.66506456241033.\n",
      "[I 2025-04-24 12:19:25,870] Trial 4 finished with value: 0.6533428981348637 and parameters: {'C': 0.032615699130920266, 'penalty': 'l2', 'max_iter': 207}. Best is trial 1 with value: 0.66506456241033.\n",
      "[I 2025-04-24 12:19:26,074] Trial 5 finished with value: 0.4169679579148733 and parameters: {'C': 0.005680790440926604, 'penalty': 'l1', 'max_iter': 403}. Best is trial 1 with value: 0.66506456241033.\n",
      "[I 2025-04-24 12:19:26,462] Trial 6 finished with value: 0.66506456241033 and parameters: {'C': 2.4540484536450826, 'penalty': 'l2', 'max_iter': 783}. Best is trial 1 with value: 0.66506456241033.\n",
      "[I 2025-04-24 12:19:26,741] Trial 7 finished with value: 0.6513916786226686 and parameters: {'C': 0.041752202530916764, 'penalty': 'l1', 'max_iter': 381}. Best is trial 1 with value: 0.66506456241033.\n",
      "[I 2025-04-24 12:19:27,467] Trial 8 finished with value: 0.6611573409851745 and parameters: {'C': 69.39961448534167, 'penalty': 'l2', 'max_iter': 524}. Best is trial 1 with value: 0.66506456241033.\n",
      "[I 2025-04-24 12:19:28,035] Trial 9 finished with value: 0.6611573409851745 and parameters: {'C': 10.600597059505574, 'penalty': 'l2', 'max_iter': 802}. Best is trial 1 with value: 0.66506456241033.\n",
      "[I 2025-04-24 12:20:05,561] Trial 10 finished with value: 0.6670157819225251 and parameters: {'C': 453.5483848001512, 'penalty': 'l1', 'max_iter': 166}. Best is trial 10 with value: 0.6670157819225251.\n",
      "[I 2025-04-24 12:20:21,057] Trial 11 finished with value: 0.6660401721664275 and parameters: {'C': 153.98500160907994, 'penalty': 'l1', 'max_iter': 113}. Best is trial 10 with value: 0.6670157819225251.\n",
      "[I 2025-04-24 12:20:39,557] Trial 12 finished with value: 0.6670205643232903 and parameters: {'C': 949.5874531608288, 'penalty': 'l1', 'max_iter': 112}. Best is trial 12 with value: 0.6670205643232903.\n",
      "[I 2025-04-24 12:21:20,799] Trial 13 finished with value: 0.6679818268770923 and parameters: {'C': 747.6032509919943, 'penalty': 'l1', 'max_iter': 271}. Best is trial 13 with value: 0.6679818268770923.\n",
      "[I 2025-04-24 12:21:57,977] Trial 14 finished with value: 0.6670062171209947 and parameters: {'C': 705.5349475849729, 'penalty': 'l1', 'max_iter': 286}. Best is trial 13 with value: 0.6679818268770923.\n",
      "[I 2025-04-24 12:22:02,587] Trial 15 finished with value: 0.6631085604973697 and parameters: {'C': 31.07401985911699, 'penalty': 'l1', 'max_iter': 993}. Best is trial 13 with value: 0.6679818268770923.\n",
      "[I 2025-04-24 12:22:38,026] Trial 16 finished with value: 0.6689574366331899 and parameters: {'C': 496.87792954844946, 'penalty': 'l1', 'max_iter': 277}. Best is trial 16 with value: 0.6689574366331899.\n",
      "[I 2025-04-24 12:22:40,028] Trial 17 finished with value: 0.6640841702534672 and parameters: {'C': 16.33050792513609, 'penalty': 'l1', 'max_iter': 314}. Best is trial 16 with value: 0.6689574366331899.\n",
      "[I 2025-04-24 12:22:40,132] Trial 18 finished with value: 0.3916021042563367 and parameters: {'C': 0.00011248518471436796, 'penalty': 'l1', 'max_iter': 647}. Best is trial 16 with value: 0.6689574366331899.\n",
      "[I 2025-04-24 12:23:04,102] Trial 19 finished with value: 0.6699426111908178 and parameters: {'C': 135.6420165898479, 'penalty': 'l1', 'max_iter': 263}. Best is trial 19 with value: 0.6699426111908178.\n",
      "[I 2025-04-24 12:23:05,137] Trial 20 finished with value: 0.6631085604973697 and parameters: {'C': 4.253891303288255, 'penalty': 'l1', 'max_iter': 408}. Best is trial 19 with value: 0.6699426111908178.\n",
      "[I 2025-04-24 12:23:30,161] Trial 21 finished with value: 0.6699426111908178 and parameters: {'C': 145.34054026384646, 'penalty': 'l1', 'max_iter': 278}. Best is trial 19 with value: 0.6699426111908178.\n",
      "[I 2025-04-24 12:23:50,344] Trial 22 finished with value: 0.6689717838354854 and parameters: {'C': 124.97810566096085, 'penalty': 'l1', 'max_iter': 229}. Best is trial 19 with value: 0.6699426111908178.\n",
      "[I 2025-04-24 12:24:12,444] Trial 23 finished with value: 0.6709230033476805 and parameters: {'C': 120.40635746298676, 'penalty': 'l1', 'max_iter': 217}. Best is trial 23 with value: 0.6709230033476805.\n",
      "[I 2025-04-24 12:24:12,557] Trial 24 finished with value: 0.6670062171209947 and parameters: {'C': 0.4084625041783534, 'penalty': 'l1', 'max_iter': 362}. Best is trial 23 with value: 0.6709230033476805.\n",
      "[I 2025-04-24 12:24:30,241] Trial 25 finished with value: 0.6660353897656623 and parameters: {'C': 60.956814324612054, 'penalty': 'l1', 'max_iter': 464}. Best is trial 23 with value: 0.6709230033476805.\n",
      "[I 2025-04-24 12:25:02,769] Trial 26 finished with value: 0.6679818268770923 and parameters: {'C': 145.9753260814202, 'penalty': 'l1', 'max_iter': 626}. Best is trial 23 with value: 0.6709230033476805.\n",
      "[I 2025-04-24 12:25:04,220] Trial 27 finished with value: 0.6631085604973697 and parameters: {'C': 6.996366625409402, 'penalty': 'l1', 'max_iter': 180}. Best is trial 23 with value: 0.6709230033476805.\n",
      "[I 2025-04-24 12:25:04,553] Trial 28 finished with value: 0.6621281683405069 and parameters: {'C': 1.1519030886053208, 'penalty': 'l1', 'max_iter': 335}. Best is trial 23 with value: 0.6709230033476805.\n",
      "[I 2025-04-24 12:25:06,820] Trial 29 finished with value: 0.6621329507412721 and parameters: {'C': 19.804936465271073, 'penalty': 'l1', 'max_iter': 447}. Best is trial 23 with value: 0.6709230033476805.\n",
      "2025-04-24 12:25:06,841 - INFO - Optimization complete for logistic.\n",
      "2025-04-24 12:25:06,841 - INFO - Best CV score: 0.67092\n",
      "2025-04-24 12:25:06,841 - INFO - Best params: {'C': 120.40635746298676, 'penalty': 'l1', 'max_iter': 217}\n",
      "2025-04-24 12:25:06,841 - INFO - Saved Optuna summary: optuna_trials/logistic_study_summary_20250424_121903.txt\n",
      "2025-04-24 12:25:06,841 - INFO - Instantiating final logistic model...\n",
      "2025-04-24 12:25:06,841 - ERROR - Failed final instantiate/fit/save process logistic: cannot access local variable 'svc_p' where it is not associated with a value\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 709, in optimize_model\n",
      "    if 'class_weight' not in svc_p: svc_p['class_weight'] = 'balanced'; final_model = SVC(**svc_p)\n",
      "                             ^^^^^\n",
      "UnboundLocalError: cannot access local variable 'svc_p' where it is not associated with a value\n",
      "2025-04-24 12:25:06,841 - INFO - --- NOT QUALIFIED: logistic (CV Score: 0.67092  - Final fit/save failed) ---\n",
      "2025-04-24 12:25:06,850 - INFO - --- Optimizing knn ---\n",
      "2025-04-24 12:25:06,851 - INFO - Starting knn optimization (25 trials)...\n",
      "2025-04-24 12:25:06,853 - INFO - Optuna timeout for knn: 3600s.\n",
      "[I 2025-04-24 12:25:07,067] A new study created in RDB with name: knn_opt_20250424_121903\n",
      "2025-04-24 12:25:07,070 - INFO - Setting Optuna timeout for knn to 3600 seconds.\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[I 2025-04-24 12:25:07,383] Trial 0 finished with value: 0.5986465805834529 and parameters: {'metric': 'chebyshev', 'n_neighbors': 31, 'weights': 'distance'}. Best is trial 0 with value: 0.5986465805834529.\n",
      "[I 2025-04-24 12:25:07,670] Trial 1 finished with value: 0.6435485413677666 and parameters: {'metric': 'minkowski', 'n_neighbors': 63, 'weights': 'distance', 'p': 3}. Best is trial 1 with value: 0.6435485413677666.\n",
      "[I 2025-04-24 12:25:07,824] Trial 2 finished with value: 0.6738354854136777 and parameters: {'metric': 'manhattan', 'n_neighbors': 13, 'weights': 'distance'}. Best is trial 2 with value: 0.6738354854136777.\n",
      "[I 2025-04-24 12:25:07,969] Trial 3 finished with value: 0.6777522716403634 and parameters: {'metric': 'manhattan', 'n_neighbors': 27, 'weights': 'distance'}. Best is trial 3 with value: 0.6777522716403634.\n",
      "[I 2025-04-24 12:25:08,095] Trial 4 finished with value: 0.6289383070301291 and parameters: {'metric': 'manhattan', 'n_neighbors': 61, 'weights': 'uniform'}. Best is trial 3 with value: 0.6777522716403634.\n",
      "[I 2025-04-24 12:25:08,264] Trial 5 finished with value: 0.64844093735055 and parameters: {'metric': 'minkowski', 'n_neighbors': 23, 'weights': 'uniform', 'p': 2}. Best is trial 3 with value: 0.6777522716403634.\n",
      "[I 2025-04-24 12:25:08,429] Trial 6 finished with value: 0.655298900047824 and parameters: {'metric': 'manhattan', 'n_neighbors': 65, 'weights': 'distance'}. Best is trial 3 with value: 0.6777522716403634.\n",
      "[I 2025-04-24 12:25:08,559] Trial 7 finished with value: 0.6387039693926351 and parameters: {'metric': 'manhattan', 'n_neighbors': 37, 'weights': 'uniform'}. Best is trial 3 with value: 0.6777522716403634.\n",
      "[I 2025-04-24 12:25:08,741] Trial 8 finished with value: 0.6201052128168341 and parameters: {'metric': 'minkowski', 'n_neighbors': 3, 'weights': 'uniform', 'p': 2}. Best is trial 3 with value: 0.6777522716403634.\n",
      "[I 2025-04-24 12:25:08,878] Trial 9 finished with value: 0.6328263988522238 and parameters: {'metric': 'minkowski', 'n_neighbors': 45, 'weights': 'uniform', 'p': 1}. Best is trial 3 with value: 0.6777522716403634.\n",
      "[I 2025-04-24 12:25:09,043] Trial 10 finished with value: 0.5986609277857484 and parameters: {'metric': 'chebyshev', 'n_neighbors': 45, 'weights': 'distance'}. Best is trial 3 with value: 0.6777522716403634.\n",
      "[I 2025-04-24 12:25:09,206] Trial 11 finished with value: 0.6738354854136777 and parameters: {'metric': 'manhattan', 'n_neighbors': 13, 'weights': 'distance'}. Best is trial 3 with value: 0.6777522716403634.\n",
      "[I 2025-04-24 12:25:09,357] Trial 12 finished with value: 0.6748110951697752 and parameters: {'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'distance'}. Best is trial 3 with value: 0.6777522716403634.\n",
      "[I 2025-04-24 12:25:09,499] Trial 13 finished with value: 0.6855571496891439 and parameters: {'metric': 'manhattan', 'n_neighbors': 21, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:09,647] Trial 14 finished with value: 0.6728694404591105 and parameters: {'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:09,795] Trial 15 finished with value: 0.6679913916786226 and parameters: {'metric': 'manhattan', 'n_neighbors': 39, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:09,964] Trial 16 finished with value: 0.5898613103778096 and parameters: {'metric': 'chebyshev', 'n_neighbors': 21, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:10,108] Trial 17 finished with value: 0.6572501195600191 and parameters: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:10,262] Trial 18 finished with value: 0.6777522716403634 and parameters: {'metric': 'manhattan', 'n_neighbors': 27, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:10,432] Trial 19 finished with value: 0.604519368723099 and parameters: {'metric': 'chebyshev', 'n_neighbors': 55, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:10,591] Trial 20 finished with value: 0.6835963653754185 and parameters: {'metric': 'manhattan', 'n_neighbors': 15, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:10,735] Trial 21 finished with value: 0.6835963653754185 and parameters: {'metric': 'manhattan', 'n_neighbors': 15, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:10,889] Trial 22 finished with value: 0.6738354854136777 and parameters: {'metric': 'manhattan', 'n_neighbors': 13, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:11,033] Trial 23 finished with value: 0.6816451458632233 and parameters: {'metric': 'manhattan', 'n_neighbors': 17, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "[I 2025-04-24 12:25:11,185] Trial 24 finished with value: 0.6572501195600191 and parameters: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}. Best is trial 13 with value: 0.6855571496891439.\n",
      "2025-04-24 12:25:11,193 - INFO - Optimization complete for knn.\n",
      "2025-04-24 12:25:11,193 - INFO - Best CV score: 0.68556\n",
      "2025-04-24 12:25:11,194 - INFO - Best params: {'metric': 'manhattan', 'n_neighbors': 21, 'weights': 'distance'}\n",
      "2025-04-24 12:25:11,194 - INFO - Saved Optuna summary: optuna_trials/knn_study_summary_20250424_121903.txt\n",
      "2025-04-24 12:25:11,194 - INFO - Instantiating final knn model...\n",
      "2025-04-24 12:25:11,196 - ERROR - Failed final instantiate/fit/save process knn: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 705, in optimize_model\n",
      "    if 'class_weight' not in final_p_log: final_p_log['class_weight'] = 'balanced'; final_model = LogisticRegression(**final_p_log)\n",
      "                             ^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "2025-04-24 12:25:11,196 - INFO - --- NOT QUALIFIED: knn (CV Score: 0.68556  - Final fit/save failed) ---\n",
      "2025-04-24 12:25:11,198 - INFO - --- Optimizing adaboost ---\n",
      "2025-04-24 12:25:11,199 - INFO - Starting adaboost optimization (30 trials)...\n",
      "2025-04-24 12:25:11,200 - INFO - Optuna timeout for adaboost: 3600s.\n",
      "[I 2025-04-24 12:25:11,315] A new study created in RDB with name: adaboost_opt_20250424_121903\n",
      "2025-04-24 12:25:11,331 - INFO - Setting Optuna timeout for adaboost to 3600 seconds.\n",
      "[I 2025-04-24 12:25:42,908] Trial 0 finished with value: 0.6962697274031564 and parameters: {'base_estimator_max_depth': 4, 'n_estimators': 750, 'learning_rate': 0.07185831757293501}. Best is trial 0 with value: 0.6962697274031564.\n",
      "[I 2025-04-24 12:26:26,105] Trial 1 finished with value: 0.7275370636059302 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 750, 'learning_rate': 0.9455004039436984}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:26:45,815] Trial 2 finished with value: 0.6826446676231469 and parameters: {'base_estimator_max_depth': 3, 'n_estimators': 600, 'learning_rate': 0.05466987799489618}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:27:20,831] Trial 3 finished with value: 0.6953036824485892 and parameters: {'base_estimator_max_depth': 5, 'n_estimators': 700, 'learning_rate': 0.03683047319212475}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:27:23,614] Trial 4 finished with value: 0.6670253467240554 and parameters: {'base_estimator_max_depth': 1, 'n_estimators': 200, 'learning_rate': 0.2987001593686998}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:28:12,683] Trial 5 finished with value: 0.7050454328072692 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 550, 'learning_rate': 0.18454341702106733}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:31:16,561] Trial 6 finished with value: 0.714830224772836 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 750, 'learning_rate': 0.23017571828434497}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:32:39,284] Trial 7 finished with value: 0.6875131516021042 and parameters: {'base_estimator_max_depth': 5, 'n_estimators': 450, 'learning_rate': 0.01275332745152535}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:32:51,636] Trial 8 finished with value: 0.6777522716403634 and parameters: {'base_estimator_max_depth': 2, 'n_estimators': 500, 'learning_rate': 0.2115290279720699}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:33:03,239] Trial 9 finished with value: 0.7070301291248207 and parameters: {'base_estimator_max_depth': 5, 'n_estimators': 200, 'learning_rate': 0.28300026483342}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:33:04,940] Trial 10 finished with value: 0.6386704925872788 and parameters: {'base_estimator_max_depth': 3, 'n_estimators': 50, 'learning_rate': 1.3626656633836127}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:33:50,104] Trial 11 finished with value: 0.7187517934002869 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 800, 'learning_rate': 1.018130661762377}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:37:14,445] Trial 12 finished with value: 0.7070157819225251 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 800, 'learning_rate': 1.4095056116578946}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:39:21,096] Trial 13 finished with value: 0.7040889526542324 and parameters: {'base_estimator_max_depth': 4, 'n_estimators': 650, 'learning_rate': 0.6746061386959858}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:40:59,758] Trial 14 finished with value: 0.722649450023912 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 350, 'learning_rate': 0.6429050502396485}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:42:03,192] Trial 15 finished with value: 0.7099426111908178 and parameters: {'base_estimator_max_depth': 5, 'n_estimators': 300, 'learning_rate': 0.516217156083386}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:42:17,950] Trial 16 finished with value: 0.7040698230511716 and parameters: {'base_estimator_max_depth': 4, 'n_estimators': 350, 'learning_rate': 0.5329245977652342}. Best is trial 1 with value: 0.7275370636059302.\n",
      "[I 2025-04-24 12:42:41,299] Trial 17 finished with value: 0.7304543280726925 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 350, 'learning_rate': 0.7467618196027865}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:42:51,920] Trial 18 finished with value: 0.700205643232903 and parameters: {'base_estimator_max_depth': 5, 'n_estimators': 200, 'learning_rate': 0.1368400005878025}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:42:53,061] Trial 19 finished with value: 0.6562984218077476 and parameters: {'base_estimator_max_depth': 1, 'n_estimators': 50, 'learning_rate': 0.45175322967636755}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:43:04,616] Trial 20 finished with value: 0.6513725490196078 and parameters: {'base_estimator_max_depth': 2, 'n_estimators': 450, 'learning_rate': 0.01728604965613094}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:43:25,580] Trial 21 finished with value: 0.7284839789574367 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 350, 'learning_rate': 0.7766929423472807}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:43:43,904] Trial 22 finished with value: 0.7197226207556193 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 300, 'learning_rate': 0.8729724903577666}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:44:04,431] Trial 23 finished with value: 0.7070062171209948 and parameters: {'base_estimator_max_depth': 5, 'n_estimators': 400, 'learning_rate': 0.349518133874093}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:44:36,724] Trial 24 finished with value: 0.72168818747011 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 550, 'learning_rate': 0.9284534735884933}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:44:52,509] Trial 25 finished with value: 0.7040937350549976 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 250, 'learning_rate': 1.4582451786267765}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:45:13,536] Trial 26 finished with value: 0.7177618364418938 and parameters: {'base_estimator_max_depth': 5, 'n_estimators': 400, 'learning_rate': 0.3836808736400511}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:45:17,875] Trial 27 finished with value: 0.7090100430416069 and parameters: {'base_estimator_max_depth': 4, 'n_estimators': 100, 'learning_rate': 0.1288466874131758}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:45:46,623] Trial 28 finished with value: 0.7236537541846007 and parameters: {'base_estimator_max_depth': 6, 'n_estimators': 500, 'learning_rate': 0.7928860032099135}. Best is trial 17 with value: 0.7304543280726925.\n",
      "[I 2025-04-24 12:46:02,401] Trial 29 finished with value: 0.7187374461979914 and parameters: {'base_estimator_max_depth': 5, 'n_estimators': 300, 'learning_rate': 1.0979892307917327}. Best is trial 17 with value: 0.7304543280726925.\n",
      "2025-04-24 12:46:02,414 - INFO - Optimization complete for adaboost.\n",
      "2025-04-24 12:46:02,416 - INFO - Best CV score: 0.73045\n",
      "2025-04-24 12:46:02,417 - INFO - Best params: {'base_estimator_max_depth': 6, 'n_estimators': 350, 'learning_rate': 0.7467618196027865}\n",
      "2025-04-24 12:46:02,419 - INFO - Saved Optuna summary: optuna_trials/adaboost_study_summary_20250424_121903.txt\n",
      "2025-04-24 12:46:02,420 - INFO - Instantiating final adaboost model...\n",
      "2025-04-24 12:46:02,420 - INFO - Reconstruct AdaBoost DT(max_depth=6) using SAMME\n",
      "2025-04-24 12:46:02,421 - ERROR - Failed final instantiate/fit/save process adaboost: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 705, in optimize_model\n",
      "    if 'class_weight' not in final_p_log: final_p_log['class_weight'] = 'balanced'; final_model = LogisticRegression(**final_p_log)\n",
      "                             ^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "2025-04-24 12:46:02,423 - INFO - --- NOT QUALIFIED: adaboost (CV Score: 0.73045  - Final fit/save failed) ---\n",
      "2025-04-24 12:46:02,424 - INFO - --- Optimizing randomforest ---\n",
      "2025-04-24 12:46:02,425 - INFO - Starting randomforest optimization (40 trials)...\n",
      "2025-04-24 12:46:02,427 - INFO - Optuna timeout for randomforest: 5400s.\n",
      "[I 2025-04-24 12:46:02,617] A new study created in RDB with name: randomforest_opt_20250424_121903\n",
      "2025-04-24 12:46:02,622 - INFO - Setting Optuna timeout for randomforest to 5400 seconds.\n",
      "[I 2025-04-24 12:46:12,092] Trial 0 finished with value: 0.6357532281205165 and parameters: {'n_estimators': 1900, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 17, 'max_features': 'log2', 'bootstrap': True, 'class_weight': None}. Best is trial 0 with value: 0.6357532281205165.\n",
      "[I 2025-04-24 12:46:13,145] Trial 1 finished with value: 0.6376470588235295 and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 16, 'min_samples_leaf': 1, 'max_features': 0.6, 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 1 with value: 0.6376470588235295.\n",
      "[I 2025-04-24 12:46:15,359] Trial 2 finished with value: 0.6816547106647537 and parameters: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 23, 'min_samples_leaf': 14, 'max_features': 0.8, 'bootstrap': True, 'class_weight': 'balanced_subsample'}. Best is trial 2 with value: 0.6816547106647537.\n",
      "[I 2025-04-24 12:46:26,025] Trial 3 finished with value: 0.7099760879961741 and parameters: {'n_estimators': 2000, 'max_depth': 25, 'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:46:27,708] Trial 4 finished with value: 0.6562553802008608 and parameters: {'n_estimators': 300, 'max_depth': 35, 'min_samples_split': 15, 'min_samples_leaf': 15, 'max_features': 'sqrt', 'bootstrap': True, 'class_weight': None}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:46:37,695] Trial 5 finished with value: 0.6855380200860832 and parameters: {'n_estimators': 1400, 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 0.6, 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:46:39,320] Trial 6 finished with value: 0.6533428981348637 and parameters: {'n_estimators': 200, 'max_depth': 35, 'min_samples_split': 14, 'min_samples_leaf': 18, 'max_features': 'sqrt', 'bootstrap': True, 'class_weight': 'balanced_subsample'}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:46:45,004] Trial 7 finished with value: 0.7070444763271162 and parameters: {'n_estimators': 1400, 'max_depth': 35, 'min_samples_split': 14, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': False, 'class_weight': None}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:46:50,783] Trial 8 finished with value: 0.666001912960306 and parameters: {'n_estimators': 1000, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 11, 'max_features': 0.8, 'bootstrap': False, 'class_weight': None}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:46:58,216] Trial 9 finished with value: 0.6650597800095648 and parameters: {'n_estimators': 1500, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'bootstrap': True, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:47:06,825] Trial 10 finished with value: 0.6894643711142994 and parameters: {'n_estimators': 2000, 'max_depth': 20, 'min_samples_split': 21, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:47:10,544] Trial 11 finished with value: 0.7080200860832138 and parameters: {'n_estimators': 900, 'max_depth': 25, 'min_samples_split': 19, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': False, 'class_weight': None}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:47:13,825] Trial 12 finished with value: 0.6953228120516499 and parameters: {'n_estimators': 800, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 6, 'max_features': 'log2', 'bootstrap': False, 'class_weight': None}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:47:17,004] Trial 13 finished with value: 0.6894691535150645 and parameters: {'n_estimators': 700, 'max_depth': 20, 'min_samples_split': 19, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:47:24,605] Trial 14 finished with value: 0.6826255380200861 and parameters: {'n_estimators': 1700, 'max_depth': 25, 'min_samples_split': 11, 'min_samples_leaf': 9, 'max_features': 'log2', 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:47:27,207] Trial 15 finished with value: 0.6992252510760402 and parameters: {'n_estimators': 600, 'max_depth': 15, 'min_samples_split': 25, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:47:33,479] Trial 16 finished with value: 0.6855236728837876 and parameters: {'n_estimators': 1100, 'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': 0.6, 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:47:43,383] Trial 17 finished with value: 0.680626494500239 and parameters: {'n_estimators': 1100, 'max_depth': 25, 'min_samples_split': 18, 'min_samples_leaf': 2, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.7099760879961741.\n",
      "[I 2025-04-24 12:47:45,761] Trial 18 finished with value: 0.7109469153515064 and parameters: {'n_estimators': 500, 'max_depth': 30, 'min_samples_split': 12, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 18 with value: 0.7109469153515064.\n",
      "[I 2025-04-24 12:47:47,723] Trial 19 finished with value: 0.6816403634624582 and parameters: {'n_estimators': 400, 'max_depth': 30, 'min_samples_split': 12, 'min_samples_leaf': 12, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 18 with value: 0.7109469153515064.\n",
      "[I 2025-04-24 12:47:50,039] Trial 20 finished with value: 0.6943472022955524 and parameters: {'n_estimators': 500, 'max_depth': 40, 'min_samples_split': 17, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 18 with value: 0.7109469153515064.\n",
      "[I 2025-04-24 12:47:54,176] Trial 21 finished with value: 0.7129029172644668 and parameters: {'n_estimators': 900, 'max_depth': 20, 'min_samples_split': 12, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 21 with value: 0.7129029172644668.\n",
      "[I 2025-04-24 12:48:00,254] Trial 22 finished with value: 0.7099713055954089 and parameters: {'n_estimators': 1300, 'max_depth': 20, 'min_samples_split': 12, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 21 with value: 0.7129029172644668.\n",
      "[I 2025-04-24 12:48:03,357] Trial 23 finished with value: 0.6621281683405069 and parameters: {'n_estimators': 700, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 20, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 21 with value: 0.7129029172644668.\n",
      "[I 2025-04-24 12:48:08,748] Trial 24 finished with value: 0.7168149210903874 and parameters: {'n_estimators': 1200, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 24 with value: 0.7168149210903874.\n",
      "[I 2025-04-24 12:48:14,239] Trial 25 finished with value: 0.7168292682926829 and parameters: {'n_estimators': 1200, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 25 with value: 0.7168292682926829.\n",
      "[I 2025-04-24 12:48:20,303] Trial 26 finished with value: 0.6904304160688666 and parameters: {'n_estimators': 1200, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True, 'class_weight': None}. Best is trial 25 with value: 0.7168292682926829.\n",
      "[I 2025-04-24 12:48:27,461] Trial 27 finished with value: 0.6748206599713056 and parameters: {'n_estimators': 1600, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 25 with value: 0.7168292682926829.\n",
      "[I 2025-04-24 12:48:32,537] Trial 28 finished with value: 0.7129029172644668 and parameters: {'n_estimators': 900, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 25 with value: 0.7168292682926829.\n",
      "[I 2025-04-24 12:48:44,648] Trial 29 finished with value: 0.7090052606408417 and parameters: {'n_estimators': 1700, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 0.8, 'bootstrap': True, 'class_weight': None}. Best is trial 25 with value: 0.7168292682926829.\n",
      "[I 2025-04-24 12:48:53,149] Trial 30 finished with value: 0.6855475848876136 and parameters: {'n_estimators': 1300, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': 0.6, 'bootstrap': False, 'class_weight': None}. Best is trial 25 with value: 0.7168292682926829.\n",
      "[I 2025-04-24 12:48:57,650] Trial 31 finished with value: 0.7129029172644668 and parameters: {'n_estimators': 900, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 25 with value: 0.7168292682926829.\n",
      "[I 2025-04-24 12:49:03,718] Trial 32 finished with value: 0.7207173601147776 and parameters: {'n_estimators': 1000, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 32 with value: 0.7207173601147776.\n",
      "[I 2025-04-24 12:49:09,536] Trial 33 finished with value: 0.7177905308464849 and parameters: {'n_estimators': 1200, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 32 with value: 0.7207173601147776.\n",
      "[I 2025-04-24 12:49:14,433] Trial 34 finished with value: 0.6699378287900526 and parameters: {'n_estimators': 1200, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 32 with value: 0.7207173601147776.\n",
      "[I 2025-04-24 12:49:20,406] Trial 35 finished with value: 0.7148732663797226 and parameters: {'n_estimators': 1200, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 32 with value: 0.7207173601147776.\n",
      "[I 2025-04-24 12:49:26,472] Trial 36 finished with value: 0.6787087517934003 and parameters: {'n_estimators': 1000, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 0.6, 'bootstrap': True, 'class_weight': None}. Best is trial 32 with value: 0.7207173601147776.\n",
      "[I 2025-04-24 12:49:33,175] Trial 37 finished with value: 0.7138880918220947 and parameters: {'n_estimators': 1400, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 32 with value: 0.7207173601147776.\n",
      "[I 2025-04-24 12:49:43,754] Trial 38 finished with value: 0.664026781444285 and parameters: {'n_estimators': 1300, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.8, 'bootstrap': False, 'class_weight': None}. Best is trial 32 with value: 0.7207173601147776.\n",
      "[I 2025-04-24 12:49:56,902] Trial 39 finished with value: 0.6533333333333333 and parameters: {'n_estimators': 1800, 'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 14, 'max_features': 'sqrt', 'bootstrap': True, 'class_weight': 'balanced_subsample'}. Best is trial 32 with value: 0.7207173601147776.\n",
      "2025-04-24 12:49:56,910 - INFO - Optimization complete for randomforest.\n",
      "2025-04-24 12:49:56,910 - INFO - Best CV score: 0.72072\n",
      "2025-04-24 12:49:56,912 - INFO - Best params: {'n_estimators': 1000, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}\n",
      "2025-04-24 12:49:56,913 - INFO - Saved Optuna summary: optuna_trials/randomforest_study_summary_20250424_121903.txt\n",
      "2025-04-24 12:49:56,913 - INFO - Instantiating final randomforest model...\n",
      "2025-04-24 12:49:56,914 - ERROR - Failed final instantiate/fit/save process randomforest: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 705, in optimize_model\n",
      "    if 'class_weight' not in final_p_log: final_p_log['class_weight'] = 'balanced'; final_model = LogisticRegression(**final_p_log)\n",
      "                             ^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "2025-04-24 12:49:56,915 - INFO - --- NOT QUALIFIED: randomforest (CV Score: 0.72072  - Final fit/save failed) ---\n",
      "2025-04-24 12:49:56,915 - INFO - --- Optimizing extratrees ---\n",
      "2025-04-24 12:49:56,916 - INFO - Starting extratrees optimization (40 trials)...\n",
      "2025-04-24 12:49:56,917 - INFO - Optuna timeout for extratrees: 5400s.\n",
      "[I 2025-04-24 12:49:57,114] A new study created in RDB with name: extratrees_opt_20250424_121903\n",
      "2025-04-24 12:49:57,120 - INFO - Setting Optuna timeout for extratrees to 5400 seconds.\n",
      "[I 2025-04-24 12:49:59,383] Trial 0 finished with value: 0.6796843615494979 and parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 11, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6796843615494979.\n",
      "[I 2025-04-24 12:50:02,306] Trial 1 finished with value: 0.68454806312769 and parameters: {'n_estimators': 700, 'max_depth': 45, 'min_samples_split': 10, 'min_samples_leaf': 12, 'max_features': 0.6, 'bootstrap': False, 'class_weight': None}. Best is trial 1 with value: 0.68454806312769.\n",
      "[I 2025-04-24 12:50:08,516] Trial 2 finished with value: 0.6396556671449067 and parameters: {'n_estimators': 1400, 'max_depth': 25, 'min_samples_split': 25, 'min_samples_leaf': 20, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 1 with value: 0.68454806312769.\n",
      "[I 2025-04-24 12:50:16,586] Trial 3 finished with value: 0.681635581061693 and parameters: {'n_estimators': 1500, 'max_depth': 20, 'min_samples_split': 23, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'class_weight': None}. Best is trial 1 with value: 0.68454806312769.\n",
      "[I 2025-04-24 12:50:20,006] Trial 4 finished with value: 0.6416021042563367 and parameters: {'n_estimators': 800, 'max_depth': 40, 'min_samples_split': 21, 'min_samples_leaf': 20, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 1 with value: 0.68454806312769.\n",
      "[I 2025-04-24 12:50:30,731] Trial 5 finished with value: 0.6464992826398852 and parameters: {'n_estimators': 2000, 'max_depth': 45, 'min_samples_split': 8, 'min_samples_leaf': 12, 'max_features': 'log2', 'bootstrap': True, 'class_weight': 'balanced'}. Best is trial 1 with value: 0.68454806312769.\n",
      "[I 2025-04-24 12:50:36,691] Trial 6 finished with value: 0.6747919655667145 and parameters: {'n_estimators': 800, 'max_depth': 25, 'min_samples_split': 10, 'min_samples_leaf': 12, 'max_features': 0.6, 'bootstrap': True, 'class_weight': 'balanced_subsample'}. Best is trial 1 with value: 0.68454806312769.\n",
      "[I 2025-04-24 12:50:42,823] Trial 7 finished with value: 0.6699282639885222 and parameters: {'n_estimators': 1200, 'max_depth': 5, 'min_samples_split': 20, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'bootstrap': True, 'class_weight': 'balanced'}. Best is trial 1 with value: 0.68454806312769.\n",
      "[I 2025-04-24 12:50:50,550] Trial 8 finished with value: 0.6748158775705404 and parameters: {'n_estimators': 1800, 'max_depth': 20, 'min_samples_split': 19, 'min_samples_leaf': 9, 'max_features': 'log2', 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 1 with value: 0.68454806312769.\n",
      "[I 2025-04-24 12:50:51,181] Trial 9 finished with value: 0.6914060258249641 and parameters: {'n_estimators': 100, 'max_depth': 35, 'min_samples_split': 17, 'min_samples_leaf': 7, 'max_features': 0.6, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 9 with value: 0.6914060258249641.\n",
      "[I 2025-04-24 12:50:52,319] Trial 10 finished with value: 0.7314490674318508 and parameters: {'n_estimators': 200, 'max_depth': 35, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 10 with value: 0.7314490674318508.\n",
      "[I 2025-04-24 12:50:53,021] Trial 11 finished with value: 0.7265662362505978 and parameters: {'n_estimators': 100, 'max_depth': 35, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 10 with value: 0.7314490674318508.\n",
      "[I 2025-04-24 12:50:53,726] Trial 12 finished with value: 0.7246197991391679 and parameters: {'n_estimators': 100, 'max_depth': 35, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 10 with value: 0.7314490674318508.\n",
      "[I 2025-04-24 12:50:55,662] Trial 13 finished with value: 0.7255906264945002 and parameters: {'n_estimators': 400, 'max_depth': 35, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 10 with value: 0.7314490674318508.\n",
      "[I 2025-04-24 12:50:57,196] Trial 14 finished with value: 0.7236346245815399 and parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 10 with value: 0.7314490674318508.\n",
      "[I 2025-04-24 12:50:57,860] Trial 15 finished with value: 0.675791487326638 and parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 16, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 10 with value: 0.7314490674318508.\n",
      "[I 2025-04-24 12:51:00,497] Trial 16 finished with value: 0.713868962219034 and parameters: {'n_estimators': 600, 'max_depth': 40, 'min_samples_split': 13, 'min_samples_leaf': 4, 'max_features': 0.8, 'bootstrap': False, 'class_weight': None}. Best is trial 10 with value: 0.7314490674318508.\n",
      "[I 2025-04-24 12:51:02,156] Trial 17 finished with value: 0.7324246771879483 and parameters: {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:08,875] Trial 18 finished with value: 0.708000956480153 and parameters: {'n_estimators': 900, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 0.8, 'bootstrap': True, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:13,518] Trial 19 finished with value: 0.6679818268770923 and parameters: {'n_estimators': 1100, 'max_depth': 5, 'min_samples_split': 16, 'min_samples_leaf': 3, 'max_features': 0.8, 'bootstrap': False, 'class_weight': None}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:15,009] Trial 20 finished with value: 0.6767814442850311 and parameters: {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 9, 'max_features': 'log2', 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:16,588] Trial 21 finished with value: 0.7285174557627929 and parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:18,551] Trial 22 finished with value: 0.7246150167384027 and parameters: {'n_estimators': 400, 'max_depth': 25, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:20,128] Trial 23 finished with value: 0.7285126733620277 and parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:23,013] Trial 24 finished with value: 0.7060640841702535 and parameters: {'n_estimators': 600, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:25,434] Trial 25 finished with value: 0.7314395026303204 and parameters: {'n_estimators': 500, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:29,361] Trial 26 finished with value: 0.6659923481587757 and parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 15, 'max_features': 0.8, 'bootstrap': True, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:34,127] Trial 27 finished with value: 0.7089813486370158 and parameters: {'n_estimators': 1000, 'max_depth': 15, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:35,104] Trial 28 finished with value: 0.6728742228598756 and parameters: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False, 'class_weight': None}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:37,312] Trial 29 finished with value: 0.7001865136298422 and parameters: {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 0.6, 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:39,646] Trial 30 finished with value: 0.7080057388809182 and parameters: {'n_estimators': 500, 'max_depth': 15, 'min_samples_split': 14, 'min_samples_leaf': 7, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:41,355] Trial 31 finished with value: 0.7285174557627929 and parameters: {'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 17 with value: 0.7324246771879483.\n",
      "[I 2025-04-24 12:51:44,849] Trial 32 finished with value: 0.7334002869440459 and parameters: {'n_estimators': 700, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 32 with value: 0.7334002869440459.\n",
      "[I 2025-04-24 12:51:49,895] Trial 33 finished with value: 0.7275418460066954 and parameters: {'n_estimators': 700, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 32 with value: 0.7334002869440459.\n",
      "[I 2025-04-24 12:51:54,333] Trial 34 finished with value: 0.727532281205165 and parameters: {'n_estimators': 600, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 32 with value: 0.7334002869440459.\n",
      "[I 2025-04-24 12:51:58,934] Trial 35 finished with value: 0.7236298421807748 and parameters: {'n_estimators': 800, 'max_depth': 25, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False, 'class_weight': None}. Best is trial 32 with value: 0.7334002869440459.\n",
      "[I 2025-04-24 12:52:19,904] Trial 36 finished with value: 0.6982400765184122 and parameters: {'n_estimators': 700, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 0.6, 'bootstrap': False, 'class_weight': 'balanced_subsample'}. Best is trial 32 with value: 0.7334002869440459.\n",
      "[I 2025-04-24 12:52:34,656] Trial 37 finished with value: 0.6992156862745098 and parameters: {'n_estimators': 1300, 'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': True, 'class_weight': 'balanced_subsample'}. Best is trial 32 with value: 0.7334002869440459.\n",
      "[I 2025-04-24 12:52:38,826] Trial 38 finished with value: 0.6806695361071258 and parameters: {'n_estimators': 900, 'max_depth': 40, 'min_samples_split': 5, 'min_samples_leaf': 14, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced'}. Best is trial 32 with value: 0.7334002869440459.\n",
      "[I 2025-04-24 12:52:42,279] Trial 39 finished with value: 0.6777379244380679 and parameters: {'n_estimators': 400, 'max_depth': 20, 'min_samples_split': 25, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': True, 'class_weight': 'balanced_subsample'}. Best is trial 32 with value: 0.7334002869440459.\n",
      "2025-04-24 12:52:42,289 - INFO - Optimization complete for extratrees.\n",
      "2025-04-24 12:52:42,290 - INFO - Best CV score: 0.73340\n",
      "2025-04-24 12:52:42,290 - INFO - Best params: {'n_estimators': 700, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 0.8, 'bootstrap': False, 'class_weight': 'balanced_subsample'}\n",
      "2025-04-24 12:52:42,291 - INFO - Saved Optuna summary: optuna_trials/extratrees_study_summary_20250424_121903.txt\n",
      "2025-04-24 12:52:42,292 - INFO - Instantiating final extratrees model...\n",
      "2025-04-24 12:52:42,292 - ERROR - Failed final instantiate/fit/save process extratrees: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 705, in optimize_model\n",
      "    if 'class_weight' not in final_p_log: final_p_log['class_weight'] = 'balanced'; final_model = LogisticRegression(**final_p_log)\n",
      "                             ^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "2025-04-24 12:52:42,295 - INFO - --- NOT QUALIFIED: extratrees (CV Score: 0.73340  - Final fit/save failed) ---\n",
      "2025-04-24 12:52:42,296 - INFO - --- Optimizing gradientboosting ---\n",
      "2025-04-24 12:52:42,297 - INFO - Starting gradientboosting optimization (40 trials)...\n",
      "2025-04-24 12:52:42,298 - INFO - Optuna timeout for gradientboosting: 5400s.\n",
      "[I 2025-04-24 12:52:42,501] A new study created in RDB with name: gradientboosting_opt_20250424_121903\n",
      "2025-04-24 12:52:42,508 - INFO - Setting Optuna timeout for gradientboosting to 5400 seconds.\n",
      "[I 2025-04-24 12:52:52,135] Trial 0 finished with value: 0.7099617407938785 and parameters: {'n_estimators': 200, 'learning_rate': 0.1325179230463795, 'max_depth': 10, 'min_samples_split': 21, 'min_samples_leaf': 4, 'subsample': 0.7147026176663781, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.7099617407938785.\n",
      "[I 2025-04-24 12:53:45,562] Trial 1 finished with value: 0.7167910090865615 and parameters: {'n_estimators': 1200, 'learning_rate': 0.02098734322836931, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 2, 'subsample': 0.8615111370398629, 'max_features': 'log2'}. Best is trial 1 with value: 0.7167910090865615.\n",
      "[I 2025-04-24 12:54:12,238] Trial 2 finished with value: 0.7109469153515064 and parameters: {'n_estimators': 1300, 'learning_rate': 0.16896940245271658, 'max_depth': 7, 'min_samples_split': 16, 'min_samples_leaf': 15, 'subsample': 0.976492615472413, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7167910090865615.\n",
      "[I 2025-04-24 12:56:32,795] Trial 3 finished with value: 0.7119177427068388 and parameters: {'n_estimators': 1000, 'learning_rate': 0.08617633648489356, 'max_depth': 6, 'min_samples_split': 25, 'min_samples_leaf': 9, 'subsample': 0.8640026392853541, 'max_features': None}. Best is trial 1 with value: 0.7167910090865615.\n",
      "[I 2025-04-24 12:57:22,804] Trial 4 finished with value: 0.7158393113342898 and parameters: {'n_estimators': 1100, 'learning_rate': 0.009204512567633551, 'max_depth': 8, 'min_samples_split': 24, 'min_samples_leaf': 12, 'subsample': 0.9877050530805004, 'max_features': 'log2'}. Best is trial 1 with value: 0.7167910090865615.\n",
      "[I 2025-04-24 12:57:46,260] Trial 5 finished with value: 0.7080248684839789 and parameters: {'n_estimators': 1400, 'learning_rate': 0.17184295213085585, 'max_depth': 3, 'min_samples_split': 19, 'min_samples_leaf': 14, 'subsample': 0.6595703772310546, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7167910090865615.\n",
      "[I 2025-04-24 12:58:05,303] Trial 6 finished with value: 0.7207173601147776 and parameters: {'n_estimators': 100, 'learning_rate': 0.19980382997768778, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 10, 'subsample': 0.9552517046926221, 'max_features': None}. Best is trial 6 with value: 0.7207173601147776.\n",
      "[I 2025-04-24 12:58:22,678] Trial 7 finished with value: 0.7050980392156863 and parameters: {'n_estimators': 800, 'learning_rate': 0.18570749815598075, 'max_depth': 9, 'min_samples_split': 18, 'min_samples_leaf': 3, 'subsample': 0.5708399933238084, 'max_features': 'sqrt'}. Best is trial 6 with value: 0.7207173601147776.\n",
      "[I 2025-04-24 12:58:49,854] Trial 8 finished with value: 0.6933620277379244 and parameters: {'n_estimators': 1100, 'learning_rate': 0.15829208531909356, 'max_depth': 10, 'min_samples_split': 15, 'min_samples_leaf': 18, 'subsample': 0.6396488712927129, 'max_features': 'sqrt'}. Best is trial 6 with value: 0.7207173601147776.\n",
      "[I 2025-04-24 12:59:04,663] Trial 9 finished with value: 0.723663318986131 and parameters: {'n_estimators': 400, 'learning_rate': 0.01843514240438013, 'max_depth': 7, 'min_samples_split': 20, 'min_samples_leaf': 2, 'subsample': 0.7328225225297286, 'max_features': 'log2'}. Best is trial 9 with value: 0.723663318986131.\n",
      "[I 2025-04-24 12:59:24,547] Trial 10 finished with value: 0.7070444763271162 and parameters: {'n_estimators': 500, 'learning_rate': 0.00568031122568491, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 7, 'subsample': 0.5120544997982305, 'max_features': 'log2'}. Best is trial 9 with value: 0.723663318986131.\n",
      "[I 2025-04-24 12:59:35,457] Trial 11 finished with value: 0.7178000956480153 and parameters: {'n_estimators': 100, 'learning_rate': 0.039584978653428964, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 6, 'subsample': 0.8166173304017801, 'max_features': None}. Best is trial 9 with value: 0.723663318986131.\n",
      "[I 2025-04-24 13:01:04,233] Trial 12 finished with value: 0.7265949306551889 and parameters: {'n_estimators': 500, 'learning_rate': 0.028793834255426174, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 10, 'subsample': 0.7468382720496426, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:01:23,459] Trial 13 finished with value: 0.7011908177905308 and parameters: {'n_estimators': 500, 'learning_rate': 0.02452041917979934, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 20, 'subsample': 0.762061697635761, 'max_features': 'log2'}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:01:57,973] Trial 14 finished with value: 0.7197321855571497 and parameters: {'n_estimators': 500, 'learning_rate': 0.048458103171279146, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 1, 'subsample': 0.7244654253966442, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:02:16,005] Trial 15 finished with value: 0.7138833094213295 and parameters: {'n_estimators': 700, 'learning_rate': 0.013329975966582298, 'max_depth': 5, 'min_samples_split': 21, 'min_samples_leaf': 6, 'subsample': 0.7901534590442103, 'max_features': 'log2'}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:03:04,548] Trial 16 finished with value: 0.709014825442372 and parameters: {'n_estimators': 300, 'learning_rate': 0.06613382014784282, 'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 13, 'subsample': 0.6573671318402361, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:03:25,939] Trial 17 finished with value: 0.7148732663797226 and parameters: {'n_estimators': 700, 'learning_rate': 0.024425956372452876, 'max_depth': 5, 'min_samples_split': 13, 'min_samples_leaf': 16, 'subsample': 0.8771926473978255, 'max_features': 'log2'}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:04:16,011] Trial 18 finished with value: 0.7197560975609756 and parameters: {'n_estimators': 400, 'learning_rate': 0.014599144638385872, 'max_depth': 8, 'min_samples_split': 13, 'min_samples_leaf': 8, 'subsample': 0.5985118783112966, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:04:38,369] Trial 19 finished with value: 0.7070779531324725 and parameters: {'n_estimators': 800, 'learning_rate': 0.0067681260294023965, 'max_depth': 6, 'min_samples_split': 21, 'min_samples_leaf': 11, 'subsample': 0.7300472080840503, 'max_features': 'log2'}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:05:52,151] Trial 20 finished with value: 0.7080439980870397 and parameters: {'n_estimators': 600, 'learning_rate': 0.03201282240093138, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 17, 'subsample': 0.6896636103527709, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:06:11,544] Trial 21 finished with value: 0.715844093735055 and parameters: {'n_estimators': 100, 'learning_rate': 0.0663484152082804, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 9, 'subsample': 0.9174925084420636, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:06:46,967] Trial 22 finished with value: 0.708986131037781 and parameters: {'n_estimators': 300, 'learning_rate': 0.27000537470537345, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 10, 'subsample': 0.8222139763663521, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:08:13,550] Trial 23 finished with value: 0.7168292682926829 and parameters: {'n_estimators': 300, 'learning_rate': 0.014718617698626639, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 5, 'subsample': 0.7743538231451893, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:10:34,702] Trial 24 finished with value: 0.7187422285987566 and parameters: {'n_estimators': 200, 'learning_rate': 0.103983395410556, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 11, 'subsample': 0.814082738572845, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:14:34,655] Trial 25 finished with value: 0.7197465327594452 and parameters: {'n_estimators': 400, 'learning_rate': 0.00978672262809619, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 13, 'subsample': 0.9205154204000322, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:15:11,206] Trial 26 finished with value: 0.7080296508847442 and parameters: {'n_estimators': 200, 'learning_rate': 0.28509240752160964, 'max_depth': 6, 'min_samples_split': 11, 'min_samples_leaf': 8, 'subsample': 0.7640887878911875, 'max_features': 'log2'}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:20:31,600] Trial 27 finished with value: 0.715844093735055 and parameters: {'n_estimators': 400, 'learning_rate': 0.03383987283695465, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 4, 'subsample': 0.6954294713291054, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:23:29,798] Trial 28 finished with value: 0.7256001912960306 and parameters: {'n_estimators': 600, 'learning_rate': 0.019557164688615788, 'max_depth': 7, 'min_samples_split': 23, 'min_samples_leaf': 10, 'subsample': 0.9292877858661255, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:25:28,007] Trial 29 finished with value: 0.7178000956480153 and parameters: {'n_estimators': 900, 'learning_rate': 0.020550267720097916, 'max_depth': 6, 'min_samples_split': 23, 'min_samples_leaf': 4, 'subsample': 0.7000188586608618, 'max_features': 'log2'}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:26:19,334] Trial 30 finished with value: 0.6806886657101866 and parameters: {'n_estimators': 600, 'learning_rate': 0.010857844852908825, 'max_depth': 4, 'min_samples_split': 21, 'min_samples_leaf': 19, 'subsample': 0.6124867820797593, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:32:18,811] Trial 31 finished with value: 0.7197513151602104 and parameters: {'n_estimators': 600, 'learning_rate': 0.01811840772495135, 'max_depth': 7, 'min_samples_split': 19, 'min_samples_leaf': 10, 'subsample': 0.9401162921753788, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:32:49,311] Trial 32 finished with value: 0.7197513151602104 and parameters: {'n_estimators': 200, 'learning_rate': 0.04752884584366976, 'max_depth': 7, 'min_samples_split': 23, 'min_samples_leaf': 12, 'subsample': 0.8998004272532729, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:37:39,095] Trial 33 finished with value: 0.7236346245815399 and parameters: {'n_estimators': 700, 'learning_rate': 0.02874941047111014, 'max_depth': 8, 'min_samples_split': 17, 'min_samples_leaf': 1, 'subsample': 0.9558447122548186, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:41:35,097] Trial 34 finished with value: 0.7255906264945002 and parameters: {'n_estimators': 700, 'learning_rate': 0.027076778297159345, 'max_depth': 7, 'min_samples_split': 17, 'min_samples_leaf': 1, 'subsample': 0.9587337274832836, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:47:40,869] Trial 35 finished with value: 0.7236394069823051 and parameters: {'n_estimators': 900, 'learning_rate': 0.01855770960726437, 'max_depth': 7, 'min_samples_split': 15, 'min_samples_leaf': 3, 'subsample': 0.846413292249649, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:49:03,297] Trial 36 finished with value: 0.7109564801530368 and parameters: {'n_estimators': 600, 'learning_rate': 0.02567207398851585, 'max_depth': 6, 'min_samples_split': 25, 'min_samples_leaf': 2, 'subsample': 0.9825477105147685, 'max_features': 'log2'}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:53:00,947] Trial 37 finished with value: 0.7168149210903874 and parameters: {'n_estimators': 1500, 'learning_rate': 0.047486076823419106, 'max_depth': 7, 'min_samples_split': 20, 'min_samples_leaf': 2, 'subsample': 0.9991357759016799, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:58:05,096] Trial 38 finished with value: 0.7187709230033477 and parameters: {'n_estimators': 800, 'learning_rate': 0.012568887949443237, 'max_depth': 8, 'min_samples_split': 23, 'min_samples_leaf': 15, 'subsample': 0.8478717398793713, 'max_features': None}. Best is trial 12 with value: 0.7265949306551889.\n",
      "[I 2025-04-24 13:58:33,813] Trial 39 finished with value: 0.7168149210903874 and parameters: {'n_estimators': 500, 'learning_rate': 0.017356160645519723, 'max_depth': 7, 'min_samples_split': 17, 'min_samples_leaf': 5, 'subsample': 0.8924938965622176, 'max_features': 'log2'}. Best is trial 12 with value: 0.7265949306551889.\n",
      "2025-04-24 13:58:33,820 - INFO - Optimization complete for gradientboosting.\n",
      "2025-04-24 13:58:33,822 - INFO - Best CV score: 0.72659\n",
      "2025-04-24 13:58:33,822 - INFO - Best params: {'n_estimators': 500, 'learning_rate': 0.028793834255426174, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 10, 'subsample': 0.7468382720496426, 'max_features': None}\n",
      "2025-04-24 13:58:33,822 - INFO - Saved Optuna summary: optuna_trials/gradientboosting_study_summary_20250424_121903.txt\n",
      "2025-04-24 13:58:33,822 - INFO - Instantiating final gradientboosting model...\n",
      "2025-04-24 13:58:33,822 - ERROR - Failed final instantiate/fit/save process gradientboosting: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 705, in optimize_model\n",
      "    if 'class_weight' not in final_p_log: final_p_log['class_weight'] = 'balanced'; final_model = LogisticRegression(**final_p_log)\n",
      "                             ^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "2025-04-24 13:58:33,827 - INFO - --- NOT QUALIFIED: gradientboosting (CV Score: 0.72659  - Final fit/save failed) ---\n",
      "2025-04-24 13:58:33,828 - INFO - --- Optimizing mlp ---\n",
      "2025-04-24 13:58:33,829 - INFO - Starting mlp optimization (25 trials)...\n",
      "2025-04-24 13:58:33,829 - INFO - Optuna timeout for mlp: 5400s.\n",
      "[I 2025-04-24 13:58:34,028] A new study created in RDB with name: mlp_opt_20250424_121903\n",
      "2025-04-24 13:58:34,044 - INFO - Setting Optuna timeout for mlp to 5400 seconds.\n",
      "[I 2025-04-24 13:58:38,965] Trial 0 finished with value: 0.6592061214729794 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.002905795585232086, 'learning_rate_init': 0.0001344347438032115, 'max_iter': 798, 'batch_size': 256}. Best is trial 0 with value: 0.6592061214729794.\n",
      "[I 2025-04-24 13:58:40,452] Trial 1 finished with value: 0.6132759445241511 and parameters: {'hidden_layer_sizes': [50, 50], 'activation': 'relu', 'solver': 'adam', 'alpha': 9.419229178261498e-05, 'learning_rate_init': 0.00017898530114000182, 'max_iter': 1155, 'batch_size': 256}. Best is trial 0 with value: 0.6592061214729794.\n",
      "[I 2025-04-24 13:58:42,440] Trial 2 finished with value: 0.6689574366331899 and parameters: {'hidden_layer_sizes': [128, 64], 'activation': 'relu', 'solver': 'adam', 'alpha': 0.009110603226745772, 'learning_rate_init': 0.003839935046239689, 'max_iter': 457, 'batch_size': 128}. Best is trial 2 with value: 0.6689574366331899.\n",
      "[I 2025-04-24 13:58:43,499] Trial 3 finished with value: 0.649402199904352 and parameters: {'hidden_layer_sizes': [50, 50], 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0066734656911634676, 'learning_rate_init': 0.0005950557726250009, 'max_iter': 1108, 'batch_size': 128}. Best is trial 2 with value: 0.6689574366331899.\n",
      "[I 2025-04-24 13:58:46,582] Trial 4 finished with value: 0.6133524629363942 and parameters: {'hidden_layer_sizes': [100, 50], 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0002068812876517367, 'learning_rate_init': 0.0001263987600761374, 'max_iter': 537, 'batch_size': 256}. Best is trial 2 with value: 0.6689574366331899.\n",
      "[I 2025-04-24 13:58:47,675] Trial 5 finished with value: 0.6308799617407939 and parameters: {'hidden_layer_sizes': [100], 'activation': 'relu', 'solver': 'adam', 'alpha': 0.005678940572431525, 'learning_rate_init': 0.0001791394060968039, 'max_iter': 880, 'batch_size': 128}. Best is trial 2 with value: 0.6689574366331899.\n",
      "[I 2025-04-24 13:58:48,271] Trial 6 finished with value: 0.6767862266857962 and parameters: {'hidden_layer_sizes': [50, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.005009436804699656, 'learning_rate_init': 0.005326908940626226, 'max_iter': 832, 'batch_size': 128}. Best is trial 6 with value: 0.6767862266857962.\n",
      "[I 2025-04-24 13:58:51,235] Trial 7 finished with value: 0.6582257293161167 and parameters: {'hidden_layer_sizes': [128, 64], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.9043404016215215e-05, 'learning_rate_init': 0.0005560891453088472, 'max_iter': 966, 'batch_size': 256}. Best is trial 6 with value: 0.6767862266857962.\n",
      "[I 2025-04-24 13:58:52,153] Trial 8 finished with value: 0.6337541846006695 and parameters: {'hidden_layer_sizes': [100], 'activation': 'tanh', 'solver': 'adam', 'alpha': 6.0318444504376455e-06, 'learning_rate_init': 0.00018922831618733456, 'max_iter': 425, 'batch_size': 64}. Best is trial 6 with value: 0.6767862266857962.\n",
      "[I 2025-04-24 13:58:55,468] Trial 9 finished with value: 0.6611525585844094 and parameters: {'hidden_layer_sizes': [128, 64], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0016583634198177402, 'learning_rate_init': 0.0012108725923148636, 'max_iter': 1028, 'batch_size': 256}. Best is trial 6 with value: 0.6767862266857962.\n",
      "[I 2025-04-24 13:58:56,173] Trial 10 finished with value: 0.6777331420373027 and parameters: {'hidden_layer_sizes': [64, 32, 16], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.1604787869341097e-06, 'learning_rate_init': 0.009926377540804365, 'max_iter': 720, 'batch_size': 64}. Best is trial 10 with value: 0.6777331420373027.\n",
      "[I 2025-04-24 13:58:56,992] Trial 11 finished with value: 0.6659636537541845 and parameters: {'hidden_layer_sizes': [64, 32, 16], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.3545076327899601e-06, 'learning_rate_init': 0.008321663766087851, 'max_iter': 681, 'batch_size': 64}. Best is trial 10 with value: 0.6777331420373027.\n",
      "[I 2025-04-24 13:58:57,660] Trial 12 finished with value: 0.6708990913438545 and parameters: {'hidden_layer_sizes': [64, 32, 16], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0007378887870683631, 'learning_rate_init': 0.00979644985527043, 'max_iter': 667, 'batch_size': 64}. Best is trial 10 with value: 0.6777331420373027.\n",
      "[I 2025-04-24 13:58:58,653] Trial 13 finished with value: 0.6474461979913917 and parameters: {'hidden_layer_sizes': [64, 32, 16], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.1070235499003907e-06, 'learning_rate_init': 0.002767656187488167, 'max_iter': 745, 'batch_size': 64}. Best is trial 10 with value: 0.6777331420373027.\n",
      "[I 2025-04-24 13:58:59,235] Trial 14 finished with value: 0.680674318507891 and parameters: {'hidden_layer_sizes': [50, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 2.7384289159570817e-05, 'learning_rate_init': 0.0044804075064812715, 'max_iter': 891, 'batch_size': 128}. Best is trial 14 with value: 0.680674318507891.\n",
      "[I 2025-04-24 13:59:01,484] Trial 15 finished with value: 0.6777283596365374 and parameters: {'hidden_layer_sizes': [100, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.1751996165583309e-05, 'learning_rate_init': 0.00206333814721052, 'max_iter': 597, 'batch_size': 128}. Best is trial 14 with value: 0.680674318507891.\n",
      "[I 2025-04-24 13:59:05,249] Trial 16 finished with value: 0.6611573409851745 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 5.091655476208006e-05, 'learning_rate_init': 0.0068351608055769245, 'max_iter': 893, 'batch_size': 64}. Best is trial 14 with value: 0.680674318507891.\n",
      "[I 2025-04-24 13:59:06,039] Trial 17 finished with value: 0.662080344332855 and parameters: {'hidden_layer_sizes': [50, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 3.552889308039751e-06, 'learning_rate_init': 0.001723874884816936, 'max_iter': 985, 'batch_size': 128}. Best is trial 14 with value: 0.680674318507891.\n",
      "[I 2025-04-24 13:59:06,992] Trial 18 finished with value: 0.6367049258727882 and parameters: {'hidden_layer_sizes': [64, 32, 16], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00032978007383944563, 'learning_rate_init': 0.00415592060739176, 'max_iter': 642, 'batch_size': 64}. Best is trial 14 with value: 0.680674318507891.\n",
      "[I 2025-04-24 13:59:07,622] Trial 19 finished with value: 0.6748158775705404 and parameters: {'hidden_layer_sizes': [50, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 3.247121002344147e-05, 'learning_rate_init': 0.0033835668614954155, 'max_iter': 751, 'batch_size': 128}. Best is trial 14 with value: 0.680674318507891.\n",
      "[I 2025-04-24 13:59:08,994] Trial 20 finished with value: 0.657197513151602 and parameters: {'hidden_layer_sizes': [64, 32, 16], 'activation': 'tanh', 'solver': 'adam', 'alpha': 3.4703492775219328e-06, 'learning_rate_init': 0.0006935275212710813, 'max_iter': 543, 'batch_size': 64}. Best is trial 14 with value: 0.680674318507891.\n",
      "[I 2025-04-24 13:59:10,972] Trial 21 finished with value: 0.6777044476327115 and parameters: {'hidden_layer_sizes': [100, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.1329743249350796e-05, 'learning_rate_init': 0.002230231008668808, 'max_iter': 593, 'batch_size': 128}. Best is trial 14 with value: 0.680674318507891.\n",
      "[I 2025-04-24 13:59:12,200] Trial 22 finished with value: 0.666001912960306 and parameters: {'hidden_layer_sizes': [100, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 9.4682662355032e-06, 'learning_rate_init': 0.0058668611774234354, 'max_iter': 724, 'batch_size': 128}. Best is trial 14 with value: 0.680674318507891.\n",
      "[I 2025-04-24 13:59:13,691] Trial 23 finished with value: 0.669899569583931 and parameters: {'hidden_layer_sizes': [100, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 3.018037990145469e-06, 'learning_rate_init': 0.001580611784113509, 'max_iter': 887, 'batch_size': 128}. Best is trial 14 with value: 0.680674318507891.\n",
      "[I 2025-04-24 13:59:15,260] Trial 24 finished with value: 0.662080344332855 and parameters: {'hidden_layer_sizes': [100, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 2.9594395425202956e-05, 'learning_rate_init': 0.0055611493338997686, 'max_iter': 515, 'batch_size': 128}. Best is trial 14 with value: 0.680674318507891.\n",
      "2025-04-24 13:59:15,271 - INFO - Optimization complete for mlp.\n",
      "2025-04-24 13:59:15,271 - INFO - Best CV score: 0.68067\n",
      "2025-04-24 13:59:15,271 - INFO - Best params: {'hidden_layer_sizes': [50, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 2.7384289159570817e-05, 'learning_rate_init': 0.0044804075064812715, 'max_iter': 891, 'batch_size': 128}\n",
      "2025-04-24 13:59:15,278 - INFO - Saved Optuna summary: optuna_trials/mlp_study_summary_20250424_121903.txt\n",
      "2025-04-24 13:59:15,280 - INFO - Instantiating final mlp model...\n",
      "2025-04-24 13:59:15,282 - ERROR - Failed final instantiate/fit/save process mlp: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 705, in optimize_model\n",
      "    if 'class_weight' not in final_p_log: final_p_log['class_weight'] = 'balanced'; final_model = LogisticRegression(**final_p_log)\n",
      "                             ^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "2025-04-24 13:59:15,283 - INFO - --- NOT QUALIFIED: mlp (CV Score: 0.68067  - Final fit/save failed) ---\n",
      "2025-04-24 13:59:15,284 - INFO - --- Optimizing keras_mlp ---\n",
      "2025-04-24 13:59:15,285 - INFO - Starting keras_mlp optimization (20 trials)...\n",
      "2025-04-24 13:59:15,288 - INFO - Optuna timeout for keras_mlp: 5400s.\n",
      "[I 2025-04-24 13:59:15,432] A new study created in RDB with name: keras_mlp_opt_20250424_121903\n",
      "2025-04-24 13:59:15,432 - INFO - Setting Optuna timeout for keras_mlp to 5400 seconds.\n",
      "[W 2025-04-24 13:59:15,499] Trial 0 failed with parameters: {'optimizer': 'adam', 'learning_rate': 0.0004206153385345179, 'dropout_rate': 0.4909937324994026, 'activation': 'relu', 'n_layers': 2} because of the following error: ValueError('Samplers and other components in Optuna only accept step is 1 when `log` argument is True.').\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 570, in objective\n",
      "    for i in range(n_layers): max_units = max(32, int(last_units / 1.5)); min_units = max(16, int(last_units / 4)); units = trial.suggest_int(f'n_units_l{i}', min_units, max_units, step=4, log=True); units_list.append(units); last_units = units\n",
      "                                                                                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\_convert_positional_args.py\", line 134, in converter_wrapper\n",
      "    return func(**kwargs)  # type: ignore[call-arg]\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\trial\\_trial.py\", line 323, in suggest_int\n",
      "    distribution = IntDistribution(low=low, high=high, log=log, step=step)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\distributions.py\", line 347, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Samplers and other components in Optuna only accept step is 1 when `log` argument is True.\n",
      "[W 2025-04-24 13:59:15,569] Trial 0 failed with value None.\n",
      "2025-04-24 13:59:15,569 - ERROR - Optuna optimize call failed for keras_mlp: Samplers and other components in Optuna only accept step is 1 when `log` argument is True.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 626, in optimize_model\n",
      "    study.optimize(objective, n_trials=trials_to_run, timeout=OPTUNA_TIMEOUT_PER_MODEL, n_jobs=1) # Use n_jobs=1 for stability\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\study\\study.py\", line 475, in optimize\n",
      "    _optimize(\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 63, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 160, in _optimize_sequential\n",
      "    frozen_trial = _run_trial(study, func, catch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 248, in _run_trial\n",
      "    raise func_err\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 570, in objective\n",
      "    for i in range(n_layers): max_units = max(32, int(last_units / 1.5)); min_units = max(16, int(last_units / 4)); units = trial.suggest_int(f'n_units_l{i}', min_units, max_units, step=4, log=True); units_list.append(units); last_units = units\n",
      "                                                                                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\_convert_positional_args.py\", line 134, in converter_wrapper\n",
      "    return func(**kwargs)  # type: ignore[call-arg]\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\trial\\_trial.py\", line 323, in suggest_int\n",
      "    distribution = IntDistribution(low=low, high=high, log=log, step=step)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\optuna\\distributions.py\", line 347, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Samplers and other components in Optuna only accept step is 1 when `log` argument is True.\n",
      "2025-04-24 13:59:15,584 - INFO - --- NOT QUALIFIED: keras_mlp (CV Score: -1.00000  - Final fit/save failed) ---\n",
      "2025-04-24 13:59:15,584 - INFO - --- Optimizing catboost ---\n",
      "2025-04-24 13:59:15,584 - INFO - Starting catboost optimization (40 trials)...\n",
      "2025-04-24 13:59:15,600 - INFO - Optuna timeout for catboost: 5400s.\n",
      "[I 2025-04-24 13:59:15,759] A new study created in RDB with name: catboost_opt_20250424_121903\n",
      "2025-04-24 13:59:15,762 - INFO - Setting Optuna timeout for catboost to 5400 seconds.\n",
      "[I 2025-04-24 13:59:19,988] Trial 0 finished with value: 0.686547106647537 and parameters: {'task_type': 'GPU', 'iterations': 1100, 'depth': 4, 'learning_rate': 0.04894278107956352, 'l2_leaf_reg': 5.387854299009571, 'random_strength': 0.38455602629333163, 'border_count': 32, 'bagging_temperature': 0.6608945490190934}. Best is trial 0 with value: 0.686547106647537.\n",
      "[I 2025-04-24 14:00:02,974] Trial 1 finished with value: 0.6572022955523672 and parameters: {'task_type': 'GPU', 'iterations': 1100, 'depth': 14, 'learning_rate': 0.006471655625802204, 'l2_leaf_reg': 3.6688643055446506, 'random_strength': 0.009894201556923853, 'border_count': 254, 'bagging_temperature': 0.40856830742879424}. Best is trial 0 with value: 0.686547106647537.\n",
      "[I 2025-04-24 14:00:03,516] Trial 2 finished with value: 0.6611477761836442 and parameters: {'task_type': 'CPU', 'iterations': 700, 'depth': 5, 'learning_rate': 0.00758630229579071, 'l2_leaf_reg': 2.0834995859724668, 'random_strength': 0.24156223709924915, 'border_count': 32, 'bagging_temperature': 0.2852328524701172}. Best is trial 0 with value: 0.686547106647537.\n",
      "[I 2025-04-24 14:00:06,044] Trial 3 finished with value: 0.7158201817312291 and parameters: {'task_type': 'GPU', 'iterations': 2000, 'depth': 6, 'learning_rate': 0.15986525070621638, 'l2_leaf_reg': 2.186666429600333, 'random_strength': 0.02105281745587202, 'border_count': 254, 'bagging_temperature': 0.9206223304430614}. Best is trial 3 with value: 0.7158201817312291.\n",
      "[I 2025-04-24 14:00:08,962] Trial 4 finished with value: 0.699220468675275 and parameters: {'task_type': 'CPU', 'iterations': 1700, 'depth': 10, 'learning_rate': 0.04048984534592204, 'l2_leaf_reg': 1.2836904565913443, 'random_strength': 0.007161908631476705, 'border_count': 32, 'bagging_temperature': 0.32529863460773933}. Best is trial 3 with value: 0.7158201817312291.\n",
      "[I 2025-04-24 14:00:11,487] Trial 5 finished with value: 0.6816690578670493 and parameters: {'task_type': 'CPU', 'iterations': 700, 'depth': 10, 'learning_rate': 0.00991107418673253, 'l2_leaf_reg': 1.1684695027010097, 'random_strength': 0.03568136924275534, 'border_count': 32, 'bagging_temperature': 0.1863485768928549}. Best is trial 3 with value: 0.7158201817312291.\n",
      "[I 2025-04-24 14:00:41,993] Trial 6 finished with value: 0.6806408417025346 and parameters: {'task_type': 'CPU', 'iterations': 1300, 'depth': 12, 'learning_rate': 0.035212689338061924, 'l2_leaf_reg': 10.912934631574338, 'random_strength': 0.055557096961961996, 'border_count': 128, 'bagging_temperature': 0.9773763084454457}. Best is trial 3 with value: 0.7158201817312291.\n",
      "[I 2025-04-24 14:00:43,786] Trial 7 finished with value: 0.6836250597800096 and parameters: {'task_type': 'CPU', 'iterations': 1400, 'depth': 8, 'learning_rate': 0.18002333404140772, 'l2_leaf_reg': 7.542666659454184, 'random_strength': 2.8481270872121702, 'border_count': 64, 'bagging_temperature': 0.7153546821319919}. Best is trial 3 with value: 0.7158201817312291.\n",
      "[I 2025-04-24 14:00:46,716] Trial 8 finished with value: 0.64649450023912 and parameters: {'task_type': 'GPU', 'iterations': 1400, 'depth': 10, 'learning_rate': 0.009376196615092647, 'l2_leaf_reg': 1.5133211070738084, 'random_strength': 7.591717507847898, 'border_count': 128, 'bagging_temperature': 0.3481512387610668}. Best is trial 3 with value: 0.7158201817312291.\n",
      "[I 2025-04-24 14:01:05,508] Trial 9 finished with value: 0.6884887613582018 and parameters: {'task_type': 'GPU', 'iterations': 500, 'depth': 13, 'learning_rate': 0.021395200574772164, 'l2_leaf_reg': 5.984030257062217, 'random_strength': 0.011691214103099775, 'border_count': 64, 'bagging_temperature': 0.8196126483005396}. Best is trial 3 with value: 0.7158201817312291.\n",
      "[I 2025-04-24 14:01:08,592] Trial 10 finished with value: 0.7119129603060736 and parameters: {'task_type': 'GPU', 'iterations': 2000, 'depth': 7, 'learning_rate': 0.2655101549361598, 'l2_leaf_reg': 18.107106301352882, 'random_strength': 0.0026068799041630065, 'border_count': 254, 'bagging_temperature': 0.5711777498488295}. Best is trial 3 with value: 0.7158201817312291.\n",
      "[I 2025-04-24 14:01:11,202] Trial 11 finished with value: 0.7060640841702535 and parameters: {'task_type': 'GPU', 'iterations': 2000, 'depth': 7, 'learning_rate': 0.25196092967497924, 'l2_leaf_reg': 16.45621651816936, 'random_strength': 0.0010951649636071078, 'border_count': 254, 'bagging_temperature': 0.5823131804767457}. Best is trial 3 with value: 0.7158201817312291.\n",
      "[I 2025-04-24 14:01:13,671] Trial 12 finished with value: 0.7168340506934481 and parameters: {'task_type': 'GPU', 'iterations': 2000, 'depth': 6, 'learning_rate': 0.11665341923476992, 'l2_leaf_reg': 2.8372667731287646, 'random_strength': 0.001183543934663979, 'border_count': 254, 'bagging_temperature': 0.9847104752014452}. Best is trial 12 with value: 0.7168340506934481.\n",
      "[I 2025-04-24 14:01:16,422] Trial 13 finished with value: 0.7099760879961741 and parameters: {'task_type': 'GPU', 'iterations': 1700, 'depth': 6, 'learning_rate': 0.10620830838219104, 'l2_leaf_reg': 2.9913509294297387, 'random_strength': 0.0020703779780954596, 'border_count': 254, 'bagging_temperature': 0.9791650101037157}. Best is trial 12 with value: 0.7168340506934481.\n",
      "[I 2025-04-24 14:01:21,295] Trial 14 finished with value: 0.703170731707317 and parameters: {'task_type': 'GPU', 'iterations': 1700, 'depth': 4, 'learning_rate': 0.1072139054728503, 'l2_leaf_reg': 2.509185780133423, 'random_strength': 0.565376105018175, 'border_count': 254, 'bagging_temperature': 0.024066685006217114}. Best is trial 12 with value: 0.7168340506934481.\n",
      "[I 2025-04-24 14:01:28,136] Trial 15 finished with value: 0.7217216642754662 and parameters: {'task_type': 'GPU', 'iterations': 200, 'depth': 6, 'learning_rate': 0.10799203287601479, 'l2_leaf_reg': 2.0295660513372864, 'random_strength': 0.023031666201981323, 'border_count': 254, 'bagging_temperature': 0.8599027746905126}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:34,462] Trial 16 finished with value: 0.7099569583931133 and parameters: {'task_type': 'GPU', 'iterations': 200, 'depth': 8, 'learning_rate': 0.07606161100863995, 'l2_leaf_reg': 1.7980787945907681, 'random_strength': 0.10011783441964435, 'border_count': 254, 'bagging_temperature': 0.8204653058795883}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:36,903] Trial 17 finished with value: 0.6992300334768053 and parameters: {'task_type': 'GPU', 'iterations': 400, 'depth': 5, 'learning_rate': 0.07701467989734077, 'l2_leaf_reg': 3.9226974003117783, 'random_strength': 0.0039715993987444785, 'border_count': 254, 'bagging_temperature': 0.8101071588778257}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:40,170] Trial 18 finished with value: 0.6904399808703969 and parameters: {'task_type': 'GPU', 'iterations': 900, 'depth': 8, 'learning_rate': 0.018931535210092144, 'l2_leaf_reg': 2.9865185350203323, 'random_strength': 1.0117725445066734, 'border_count': 64, 'bagging_temperature': 0.8814741563369176}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:42,268] Trial 19 finished with value: 0.7099856527977044 and parameters: {'task_type': 'GPU', 'iterations': 200, 'depth': 6, 'learning_rate': 0.15039393091866712, 'l2_leaf_reg': 1.6242383877195756, 'random_strength': 0.001056010892748057, 'border_count': 128, 'bagging_temperature': 0.7300345403826627}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:46,222] Trial 20 finished with value: 0.6963032042085127 and parameters: {'task_type': 'GPU', 'iterations': 900, 'depth': 9, 'learning_rate': 0.0658433136301965, 'l2_leaf_reg': 8.231622424195875, 'random_strength': 0.1276358637045947, 'border_count': 254, 'bagging_temperature': 0.4611633734785845}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:48,789] Trial 21 finished with value: 0.7099808703969392 and parameters: {'task_type': 'GPU', 'iterations': 1800, 'depth': 6, 'learning_rate': 0.15195302315705222, 'l2_leaf_reg': 2.331146330459761, 'random_strength': 0.022061949369552015, 'border_count': 254, 'bagging_temperature': 0.9973637983854022}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:51,347] Trial 22 finished with value: 0.7197704447632711 and parameters: {'task_type': 'GPU', 'iterations': 1900, 'depth': 5, 'learning_rate': 0.10797034622871758, 'l2_leaf_reg': 1.029064326160148, 'random_strength': 0.019106285585203883, 'border_count': 254, 'bagging_temperature': 0.9046641966969021}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:53,521] Trial 23 finished with value: 0.7050980392156863 and parameters: {'task_type': 'GPU', 'iterations': 1500, 'depth': 5, 'learning_rate': 0.12084242606938392, 'l2_leaf_reg': 1.1076301469838914, 'random_strength': 0.0049004759527342115, 'border_count': 254, 'bagging_temperature': 0.885979883707877}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:55,283] Trial 24 finished with value: 0.703170731707317 and parameters: {'task_type': 'GPU', 'iterations': 1800, 'depth': 4, 'learning_rate': 0.21406770832976646, 'l2_leaf_reg': 1.0152804051855042, 'random_strength': 0.050707746877924356, 'border_count': 254, 'bagging_temperature': 0.7502996176377795}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:57,580] Trial 25 finished with value: 0.6894739359158297 and parameters: {'task_type': 'GPU', 'iterations': 1500, 'depth': 5, 'learning_rate': 0.058340266396953736, 'l2_leaf_reg': 1.4413253696425177, 'random_strength': 0.01604133873270338, 'border_count': 254, 'bagging_temperature': 0.6453312528813532}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:01:59,352] Trial 26 finished with value: 0.6894595887135342 and parameters: {'task_type': 'CPU', 'iterations': 1900, 'depth': 7, 'learning_rate': 0.030061844697733903, 'l2_leaf_reg': 3.0962421107436966, 'random_strength': 0.03962642859711142, 'border_count': 254, 'bagging_temperature': 0.9069824201407882}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:02,696] Trial 27 finished with value: 0.7197274031563845 and parameters: {'task_type': 'GPU', 'iterations': 1600, 'depth': 7, 'learning_rate': 0.08362114415512165, 'l2_leaf_reg': 1.8601481152983241, 'random_strength': 0.0027356237962163047, 'border_count': 128, 'bagging_temperature': 0.8094012142314602}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:07,659] Trial 28 finished with value: 0.7187565758010521 and parameters: {'task_type': 'GPU', 'iterations': 1200, 'depth': 9, 'learning_rate': 0.07055322371302365, 'l2_leaf_reg': 1.8239599828261208, 'random_strength': 0.006670607073671583, 'border_count': 128, 'bagging_temperature': 0.7744305364418365}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:10,319] Trial 29 finished with value: 0.6865279770444763 and parameters: {'task_type': 'GPU', 'iterations': 1100, 'depth': 4, 'learning_rate': 0.04787931454270095, 'l2_leaf_reg': 5.072223041394135, 'random_strength': 0.003076571890501817, 'border_count': 128, 'bagging_temperature': 0.5818403664966416}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:12,782] Trial 30 finished with value: 0.707039693926351 and parameters: {'task_type': 'GPU', 'iterations': 1500, 'depth': 7, 'learning_rate': 0.09076542486313541, 'l2_leaf_reg': 1.3663916202837927, 'random_strength': 0.30389454036355973, 'border_count': 128, 'bagging_temperature': 0.6611449382759179}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:16,583] Trial 31 finished with value: 0.7070253467240555 and parameters: {'task_type': 'GPU', 'iterations': 1200, 'depth': 9, 'learning_rate': 0.054948318957806865, 'l2_leaf_reg': 1.7296353766602441, 'random_strength': 0.005777884226175161, 'border_count': 128, 'bagging_temperature': 0.8128197488042969}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:23,949] Trial 32 finished with value: 0.6992156862745098 and parameters: {'task_type': 'GPU', 'iterations': 1000, 'depth': 11, 'learning_rate': 0.08111173646468786, 'l2_leaf_reg': 1.8081831198838516, 'random_strength': 0.007822216229248733, 'border_count': 128, 'bagging_temperature': 0.7776748239904085}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:27,099] Trial 33 finished with value: 0.7089909134385461 and parameters: {'task_type': 'GPU', 'iterations': 1600, 'depth': 8, 'learning_rate': 0.04587061655105963, 'l2_leaf_reg': 1.9678332023190614, 'random_strength': 0.015341030774180708, 'border_count': 128, 'bagging_temperature': 0.6751950252122608}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:29,333] Trial 34 finished with value: 0.7080057388809182 and parameters: {'task_type': 'GPU', 'iterations': 700, 'depth': 5, 'learning_rate': 0.09382780672479703, 'l2_leaf_reg': 1.002630405097313, 'random_strength': 0.02877820989543435, 'border_count': 128, 'bagging_temperature': 0.8628390833619164}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:32,920] Trial 35 finished with value: 0.70506456241033 and parameters: {'task_type': 'GPU', 'iterations': 1300, 'depth': 9, 'learning_rate': 0.13157028126172726, 'l2_leaf_reg': 4.024294099328809, 'random_strength': 0.008899698723509836, 'border_count': 32, 'bagging_temperature': 0.9481945151520712}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:34,574] Trial 36 finished with value: 0.7138976566236251 and parameters: {'task_type': 'CPU', 'iterations': 500, 'depth': 7, 'learning_rate': 0.06725771382580759, 'l2_leaf_reg': 1.2936120368547277, 'random_strength': 0.00210355056327605, 'border_count': 128, 'bagging_temperature': 0.8748736533950637}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:41,298] Trial 37 finished with value: 0.7099378287900526 and parameters: {'task_type': 'GPU', 'iterations': 1800, 'depth': 11, 'learning_rate': 0.18705970891669374, 'l2_leaf_reg': 2.397278336977172, 'random_strength': 0.06261279444173973, 'border_count': 32, 'bagging_temperature': 0.506177986795401}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:41,969] Trial 38 finished with value: 0.6845911047345767 and parameters: {'task_type': 'CPU', 'iterations': 800, 'depth': 6, 'learning_rate': 0.02917158630974159, 'l2_leaf_reg': 2.1073084016906014, 'random_strength': 0.16983686597380557, 'border_count': 64, 'bagging_temperature': 0.7091192598805929}. Best is trial 15 with value: 0.7217216642754662.\n",
      "[I 2025-04-24 14:02:44,413] Trial 39 finished with value: 0.6992348158775705 and parameters: {'task_type': 'GPU', 'iterations': 1200, 'depth': 5, 'learning_rate': 0.041403962012819824, 'l2_leaf_reg': 1.2313983627257008, 'random_strength': 0.010674091139345143, 'border_count': 128, 'bagging_temperature': 0.9178075832853075}. Best is trial 15 with value: 0.7217216642754662.\n",
      "2025-04-24 14:02:44,422 - INFO - Optimization complete for catboost.\n",
      "2025-04-24 14:02:44,422 - INFO - Best CV score: 0.72172\n",
      "2025-04-24 14:02:44,423 - INFO - Best params: {'task_type': 'GPU', 'iterations': 200, 'depth': 6, 'learning_rate': 0.10799203287601479, 'l2_leaf_reg': 2.0295660513372864, 'random_strength': 0.023031666201981323, 'border_count': 254, 'bagging_temperature': 0.8599027746905126}\n",
      "2025-04-24 14:02:44,424 - INFO - Saved Optuna summary: optuna_trials/catboost_study_summary_20250424_121903.txt\n",
      "2025-04-24 14:02:44,425 - INFO - Instantiating final catboost model...\n",
      "2025-04-24 14:02:44,425 - ERROR - Failed final instantiate/fit/save process catboost: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 705, in optimize_model\n",
      "    if 'class_weight' not in final_p_log: final_p_log['class_weight'] = 'balanced'; final_model = LogisticRegression(**final_p_log)\n",
      "                             ^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "2025-04-24 14:02:44,426 - INFO - --- NOT QUALIFIED: catboost (CV Score: 0.72172  - Final fit/save failed) ---\n",
      "2025-04-24 14:02:44,427 - INFO - --- Optimizing xgboost ---\n",
      "2025-04-24 14:02:44,428 - INFO - Starting xgboost optimization (50 trials)...\n",
      "2025-04-24 14:02:44,428 - INFO - Optuna timeout for xgboost: 5400s.\n",
      "[I 2025-04-24 14:02:44,623] A new study created in RDB with name: xgboost_opt_20250424_121903\n",
      "2025-04-24 14:02:44,628 - INFO - Setting Optuna timeout for xgboost to 5400 seconds.\n",
      "2025-04-24 14:02:45,032 - WARNING - CV fold 1 fail xgboost trial 0 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:45,034 - ERROR - Cross-validation failed completely for xgboost trial 0\n",
      "[I 2025-04-24 14:02:45,048] Trial 0 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1600, 'max_depth': 6, 'learning_rate': 0.007374713802348072, 'subsample': 0.8863055223229823, 'colsample_bytree': 0.7310223025251197, 'min_child_weight': 6, 'gamma': 0.7101753190847151, 'reg_alpha': 1.461575913240746e-07, 'reg_lambda': 0.3524711996286945}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:45,116 - WARNING - CV fold 1 fail xgboost trial 1 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:45,117 - ERROR - Cross-validation failed completely for xgboost trial 1\n",
      "[I 2025-04-24 14:02:45,129] Trial 1 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 700, 'max_depth': 14, 'learning_rate': 0.1179632589481486, 'subsample': 0.6459099449817196, 'colsample_bytree': 0.8044074703803968, 'min_child_weight': 10, 'gamma': 0.0018646052011073381, 'reg_alpha': 1.5462088939608847, 'reg_lambda': 6.32990465763107e-07}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:45,196 - WARNING - CV fold 1 fail xgboost trial 2 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:45,196 - ERROR - Cross-validation failed completely for xgboost trial 2\n",
      "[I 2025-04-24 14:02:45,210] Trial 2 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1700, 'max_depth': 8, 'learning_rate': 0.015104329619403785, 'subsample': 0.7667117260902946, 'colsample_bytree': 0.7463453943293274, 'min_child_weight': 11, 'gamma': 4.045441560046966e-08, 'reg_alpha': 0.25691363873419787, 'reg_lambda': 9.359719082253972e-07}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:45,291 - WARNING - CV fold 1 fail xgboost trial 3 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:45,293 - ERROR - Cross-validation failed completely for xgboost trial 3\n",
      "[I 2025-04-24 14:02:45,309] Trial 3 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1100, 'max_depth': 10, 'learning_rate': 0.007813824420310957, 'subsample': 0.815523332472758, 'colsample_bytree': 0.621622144680775, 'min_child_weight': 8, 'gamma': 0.00032226467371829405, 'reg_alpha': 8.976095587700496e-08, 'reg_lambda': 2.3766144071237088e-08}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:45,393 - WARNING - CV fold 1 fail xgboost trial 4 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:45,394 - ERROR - Cross-validation failed completely for xgboost trial 4\n",
      "[I 2025-04-24 14:02:45,414] Trial 4 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2000, 'max_depth': 5, 'learning_rate': 0.00952470665705833, 'subsample': 0.9998517844958068, 'colsample_bytree': 0.7082304096577354, 'min_child_weight': 10, 'gamma': 1.274733878431167e-07, 'reg_alpha': 1.9981457808778528e-05, 'reg_lambda': 0.3017823433882034}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:45,534 - WARNING - CV fold 1 fail xgboost trial 5 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:45,534 - ERROR - Cross-validation failed completely for xgboost trial 5\n",
      "[I 2025-04-24 14:02:45,548] Trial 5 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1200, 'max_depth': 6, 'learning_rate': 0.22474616101806666, 'subsample': 0.5237673049760716, 'colsample_bytree': 0.666807648566589, 'min_child_weight': 1, 'gamma': 4.872840153318171e-07, 'reg_alpha': 2.5696292801349877e-05, 'reg_lambda': 0.012208187498797925}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:45,640 - WARNING - CV fold 1 fail xgboost trial 6 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:45,643 - ERROR - Cross-validation failed completely for xgboost trial 6\n",
      "[I 2025-04-24 14:02:45,667] Trial 6 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 300, 'max_depth': 12, 'learning_rate': 0.03162418311702546, 'subsample': 0.5099808046915199, 'colsample_bytree': 0.6960799183110047, 'min_child_weight': 6, 'gamma': 0.0004086261618524286, 'reg_alpha': 0.08346126990225204, 'reg_lambda': 0.0001255969169338105}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:45,753 - WARNING - CV fold 1 fail xgboost trial 7 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:45,754 - ERROR - Cross-validation failed completely for xgboost trial 7\n",
      "[I 2025-04-24 14:02:45,769] Trial 7 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1200, 'max_depth': 11, 'learning_rate': 0.010408460325984737, 'subsample': 0.7292882031970009, 'colsample_bytree': 0.6414625015925353, 'min_child_weight': 10, 'gamma': 0.07964433941895015, 'reg_alpha': 1.011736480115565, 'reg_lambda': 3.6107210692097797e-06}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:45,845 - WARNING - CV fold 1 fail xgboost trial 8 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:45,845 - ERROR - Cross-validation failed completely for xgboost trial 8\n",
      "[I 2025-04-24 14:02:45,858] Trial 8 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1800, 'max_depth': 8, 'learning_rate': 0.2967416686849176, 'subsample': 0.5952316385764889, 'colsample_bytree': 0.8145965013268048, 'min_child_weight': 6, 'gamma': 3.10226340307235e-05, 'reg_alpha': 3.1123617199273826e-05, 'reg_lambda': 2.3309502473431117e-06}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:45,940 - WARNING - CV fold 1 fail xgboost trial 9 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:45,942 - ERROR - Cross-validation failed completely for xgboost trial 9\n",
      "[I 2025-04-24 14:02:45,979] Trial 9 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 700, 'max_depth': 9, 'learning_rate': 0.13581536193105762, 'subsample': 0.9086732898408252, 'colsample_bytree': 0.5433781018464143, 'min_child_weight': 3, 'gamma': 0.3605330274166556, 'reg_alpha': 0.003739229663170665, 'reg_lambda': 1.8603242430570977e-07}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:46,101 - WARNING - CV fold 1 fail xgboost trial 10 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:46,101 - ERROR - Cross-validation failed completely for xgboost trial 10\n",
      "[I 2025-04-24 14:02:46,116] Trial 10 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1600, 'max_depth': 4, 'learning_rate': 0.03348339078677057, 'subsample': 0.8817223680319728, 'colsample_bytree': 0.98235981420757, 'min_child_weight': 4, 'gamma': 0.00863663496006922, 'reg_alpha': 1.976720716755181e-08, 'reg_lambda': 13.704384592104255}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:46,241 - WARNING - CV fold 1 fail xgboost trial 11 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:46,242 - ERROR - Cross-validation failed completely for xgboost trial 11\n",
      "[I 2025-04-24 14:02:46,255] Trial 11 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 700, 'max_depth': 14, 'learning_rate': 0.08806800003867175, 'subsample': 0.6444485012325091, 'colsample_bytree': 0.8771304155485928, 'min_child_weight': 8, 'gamma': 0.006501522641188027, 'reg_alpha': 0.0022743780103309935, 'reg_lambda': 0.0003253043663948588}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:46,392 - WARNING - CV fold 1 fail xgboost trial 12 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:46,393 - ERROR - Cross-validation failed completely for xgboost trial 12\n",
      "[I 2025-04-24 14:02:46,406] Trial 12 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 800, 'max_depth': 15, 'learning_rate': 0.0627198242658393, 'subsample': 0.6763354525359412, 'colsample_bytree': 0.4283734365285204, 'min_child_weight': 12, 'gamma': 9.128023479654127e-06, 'reg_alpha': 9.805971948109024, 'reg_lambda': 0.021544070720906947}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:46,497 - WARNING - CV fold 1 fail xgboost trial 13 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:46,497 - ERROR - Cross-validation failed completely for xgboost trial 13\n",
      "[I 2025-04-24 14:02:46,512] Trial 13 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1400, 'max_depth': 3, 'learning_rate': 0.005133533746539922, 'subsample': 0.8403486616894733, 'colsample_bytree': 0.8675001709570483, 'min_child_weight': 8, 'gamma': 0.5599931214136737, 'reg_alpha': 5.001976307077234e-07, 'reg_lambda': 2.223968389268212}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:46,678 - WARNING - CV fold 1 fail xgboost trial 14 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:46,679 - ERROR - Cross-validation failed completely for xgboost trial 14\n",
      "[I 2025-04-24 14:02:46,701] Trial 14 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 300, 'max_depth': 13, 'learning_rate': 0.024782068624295675, 'subsample': 0.9789763963961834, 'colsample_bytree': 0.7948971793909493, 'min_child_weight': 4, 'gamma': 0.005310506052971928, 'reg_alpha': 4.861790228381717e-07, 'reg_lambda': 5.850446182434216e-05}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:46,850 - WARNING - CV fold 1 fail xgboost trial 15 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:46,850 - ERROR - Cross-validation failed completely for xgboost trial 15\n",
      "[I 2025-04-24 14:02:46,879] Trial 15 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.0606002395676757, 'subsample': 0.7280946657922693, 'colsample_bytree': 0.9377390029703186, 'min_child_weight': 7, 'gamma': 0.04208268788793833, 'reg_alpha': 0.032547952635925706, 'reg_lambda': 0.006580320633967173}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:47,042 - WARNING - CV fold 1 fail xgboost trial 16 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:47,043 - ERROR - Cross-validation failed completely for xgboost trial 16\n",
      "[I 2025-04-24 14:02:47,062] Trial 16 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.14336040061851685, 'subsample': 0.5968715772247621, 'colsample_bytree': 0.5500862731265812, 'min_child_weight': 9, 'gamma': 2.0435996211109554e-06, 'reg_alpha': 6.639706448798633, 'reg_lambda': 2.543056393125494e-05}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:47,203 - WARNING - CV fold 1 fail xgboost trial 17 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:47,204 - ERROR - Cross-validation failed completely for xgboost trial 17\n",
      "[I 2025-04-24 14:02:47,234] Trial 17 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1400, 'max_depth': 11, 'learning_rate': 0.020814790415379418, 'subsample': 0.7925127142385986, 'colsample_bytree': 0.7822822601091869, 'min_child_weight': 5, 'gamma': 0.0009765868255669416, 'reg_alpha': 0.0002818335667306878, 'reg_lambda': 1.3862518798011248e-08}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:47,366 - WARNING - CV fold 1 fail xgboost trial 18 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:47,367 - ERROR - Cross-validation failed completely for xgboost trial 18\n",
      "[I 2025-04-24 14:02:47,388] Trial 18 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1000, 'max_depth': 13, 'learning_rate': 0.04590575199847808, 'subsample': 0.899122342071003, 'colsample_bytree': 0.8979305033489899, 'min_child_weight': 12, 'gamma': 4.8233544146337856e-05, 'reg_alpha': 3.7593334573124614e-06, 'reg_lambda': 0.0022128284808475554}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:47,526 - WARNING - CV fold 1 fail xgboost trial 19 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:47,526 - ERROR - Cross-validation failed completely for xgboost trial 19\n",
      "[I 2025-04-24 14:02:47,540] Trial 19 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 500, 'max_depth': 15, 'learning_rate': 0.11075414356024642, 'subsample': 0.6814252378719206, 'colsample_bytree': 0.5914091437648581, 'min_child_weight': 2, 'gamma': 0.050174945305870036, 'reg_alpha': 0.0003836065311738684, 'reg_lambda': 0.40225837713329365}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:47,634 - WARNING - CV fold 1 fail xgboost trial 20 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:47,634 - ERROR - Cross-validation failed completely for xgboost trial 20\n",
      "[I 2025-04-24 14:02:47,652] Trial 20 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1400, 'max_depth': 3, 'learning_rate': 0.00504946716489787, 'subsample': 0.9431055250609964, 'colsample_bytree': 0.45630072071275907, 'min_child_weight': 7, 'gamma': 0.9244820455506942, 'reg_alpha': 0.004975095353930494, 'reg_lambda': 0.13943618593384174}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:47,754 - WARNING - CV fold 1 fail xgboost trial 21 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:47,754 - ERROR - Cross-validation failed completely for xgboost trial 21\n",
      "[I 2025-04-24 14:02:47,768] Trial 21 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1700, 'max_depth': 9, 'learning_rate': 0.015464035878401049, 'subsample': 0.7637226944108441, 'colsample_bytree': 0.7354233411695376, 'min_child_weight': 11, 'gamma': 1.2074010993111577e-08, 'reg_alpha': 0.5573948951467028, 'reg_lambda': 2.974594049221365e-07}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:47,908 - WARNING - CV fold 1 fail xgboost trial 22 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:47,909 - ERROR - Cross-validation failed completely for xgboost trial 22\n",
      "[I 2025-04-24 14:02:47,926] Trial 22 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1900, 'max_depth': 7, 'learning_rate': 0.019513220730198604, 'subsample': 0.6091704417969582, 'colsample_bytree': 0.7524535685869984, 'min_child_weight': 10, 'gamma': 1.0969864943959327e-08, 'reg_alpha': 0.14597249549172078, 'reg_lambda': 5.265485575981291e-06}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:48,044 - WARNING - CV fold 1 fail xgboost trial 23 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:48,045 - ERROR - Cross-validation failed completely for xgboost trial 23\n",
      "[I 2025-04-24 14:02:48,063] Trial 23 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1600, 'max_depth': 8, 'learning_rate': 0.014031525518561525, 'subsample': 0.8534237633118404, 'colsample_bytree': 0.832151459331375, 'min_child_weight': 11, 'gamma': 7.122157009003652e-06, 'reg_alpha': 1.1037655014294885, 'reg_lambda': 4.5212478558889097e-07}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:48,166 - WARNING - CV fold 1 fail xgboost trial 24 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:48,167 - ERROR - Cross-validation failed completely for xgboost trial 24\n",
      "[I 2025-04-24 14:02:48,182] Trial 24 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1500, 'max_depth': 6, 'learning_rate': 0.0072627957556226095, 'subsample': 0.7758658401102898, 'colsample_bytree': 0.7550793540378444, 'min_child_weight': 9, 'gamma': 2.115939902170951e-07, 'reg_alpha': 0.03075146594152012, 'reg_lambda': 0.0015214905756934678}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:48,310 - WARNING - CV fold 1 fail xgboost trial 25 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:48,310 - ERROR - Cross-validation failed completely for xgboost trial 25\n",
      "[I 2025-04-24 14:02:48,325] Trial 25 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2000, 'max_depth': 8, 'learning_rate': 0.012683345319326801, 'subsample': 0.7035587375534537, 'colsample_bytree': 0.6995984912549326, 'min_child_weight': 5, 'gamma': 0.0014614543240839804, 'reg_alpha': 3.143949292994601, 'reg_lambda': 6.676309944359421e-08}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:48,419 - WARNING - CV fold 1 fail xgboost trial 26 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:48,420 - ERROR - Cross-validation failed completely for xgboost trial 26\n",
      "[I 2025-04-24 14:02:48,438] Trial 26 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1800, 'max_depth': 5, 'learning_rate': 0.050080945760092595, 'subsample': 0.556514563752753, 'colsample_bytree': 0.8433209048822503, 'min_child_weight': 11, 'gamma': 9.981004758451426e-05, 'reg_alpha': 0.21260568395002505, 'reg_lambda': 1.1763314293898626e-05}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:48,576 - WARNING - CV fold 1 fail xgboost trial 27 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:48,577 - ERROR - Cross-validation failed completely for xgboost trial 27\n",
      "[I 2025-04-24 14:02:48,591] Trial 27 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.008187180635191303, 'subsample': 0.6370144670555811, 'colsample_bytree': 0.9347207929377931, 'min_child_weight': 9, 'gamma': 1.6487717380190891e-06, 'reg_alpha': 0.0076745305424829, 'reg_lambda': 8.35523245768037e-07}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:48,695 - WARNING - CV fold 1 fail xgboost trial 28 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:48,696 - ERROR - Cross-validation failed completely for xgboost trial 28\n",
      "[I 2025-04-24 14:02:48,711] Trial 28 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1200, 'max_depth': 7, 'learning_rate': 0.02609299556979619, 'subsample': 0.8111818479958993, 'colsample_bytree': 0.7822602955633069, 'min_child_weight': 12, 'gamma': 7.075981409122718e-08, 'reg_alpha': 1.2574043254081817e-08, 'reg_lambda': 0.000835632074992661}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:48,826 - WARNING - CV fold 1 fail xgboost trial 29 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:48,826 - ERROR - Cross-validation failed completely for xgboost trial 29\n",
      "[I 2025-04-24 14:02:48,840] Trial 29 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1000, 'max_depth': 10, 'learning_rate': 0.00700191823638249, 'subsample': 0.8432970059393233, 'colsample_bytree': 0.6020382184963016, 'min_child_weight': 7, 'gamma': 0.1695802956427922, 'reg_alpha': 2.8760049823295133e-07, 'reg_lambda': 2.7422155286770947e-08}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:48,955 - WARNING - CV fold 1 fail xgboost trial 30 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:48,956 - ERROR - Cross-validation failed completely for xgboost trial 30\n",
      "[I 2025-04-24 14:02:48,983] Trial 30 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1700, 'max_depth': 11, 'learning_rate': 0.01717213361740255, 'subsample': 0.9432192759763229, 'colsample_bytree': 0.6530919298353799, 'min_child_weight': 8, 'gamma': 0.02948424459286691, 'reg_alpha': 14.748607303503096, 'reg_lambda': 1.5085182933395067e-06}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:49,134 - WARNING - CV fold 1 fail xgboost trial 31 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:49,135 - ERROR - Cross-validation failed completely for xgboost trial 31\n",
      "[I 2025-04-24 14:02:49,150] Trial 31 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1300, 'max_depth': 5, 'learning_rate': 0.01002934447363567, 'subsample': 0.8181482760111122, 'colsample_bytree': 0.7101175193126957, 'min_child_weight': 10, 'gamma': 0.0002766752454854591, 'reg_alpha': 8.508629509998375e-08, 'reg_lambda': 7.645244202246447e-08}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:49,251 - WARNING - CV fold 1 fail xgboost trial 32 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:49,252 - ERROR - Cross-validation failed completely for xgboost trial 32\n",
      "[I 2025-04-24 14:02:49,269] Trial 32 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1100, 'max_depth': 9, 'learning_rate': 0.0067947897324122115, 'subsample': 0.7347027699719217, 'colsample_bytree': 0.598143506559633, 'min_child_weight': 9, 'gamma': 0.001765206901663326, 'reg_alpha': 4.210625024753422e-06, 'reg_lambda': 1.3171389422703252e-08}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:49,396 - WARNING - CV fold 1 fail xgboost trial 33 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:49,397 - ERROR - Cross-validation failed completely for xgboost trial 33\n",
      "[I 2025-04-24 14:02:49,421] Trial 33 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1900, 'max_depth': 13, 'learning_rate': 0.0120734663765335, 'subsample': 0.8768918203230878, 'colsample_bytree': 0.7221078885054376, 'min_child_weight': 11, 'gamma': 0.0001449250034080933, 'reg_alpha': 4.006385356144339e-06, 'reg_lambda': 8.692097202993919e-08}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:49,598 - WARNING - CV fold 1 fail xgboost trial 34 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:49,599 - ERROR - Cross-validation failed completely for xgboost trial 34\n",
      "[I 2025-04-24 14:02:49,616] Trial 34 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.009007325366025755, 'subsample': 0.7872614890931671, 'colsample_bytree': 0.6536997329721664, 'min_child_weight': 6, 'gamma': 0.00039831472912012916, 'reg_alpha': 1.2180873875204996e-07, 'reg_lambda': 9.82113388755295e-05}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:49,747 - WARNING - CV fold 1 fail xgboost trial 35 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:49,747 - ERROR - Cross-validation failed completely for xgboost trial 35\n",
      "[I 2025-04-24 14:02:49,764] Trial 35 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1100, 'max_depth': 12, 'learning_rate': 0.210764940514597, 'subsample': 0.8210702047913051, 'colsample_bytree': 0.6795879577576114, 'min_child_weight': 10, 'gamma': 8.797436966152822e-06, 'reg_alpha': 5.0822965465269404e-08, 'reg_lambda': 8.997323939127557e-06}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:49,866 - WARNING - CV fold 1 fail xgboost trial 36 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:49,867 - ERROR - Cross-validation failed completely for xgboost trial 36\n",
      "[I 2025-04-24 14:02:49,881] Trial 36 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 700, 'max_depth': 10, 'learning_rate': 0.006206879169457118, 'subsample': 0.5509373726773168, 'colsample_bytree': 0.622394781279302, 'min_child_weight': 8, 'gamma': 0.015448367047825345, 'reg_alpha': 2.4060033456850856e-05, 'reg_lambda': 0.05886129392529794}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:50,005 - WARNING - CV fold 1 fail xgboost trial 37 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:50,007 - ERROR - Cross-validation failed completely for xgboost trial 37\n",
      "[I 2025-04-24 14:02:50,022] Trial 37 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1600, 'max_depth': 8, 'learning_rate': 0.011125546816721878, 'subsample': 0.7521801651552125, 'colsample_bytree': 0.5452512628293136, 'min_child_weight': 5, 'gamma': 2.4920271260919196e-05, 'reg_alpha': 0.00017114004690083222, 'reg_lambda': 9.941432049839672e-07}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:50,152 - WARNING - CV fold 1 fail xgboost trial 38 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:50,152 - ERROR - Cross-validation failed completely for xgboost trial 38\n",
      "[I 2025-04-24 14:02:50,164] Trial 38 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1300, 'max_depth': 4, 'learning_rate': 0.19751630816813776, 'subsample': 0.7102269357643913, 'colsample_bytree': 0.49151956837794275, 'min_child_weight': 6, 'gamma': 0.2020964442036863, 'reg_alpha': 8.917797412829595e-05, 'reg_lambda': 1.82547108122669e-07}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:50,282 - WARNING - CV fold 1 fail xgboost trial 39 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:50,283 - ERROR - Cross-validation failed completely for xgboost trial 39\n",
      "[I 2025-04-24 14:02:50,303] Trial 39 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 900, 'max_depth': 12, 'learning_rate': 0.07450561981304789, 'subsample': 0.9369850328777775, 'colsample_bytree': 0.8085866292878902, 'min_child_weight': 10, 'gamma': 0.0023816561125811304, 'reg_alpha': 0.0013360954030831758, 'reg_lambda': 0.48248183678006806}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:50,413 - WARNING - CV fold 1 fail xgboost trial 40 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:50,414 - ERROR - Cross-validation failed completely for xgboost trial 40\n",
      "[I 2025-04-24 14:02:50,426] Trial 40 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 600, 'max_depth': 14, 'learning_rate': 0.030770104237338378, 'subsample': 0.8621190205700696, 'colsample_bytree': 0.762506742706099, 'min_child_weight': 8, 'gamma': 4.839863439558641e-08, 'reg_alpha': 2.1530428254347886, 'reg_lambda': 3.338488941837562}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:50,520 - WARNING - CV fold 1 fail xgboost trial 41 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:50,520 - ERROR - Cross-validation failed completely for xgboost trial 41\n",
      "[I 2025-04-24 14:02:50,533] Trial 41 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2000, 'max_depth': 5, 'learning_rate': 0.009404992710204074, 'subsample': 0.983984919502654, 'colsample_bytree': 0.6884306232903464, 'min_child_weight': 11, 'gamma': 6.53611423893186e-07, 'reg_alpha': 1.3362925832091618e-06, 'reg_lambda': 0.010132797353371198}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:50,644 - WARNING - CV fold 1 fail xgboost trial 42 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:50,645 - ERROR - Cross-validation failed completely for xgboost trial 42\n",
      "[I 2025-04-24 14:02:50,658] Trial 42 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1800, 'max_depth': 4, 'learning_rate': 0.0059002946617132054, 'subsample': 0.9066089791932319, 'colsample_bytree': 0.7333499848234031, 'min_child_weight': 10, 'gamma': 4.9770433124565616e-08, 'reg_alpha': 3.649173093305339e-08, 'reg_lambda': 2.4944630874063987}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:50,751 - WARNING - CV fold 1 fail xgboost trial 43 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:50,752 - ERROR - Cross-validation failed completely for xgboost trial 43\n",
      "[I 2025-04-24 14:02:50,768] Trial 43 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1900, 'max_depth': 5, 'learning_rate': 0.00837255380574861, 'subsample': 0.9560521739804252, 'colsample_bytree': 0.6764868263420917, 'min_child_weight': 9, 'gamma': 1.7361717062787465e-07, 'reg_alpha': 1.2538918147175494e-05, 'reg_lambda': 14.898629852601552}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:50,872 - WARNING - CV fold 1 fail xgboost trial 44 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:50,873 - ERROR - Cross-validation failed completely for xgboost trial 44\n",
      "[I 2025-04-24 14:02:50,885] Trial 44 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1700, 'max_depth': 6, 'learning_rate': 0.2674114453277061, 'subsample': 0.998900576969134, 'colsample_bytree': 0.6279139977514588, 'min_child_weight': 1, 'gamma': 1.2341618215765206e-06, 'reg_alpha': 2.465184407711832e-07, 'reg_lambda': 0.00036507941656538427}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:50,990 - WARNING - CV fold 1 fail xgboost trial 45 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:50,992 - ERROR - Cross-validation failed completely for xgboost trial 45\n",
      "[I 2025-04-24 14:02:51,004] Trial 45 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1500, 'max_depth': 7, 'learning_rate': 0.01579216964776302, 'subsample': 0.6533843813265255, 'colsample_bytree': 0.8311148041457643, 'min_child_weight': 7, 'gamma': 4.420710233734109e-06, 'reg_alpha': 9.148305911995895e-07, 'reg_lambda': 0.029806866314200237}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:51,120 - WARNING - CV fold 1 fail xgboost trial 46 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:51,121 - ERROR - Cross-validation failed completely for xgboost trial 46\n",
      "[I 2025-04-24 14:02:51,147] Trial 46 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 800, 'max_depth': 9, 'learning_rate': 0.010452334459216896, 'subsample': 0.8834271758079144, 'colsample_bytree': 0.5712352066153902, 'min_child_weight': 12, 'gamma': 3.636717640947474e-05, 'reg_alpha': 0.05359089904429462, 'reg_lambda': 0.860560168589357}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:51,264 - WARNING - CV fold 1 fail xgboost trial 47 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:51,265 - ERROR - Cross-validation failed completely for xgboost trial 47\n",
      "[I 2025-04-24 14:02:51,283] Trial 47 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1500, 'max_depth': 8, 'learning_rate': 0.007821773912171258, 'subsample': 0.7979457637862919, 'colsample_bytree': 0.7827707891627136, 'min_child_weight': 10, 'gamma': 0.0007083245462642093, 'reg_alpha': 0.40568003296058563, 'reg_lambda': 3.394267432510899e-05}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:51,389 - WARNING - CV fold 1 fail xgboost trial 48 (GPU:True, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:51,391 - ERROR - Cross-validation failed completely for xgboost trial 48\n",
      "[I 2025-04-24 14:02:51,403] Trial 48 finished with value: 0.0 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2000, 'max_depth': 14, 'learning_rate': 0.03978632657913005, 'subsample': 0.5126510217975537, 'colsample_bytree': 0.861615152980369, 'min_child_weight': 8, 'gamma': 0.0033315567464848334, 'reg_alpha': 8.206023009306058e-05, 'reg_lambda': 0.12101799554658971}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:51,502 - WARNING - CV fold 1 fail xgboost trial 49 (GPU:False, Keras:False): XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "2025-04-24 14:02:51,503 - ERROR - Cross-validation failed completely for xgboost trial 49\n",
      "[I 2025-04-24 14:02:51,519] Trial 49 finished with value: 0.0 and parameters: {'tree_method': 'hist', 'n_estimators': 1300, 'max_depth': 7, 'learning_rate': 0.13076565752009361, 'subsample': 0.8282755609389736, 'colsample_bytree': 0.9104487379904229, 'min_child_weight': 4, 'gamma': 1.7012324951466725e-07, 'reg_alpha': 0.0008341329018629517, 'reg_lambda': 3.504075400788853e-06}. Best is trial 0 with value: 0.0.\n",
      "2025-04-24 14:02:51,535 - INFO - Optimization complete for xgboost.\n",
      "2025-04-24 14:02:51,536 - INFO - Best CV score: 0.00000\n",
      "2025-04-24 14:02:51,537 - INFO - Best params: {'tree_method': 'hist', 'n_estimators': 1600, 'max_depth': 6, 'learning_rate': 0.007374713802348072, 'subsample': 0.8863055223229823, 'colsample_bytree': 0.7310223025251197, 'min_child_weight': 6, 'gamma': 0.7101753190847151, 'reg_alpha': 1.461575913240746e-07, 'reg_lambda': 0.3524711996286945}\n",
      "2025-04-24 14:02:51,538 - INFO - Saved Optuna summary: optuna_trials/xgboost_study_summary_20250424_121903.txt\n",
      "2025-04-24 14:02:51,540 - INFO - Instantiating final xgboost model...\n",
      "2025-04-24 14:02:51,542 - ERROR - Failed final instantiate/fit/save process xgboost: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_1168\\3162577724.py\", line 705, in optimize_model\n",
      "    if 'class_weight' not in final_p_log: final_p_log['class_weight'] = 'balanced'; final_model = LogisticRegression(**final_p_log)\n",
      "                             ^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'final_p_log' where it is not associated with a value\n",
      "2025-04-24 14:02:51,544 - INFO - --- NOT QUALIFIED: xgboost (CV Score: 0.00000  - Final fit/save failed) ---\n",
      "2025-04-24 14:02:51,545 - INFO - --- Model Optimization Phase Complete ---\n",
      "2025-04-24 14:02:51,546 - ERROR - CRITICAL: NO models met CV threshold 0.72. Abort.\n",
      "2025-04-24 14:02:51,560 - INFO - Pipeline execution failed.\n",
      "2025-04-24 14:02:51,561 - INFO - Total time: 6227.75 sec (103.80 min).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Pipeline execution failed.\n",
      "Total time: 6227.75 sec (103.80 min).\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,\n",
    "    AdaBoostClassifier, VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from category_encoders import TargetEncoder, CatBoostEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import logging\n",
    "import subprocess\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def create_directory_structure():\n",
    "    directories = ['models', 'features', 'results', 'submissions', 'logs', 'plots', 'optuna_trials', 'scalers']\n",
    "    for directory in directories:\n",
    "        try:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "                logger.info(f\"Created directory: {directory}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating directory {directory}: {e}\")\n",
    "            raise\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def save_feature_importance(model, feature_names, timestamp, model_name):\n",
    "    if not feature_names:\n",
    "        logger.warning(f\"No feature names provided for {model_name}. Skipping feature importance plot.\")\n",
    "        return\n",
    "\n",
    "    importances = None\n",
    "    importance_type = None\n",
    "    is_fitted = True # Assume fitted unless Keras check fails\n",
    "\n",
    "    if isinstance(model, KerasClassifier):\n",
    "        try:\n",
    "            _ = model.model_ # Check if internal model exists\n",
    "        except AttributeError:\n",
    "            is_fitted = False\n",
    "            logger.warning(f\"Keras model {model_name} not fitted. Skip importance.\")\n",
    "            return\n",
    "        if not is_fitted: return\n",
    "        logger.info(f\"Importance plot not directly available for Keras model {model_name}.\")\n",
    "        return\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        importance_type = 'Importance'\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        if model.coef_.ndim > 1:\n",
    "            importances = np.abs(model.coef_).mean(axis=0)\n",
    "        else:\n",
    "            importances = np.abs(model.coef_)\n",
    "        importance_type = 'Coefficient Magnitude'\n",
    "    else:\n",
    "        # Handling for ensembles or models without standard importance\n",
    "        if isinstance(model, (VotingClassifier, StackingClassifier)):\n",
    "            logger.info(f\"Importance plot not generated for ensemble {model_name}.\")\n",
    "            return\n",
    "        elif isinstance(model, MLPClassifier):\n",
    "            logger.info(f\"Importance plot not directly available for MLPClassifier {model_name}.\")\n",
    "            return\n",
    "        elif hasattr(model, 'estimator_') and hasattr(model.estimator_, 'feature_importances_'): # e.g., AdaBoost\n",
    "            logger.info(f\"Using importance from base estimator of {model_name}.\")\n",
    "            importances = model.estimator_.feature_importances_\n",
    "            importance_type = 'Base Estimator Importance'\n",
    "        elif hasattr(model, 'estimators_') and model.estimators_: # e.g., RF, ET (already covered by feature_importances_) or others\n",
    "            try:\n",
    "                all_importances = [est.feature_importances_ for est in model.estimators_ if hasattr(est, 'feature_importances_')]\n",
    "                if all_importances:\n",
    "                    importances = np.mean(all_importances, axis=0)\n",
    "                    importance_type = 'Mean Base Importance'\n",
    "                    logger.info(f\"Averaged importance from base estimators for {model_name}.\")\n",
    "                else:\n",
    "                    logger.info(f\"No base estimators with importance found for {model_name}.\")\n",
    "                    return\n",
    "            except Exception as avg_imp_e:\n",
    "                logger.warning(f\"Could not average base importances for {model_name}: {avg_imp_e}.\")\n",
    "                return\n",
    "        else:\n",
    "            logger.info(f\"Model {model_name} ({model.__class__.__name__}) lacks importance attributes.\")\n",
    "            return\n",
    "\n",
    "    if importances is None:\n",
    "        logger.warning(f\"Could not retrieve importances for {model_name}.\")\n",
    "        return\n",
    "    if importances.ndim > 1: # Ensure 1D array\n",
    "        logger.warning(f\"Importances shape {importances.shape} for {model_name}. Mean over axis 0.\")\n",
    "        importances = importances.mean(axis=0)\n",
    "    if len(importances) != len(feature_names):\n",
    "        logger.warning(f\"Importance len ({len(importances)}) vs names ({len(feature_names)}) mismatch for {model_name}.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values('Importance', ascending=False)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_n = min(30, len(importance_df))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df.head(top_n), palette='viridis')\n",
    "        plt.title(f'Top {top_n} Feat Importances - {model_name}')\n",
    "        plt.xlabel(f'Relative {importance_type}')\n",
    "        plt.tight_layout()\n",
    "        plot_filename = f'plots/{model_name}_feature_importance_{timestamp}.png'\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved importance plot: {plot_filename}\")\n",
    "        csv_filename = f'results/{model_name}_feature_importance_{timestamp}.csv'\n",
    "        importance_df.to_csv(csv_filename, index=False)\n",
    "        logger.info(f\"Saved importance csv: {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not save importance plot/CSV {model_name}: {e}\", exc_info=True)\n",
    "\n",
    "def build_keras_model(n_features, n_classes, optimizer='adam', learning_rate=0.001,\n",
    "                      hidden_units=[128, 64], dropout_rate=0.3, activation='relu'):\n",
    "    model = keras.Sequential(name=\"keras_mlp_tabular\")\n",
    "    model.add(layers.Input(shape=(n_features,)))\n",
    "    for units in hidden_units:\n",
    "        model.add(layers.Dense(units))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Activation(activation))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "    if optimizer.lower() == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer.lower() == 'sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        logger.warning(f\"Unsupported optimizer '{optimizer}'. Defaulting to Adam.\")\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- MODIFIED preprocess_data Function ---\n",
    "def preprocess_data(df, all_states, all_feature1, timestamp, is_training=True, feature_columns_to_use=None):\n",
    "    \"\"\"Preprocesses data incorporating FE from simpler model and robust categorical handling.\"\"\"\n",
    "    logger.info(f\"Starting preprocessing (Combined Logic). Is training: {is_training}\")\n",
    "    start_time = time.time()\n",
    "    data = df.copy()\n",
    "    y = None\n",
    "    le = None\n",
    "    target_column = 'salary_category'\n",
    "\n",
    "    # 1. Handle Target Variable (Training Only)\n",
    "    if target_column in data.columns and is_training:\n",
    "        target = data[target_column] # Keep target for TargetEncoding later\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(target)\n",
    "        logger.info(f\"Target variable '{target_column}' found and label encoded.\")\n",
    "        logger.info(f\"Target distribution (Encoded): {np.bincount(y)}\")\n",
    "        joblib.dump(le, f'features/label_encoder_{timestamp}.joblib')\n",
    "        mapping = {int(v): k for k, v in zip(le.classes_, le.transform(le.classes_))}\n",
    "        mapping_file = f'features/target_mapping_{timestamp}.json'\n",
    "        with open(mapping_file, 'w') as f:\n",
    "            json.dump(mapping, f, indent=4)\n",
    "        logger.info(f\"Saved label encoder and target mapping: {mapping_file}\")\n",
    "    elif not is_training:\n",
    "        # Load encoder\n",
    "        try:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "            if encoder_files:\n",
    "                latest_encoder_file = encoder_files[-1]\n",
    "                le = joblib.load(f'features/{latest_encoder_file}')\n",
    "                logger.info(f\"Loaded latest label encoder: {latest_encoder_file}\")\n",
    "            else:\n",
    "                logger.warning(\"No label encoder file found for test data!\")\n",
    "                le = None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load label encoder: {e}\")\n",
    "            le = None\n",
    "    else: # is_training is True but target column is missing\n",
    "        logger.error(f\"Target column '{target_column}' missing in training data!\")\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in training data.\")\n",
    "\n",
    "    # 2. Define Feature Groups\n",
    "    boolean_features_potential = ['feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_10', 'feature_11']\n",
    "    boolean_features = [f for f in boolean_features_potential if f in data.columns]\n",
    "    numerical_features = [f for f in ['feature_2', 'feature_9', 'feature_12'] if f in data.columns] # Base numerical\n",
    "    job_desc_cols = [col for col in data.columns if col.startswith('job_desc_')]\n",
    "    all_numerical_features = numerical_features + job_desc_cols\n",
    "\n",
    "    # 3. Initial Cleaning (Numerical & Boolean)\n",
    "    logger.info(\"Initial cleaning: Numerical and Boolean Features...\")\n",
    "    for col in all_numerical_features:\n",
    "        if col in data.columns:\n",
    "            if data[col].dtype == 'object':\n",
    "                data[col] = data[col].replace(['', ' ', 'NA', 'None', 'NULL'], np.nan)\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "            median_val = data[col].median()\n",
    "            fill_value = median_val if not pd.isna(median_val) else 0\n",
    "            data[col] = data[col].fillna(fill_value)\n",
    "\n",
    "    for col in boolean_features:\n",
    "        if col in data.columns:\n",
    "            numeric_view = pd.to_numeric(data[col], errors='coerce')\n",
    "            is_boolean_like = numeric_view.dropna().isin([0, 1]).all()\n",
    "            if is_boolean_like:\n",
    "                data[col] = numeric_view.fillna(0).astype(int)\n",
    "            else:\n",
    "                num_non_bool = numeric_view.dropna().loc[~numeric_view.dropna().isin([0, 1])].count()\n",
    "                logger.warning(f\"Column '{col}' contains non-0/1 values ({num_non_bool} instances). Treating as numerical and imputing with median.\")\n",
    "                median_val = numeric_view.median()\n",
    "                fill_value = median_val if not pd.isna(median_val) else 0\n",
    "                data[col] = numeric_view.fillna(fill_value)\n",
    "\n",
    "    logger.info(\"Starting Feature Engineering (using logic from simpler model)...\")\n",
    "    engineered_feature_names = [] # Track engineered features\n",
    "\n",
    "    # --- Feature Engineering Steps ---\n",
    "    if 'job_title' in data.columns:\n",
    "        data['job_title'] = data['job_title'].fillna('Unknown')\n",
    "        title_flags = ['is_senior', 'is_junior', 'is_developer', 'is_specialist']\n",
    "        data['is_senior'] = data['job_title'].str.lower().str.contains('senior|sr|lead|principal').fillna(False).astype(int)\n",
    "        data['is_junior'] = data['job_title'].str.lower().str.contains('junior|jr|associate|entry').fillna(False).astype(int)\n",
    "        data['is_developer'] = data['job_title'].str.lower().str.contains('develop|programmer|coder|engineer').fillna(False).astype(int)\n",
    "        data['is_specialist'] = data['job_title'].str.lower().str.contains('special|expert|consult').fillna(False).astype(int)\n",
    "        engineered_feature_names.extend(title_flags)\n",
    "        title_counts = data['job_title'].value_counts()\n",
    "        rare_titles = title_counts[title_counts < 10].index\n",
    "        data['job_title_grouped'] = data['job_title'].apply(lambda x: 'Other_Title' if x in rare_titles else x)\n",
    "        title_encoder_col = 'job_title_grouped'\n",
    "        target_encoded_title = 'job_title_encoded'\n",
    "        engineered_feature_names.append(target_encoded_title)\n",
    "        if is_training:\n",
    "            logger.info(f\"Applying Target Encoding to '{title_encoder_col}'...\")\n",
    "            job_encoder = TargetEncoder(cols=[title_encoder_col], handle_missing='value', handle_unknown='value')\n",
    "            data[target_encoded_title] = job_encoder.fit_transform(data[[title_encoder_col]], y)\n",
    "            joblib.dump(job_encoder, f'features/job_title_encoder_{timestamp}.joblib')\n",
    "            logger.info(f\"Fit and saved TargetEncoder for {title_encoder_col}\")\n",
    "        else:\n",
    "            encoder_path = f'features/job_title_encoder_{timestamp}.joblib'\n",
    "            encoder_files_fallback = sorted([f for f in os.listdir('features') if f.startswith('job_title_encoder_')])\n",
    "            loaded_encoder = False\n",
    "            if os.path.exists(encoder_path):\n",
    "                try:\n",
    "                    job_encoder = joblib.load(encoder_path)\n",
    "                    data[target_encoded_title] = job_encoder.transform(data[[title_encoder_col]])\n",
    "                    logger.info(f\"Loaded and applied TargetEncoder: {encoder_path}\")\n",
    "                    loaded_encoder = True\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to load/apply specific TargetEncoder '{encoder_path}': {e}. Trying fallback.\")\n",
    "            if not loaded_encoder and encoder_files_fallback:\n",
    "                 latest_encoder_file = encoder_files_fallback[-1]\n",
    "                 try:\n",
    "                     job_encoder = joblib.load(f'features/{latest_encoder_file}')\n",
    "                     data[target_encoded_title] = job_encoder.transform(data[[title_encoder_col]])\n",
    "                     logger.info(f\"Loaded and applied fallback TargetEncoder: {latest_encoder_file}\")\n",
    "                     loaded_encoder = True\n",
    "                 except Exception as e_fb:\n",
    "                      logger.error(f\"Fallback TargetEncoder failed: {e_fb}. Filling with 0.5\")\n",
    "                      data[target_encoded_title] = 0.5\n",
    "            if not loaded_encoder:\n",
    "                 logger.error(\"No TargetEncoder file found. Filling with 0.5\")\n",
    "                 data[target_encoded_title] = 0.5\n",
    "        data = data.drop(['job_title', 'job_title_grouped'], axis=1, errors='ignore')\n",
    "        logger.info(\"Processed 'job_title' (flags, grouping, target encoding).\")\n",
    "\n",
    "    if 'job_posted_date' in data.columns:\n",
    "        data['job_posted_date'] = data['job_posted_date'].fillna('2000/01')\n",
    "        def extract_year(date_str):\n",
    "            try: return int(str(date_str)[:4])\n",
    "            except: return 2000\n",
    "        def extract_month(date_str):\n",
    "            try: return int(str(date_str).split('/')[1])\n",
    "            except: return 1\n",
    "        data['job_posted_year'] = data['job_posted_date'].apply(extract_year)\n",
    "        data['job_posted_month'] = data['job_posted_date'].apply(extract_month)\n",
    "        data['job_posted_month'] = data['job_posted_month'].clip(1, 12)\n",
    "        date_features = ['month_sin', 'month_cos', 'job_recency', 'job_posted_year_norm']\n",
    "        data['month_sin'] = np.sin(2 * np.pi * data['job_posted_month'] / 12)\n",
    "        data['month_cos'] = np.cos(2 * np.pi * data['job_posted_month'] / 12)\n",
    "        data['job_recency'] = data['job_posted_year'] * 12 + data['job_posted_month']\n",
    "        mean_year = 2022\n",
    "        data['job_posted_year_norm'] = data['job_posted_year'] - mean_year\n",
    "        engineered_feature_names.extend(date_features)\n",
    "        data = data.drop(['job_posted_date', 'job_posted_year', 'job_posted_month'], axis=1, errors='ignore')\n",
    "        logger.info(\"Processed 'job_posted_date' (cyclical, recency, norm year).\")\n",
    "\n",
    "    num_transform_features = []\n",
    "    if 'feature_9' in data.columns:\n",
    "        try:\n",
    "            data['feature_9_bin'] = pd.qcut(data['feature_9'].rank(method='first'), q=5, labels=[0, 1, 2, 3, 4]).astype(int)\n",
    "        except ValueError:\n",
    "            logger.warning(\"qcut failed for feature_9, using pd.cut fallback.\")\n",
    "            try:\n",
    "                data['feature_9_bin'] = pd.cut(data['feature_9'], bins=5, labels=[0, 1, 2, 3, 4], include_lowest=True).astype(int)\n",
    "            except Exception as e_cut:\n",
    "                 logger.error(f\"pd.cut also failed for feature_9: {e_cut}. Setting bin to 0.\")\n",
    "                 data['feature_9_bin'] = 0\n",
    "        num_transform_features.append('feature_9_bin')\n",
    "        logger.info(\"Added binned feature for feature_9.\")\n",
    "        if 'feature_2' in data.columns:\n",
    "            interaction_name = 'feature_2_9_interaction'\n",
    "            data[interaction_name] = data['feature_2'] * data['feature_9']\n",
    "            num_transform_features.append(interaction_name)\n",
    "            logger.info(f\"Added interaction: {interaction_name}\")\n",
    "\n",
    "    if 'feature_2' in data.columns:\n",
    "        data['feature_2_squared'] = data['feature_2'] ** 2\n",
    "        data['feature_2_sqrt'] = np.sqrt(np.abs(data['feature_2']))\n",
    "        num_transform_features.extend(['feature_2_squared', 'feature_2_sqrt'])\n",
    "        try:\n",
    "            data['feature_2_bin'] = pd.qcut(data['feature_2'].rank(method='first'), q=5, labels=[0, 1, 2, 3, 4]).astype(int)\n",
    "        except ValueError:\n",
    "            logger.warning(\"qcut failed for feature_2, using pd.cut fallback.\")\n",
    "            try:\n",
    "                data['feature_2_bin'] = pd.cut(data['feature_2'], bins=5, labels=[0, 1, 2, 3, 4], include_lowest=True).astype(int)\n",
    "            except Exception as e_cut:\n",
    "                 logger.error(f\"pd.cut also failed for feature_2: {e_cut}. Setting bin to 0.\")\n",
    "                 data['feature_2_bin'] = 0\n",
    "        num_transform_features.append('feature_2_bin')\n",
    "        logger.info(\"Added squared, sqrt, and binned features for feature_2.\")\n",
    "    engineered_feature_names.extend(num_transform_features)\n",
    "\n",
    "    bool_agg_features = []\n",
    "    actual_boolean_cols = [col for col in boolean_features if col in data.columns]\n",
    "    if actual_boolean_cols:\n",
    "        data['boolean_sum'] = data[actual_boolean_cols].sum(axis=1)\n",
    "        data['boolean_sum_squared'] = data['boolean_sum'] ** 2\n",
    "        bool_agg_features.extend(['boolean_sum', 'boolean_sum_squared'])\n",
    "        logger.info(\"Added boolean sum and squared sum features.\")\n",
    "    else:\n",
    "        data['boolean_sum'] = 0\n",
    "        data['boolean_sum_squared'] = 0\n",
    "        logger.info(\"No boolean features found for aggregation.\")\n",
    "    engineered_feature_names.extend(bool_agg_features)\n",
    "\n",
    "    if 'feature_10' in data.columns and 'feature_8' in data.columns:\n",
    "        interaction_name = 'feature_10_8_interaction'\n",
    "        data[interaction_name] = data['feature_10'] * data['feature_8']\n",
    "        engineered_feature_names.append(interaction_name)\n",
    "        logger.info(f\"Added interaction: {interaction_name}\")\n",
    "\n",
    "    job_desc_eng_features = []\n",
    "    if job_desc_cols:\n",
    "        desc_agg = ['job_desc_mean', 'job_desc_std', 'job_desc_min', 'job_desc_max', 'job_desc_sum', 'job_desc_q25', 'job_desc_q75', 'job_desc_iqr']\n",
    "        data['job_desc_mean'] = data[job_desc_cols].mean(axis=1)\n",
    "        data['job_desc_std'] = data[job_desc_cols].std(axis=1).fillna(0)\n",
    "        data['job_desc_min'] = data[job_desc_cols].min(axis=1)\n",
    "        data['job_desc_max'] = data[job_desc_cols].max(axis=1)\n",
    "        data['job_desc_sum'] = data[job_desc_cols].sum(axis=1)\n",
    "        data['job_desc_q25'] = data[job_desc_cols].quantile(0.25, axis=1)\n",
    "        data['job_desc_q75'] = data[job_desc_cols].quantile(0.75, axis=1)\n",
    "        data['job_desc_iqr'] = data['job_desc_q75'] - data['job_desc_q25']\n",
    "        job_desc_eng_features.extend(desc_agg)\n",
    "        n_pca_components = 15\n",
    "        if len(job_desc_cols) > n_pca_components:\n",
    "            logger.info(f\"Applying PCA (n={n_pca_components}) to job description features...\")\n",
    "            pca_names = [f'job_desc_pca_{i}' for i in range(n_pca_components)]\n",
    "            job_desc_eng_features.extend(pca_names)\n",
    "            job_desc_pca_result = None # Initialize\n",
    "            if is_training:\n",
    "                pca = PCA(n_components=n_pca_components, random_state=42)\n",
    "                job_desc_pca_result = pca.fit_transform(data[job_desc_cols])\n",
    "                joblib.dump(pca, f'features/job_desc_pca_{timestamp}.joblib')\n",
    "                logger.info(\"Fit and saved PCA model for job description.\")\n",
    "            else:\n",
    "                pca_path = f'features/job_desc_pca_{timestamp}.joblib'\n",
    "                pca_files_fallback = sorted([f for f in os.listdir('features') if f.startswith('job_desc_pca_')])\n",
    "                pca_loaded = False\n",
    "                pca = None # Define pca before try block\n",
    "                if os.path.exists(pca_path):\n",
    "                    try:\n",
    "                        pca = joblib.load(pca_path)\n",
    "                        pca_loaded=True\n",
    "                        logger.info(f\"Loaded specific PCA model: {pca_path}\")\n",
    "                    except Exception as e: logger.error(f\"Failed load specific PCA: {e}. Try fallback.\")\n",
    "                if not pca_loaded and pca_files_fallback:\n",
    "                     latest_pca_file = pca_files_fallback[-1]\n",
    "                     try:\n",
    "                         pca = joblib.load(f'features/{latest_pca_file}')\n",
    "                         pca_loaded=True\n",
    "                         logger.info(f\"Loaded fallback PCA model: {latest_pca_file}\")\n",
    "                     except Exception as e_fb: logger.error(f\"Fallback PCA load failed: {e_fb}.\")\n",
    "                if pca_loaded and pca is not None: # Check if pca object exists\n",
    "                     try: job_desc_pca_result = pca.transform(data[job_desc_cols])\n",
    "                     except Exception as e_trans: logger.error(f\"PCA transform failed: {e_trans}. Filling PCA features with 0.\")\n",
    "                # Fallback if loading/transform failed or no model found\n",
    "                if job_desc_pca_result is None:\n",
    "                    logger.error(\"PCA result not generated. Filling PCA features with 0.\")\n",
    "                    job_desc_pca_result = np.zeros((data.shape[0], n_pca_components))\n",
    "\n",
    "            # Add PCA features to dataframe\n",
    "            for i in range(min(n_pca_components, job_desc_pca_result.shape[1])):\n",
    "                data[pca_names[i]] = job_desc_pca_result[:, i]\n",
    "        else:\n",
    "            logger.warning(f\"Skipping PCA for job description: Not enough features ({len(job_desc_cols)}) for {n_pca_components} components.\")\n",
    "        data = data.drop(columns=job_desc_cols, errors='ignore')\n",
    "        logger.info(\"Finished processing job description features (aggregates and PCA).\")\n",
    "    else:\n",
    "        logger.info(\"No job description features found.\")\n",
    "    engineered_feature_names.extend(job_desc_eng_features)\n",
    "\n",
    "    # --- Robust Categorical Handling ---\n",
    "    if 'job_state' in data.columns:\n",
    "        data['job_state'] = data['job_state'].fillna('Unknown')\n",
    "    if 'feature_1' in data.columns:\n",
    "        data['feature_1'] = data['feature_1'].fillna('Unknown')\n",
    "\n",
    "    manual_ohe_features = []\n",
    "    logger.info(f\"Applying manual One-Hot Encoding for 'job_state' using {len(all_states)} total unique states.\")\n",
    "    if 'job_state' in data.columns:\n",
    "        for state in all_states:\n",
    "            col_name = f'state_{state}' # Consistent naming\n",
    "            data[col_name] = (data['job_state'] == state).astype(int)\n",
    "            manual_ohe_features.append(col_name)\n",
    "        data = data.drop('job_state', axis=1, errors='ignore') # Drop original\n",
    "    else:\n",
    "        logger.warning(\"'job_state' column not found for manual OHE.\")\n",
    "\n",
    "    logger.info(f\"Applying manual One-Hot Encoding for 'feature_1' using {len(all_feature1)} total unique values.\")\n",
    "    if 'feature_1' in data.columns:\n",
    "        for feat in all_feature1:\n",
    "            col_name = f'feat1_{feat}' # Consistent naming\n",
    "            data[col_name] = (data['feature_1'] == feat).astype(int)\n",
    "            manual_ohe_features.append(col_name)\n",
    "        data = data.drop('feature_1', axis=1, errors='ignore') # Drop original\n",
    "    else:\n",
    "        logger.warning(\"'feature_1' column not found for manual OHE.\")\n",
    "    engineered_feature_names.extend(manual_ohe_features)\n",
    "    # --- End FE ---\n",
    "\n",
    "    # 5. Final Cleanup and Column Management\n",
    "    logger.info(\"Final cleanup and column alignment...\")\n",
    "    columns_to_exclude = ['obs']\n",
    "    if is_training and target_column in df.columns:\n",
    "        columns_to_exclude.append(target_column)\n",
    "    potential_feature_cols = [col for col in data.columns if col not in columns_to_exclude]\n",
    "\n",
    "    inf_cols_handled = []\n",
    "    nan_cols_handled = []\n",
    "    for col in potential_feature_cols:\n",
    "        if pd.api.types.is_numeric_dtype(data[col]):\n",
    "            if np.isinf(data[col]).any():\n",
    "                inf_cols_handled.append(col)\n",
    "                data[col] = data[col].replace([np.inf, -np.inf], np.nan)\n",
    "            if data[col].isnull().any():\n",
    "                nan_cols_handled.append(col)\n",
    "                data[col] = data[col].fillna(0) # Simple fill with 0\n",
    "    if inf_cols_handled: logger.warning(f\"Replaced Inf values with NaN in: {inf_cols_handled}\")\n",
    "    final_nan_cols = list(set(nan_cols_handled) - set(inf_cols_handled))\n",
    "    if final_nan_cols: logger.info(f\"Filled NaN values with 0 in columns: {final_nan_cols}\")\n",
    "\n",
    "    for col in potential_feature_cols:\n",
    "        if data[col].dtype == 'bool': data[col] = data[col].astype(int)\n",
    "\n",
    "    # --- Constant Column Handling (Warn only) ---\n",
    "    if is_training:\n",
    "        constant_cols_found = []\n",
    "        for col in potential_feature_cols:\n",
    "            nunique_val = data[col].nunique(dropna=False)\n",
    "            if nunique_val <= 1:\n",
    "                is_engineered = col in engineered_feature_names\n",
    "                logger.warning(f\"Column '{col}' identified as constant (nunique={nunique_val}) in training data. Engineered: {is_engineered}. Keeping column.\")\n",
    "                constant_cols_found.append(col)\n",
    "            elif nunique_val <= 3 and col in engineered_feature_names:\n",
    "                logger.info(f\"Engineered column '{col}' has low cardinality (nunique={nunique_val}) in training data.\")\n",
    "\n",
    "        final_feature_columns = potential_feature_cols\n",
    "        joblib.dump(final_feature_columns, f'features/feature_columns_{timestamp}.joblib')\n",
    "        logger.info(f\"Saved {len(final_feature_columns)} final feature column names (constant columns NOT dropped).\")\n",
    "        X = data[final_feature_columns]\n",
    "        logger.info(f\"Preprocessing train done. Shape: {X.shape}. Time: {time.time() - start_time:.2f}s\")\n",
    "        try: X.head().to_csv(f'features/processed_features_head_{timestamp}.csv', index=False)\n",
    "        except Exception as e: logger.warning(f\"Could not save head: {e}\")\n",
    "        return X, y, final_feature_columns, le\n",
    "    else: # Test Data Processing (Alignment)\n",
    "        if feature_columns_to_use is None:\n",
    "            try:\n",
    "                col_files = sorted([f for f in os.listdir('features') if f.startswith('feature_columns_')])\n",
    "                if col_files:\n",
    "                    latest_col_file = col_files[-1]\n",
    "                    feature_columns_to_use = joblib.load(f'features/{latest_col_file}')\n",
    "                    logger.info(f\"Loaded {len(feature_columns_to_use)} feature columns from: {latest_col_file}\")\n",
    "                else:\n",
    "                    logger.error(\"CRITICAL: No feature_columns file found.\"); raise FileNotFoundError(\"feature_columns_*.joblib missing.\")\n",
    "            except Exception as e: logger.error(f\"Failed load feature columns: {e}.\"); raise\n",
    "\n",
    "        X = pd.DataFrame(columns=feature_columns_to_use)\n",
    "        missing_cols_in_test = []\n",
    "        processed_test_cols = list(data.columns)\n",
    "        extra_cols_in_test = list(set(processed_test_cols) - set(feature_columns_to_use) - set(columns_to_exclude))\n",
    "\n",
    "        for col in feature_columns_to_use:\n",
    "            if col in data.columns:\n",
    "                X[col] = data[col]\n",
    "            else:\n",
    "                X[col] = 0 # Add missing column, fill with 0\n",
    "                missing_cols_in_test.append(col)\n",
    "\n",
    "        if missing_cols_in_test: logger.warning(f\"Cols missing in test (filled 0): {missing_cols_in_test}\")\n",
    "        if extra_cols_in_test: logger.warning(f\"Cols extra in test (dropped during align): {extra_cols_in_test}\")\n",
    "\n",
    "        X = X[feature_columns_to_use] # Reorder to match training\n",
    "        logger.info(f\"Preprocessing test done. Shape: {X.shape}. Time: {time.time() - start_time:.2f}s\")\n",
    "        return X, y, feature_columns_to_use, le # y is None\n",
    "\n",
    "# --- optimize_model ---\n",
    "def optimize_model(X, y, timestamp, model_type, n_trials=30, n_jobs_optuna=18): # Allow passing n_jobs\n",
    "    logger.info(f\"Starting {model_type} optimization ({n_trials} trials)...\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    if not isinstance(y, (np.ndarray, pd.Series)): y = np.array(y)\n",
    "    if isinstance(X, np.ndarray): X = pd.DataFrame(X)\n",
    "\n",
    "    n_classes = len(np.unique(y))\n",
    "    n_features = X.shape[1]\n",
    "    y_keras = to_categorical(y, num_classes=n_classes) if model_type == 'keras_mlp' else y\n",
    "\n",
    "    KERAS_EPOCHS = 150\n",
    "    KERAS_PATIENCE = 20\n",
    "    OPTUNA_TIMEOUT_PER_MODEL = 3600\n",
    "    if model_type in ['xgboost', 'catboost', 'randomforest', 'gradientboosting', 'keras_mlp', 'mlp', 'extratrees']:\n",
    "        OPTUNA_TIMEOUT_PER_MODEL = 5400\n",
    "    logger.info(f\"Optuna timeout for {model_type}: {OPTUNA_TIMEOUT_PER_MODEL}s.\")\n",
    "\n",
    "    def objective(trial):\n",
    "        model = None\n",
    "        fit_params = {}\n",
    "        use_gpu = False\n",
    "        is_keras = False\n",
    "        # --- Model Definitions ---\n",
    "        if model_type == 'xgboost':\n",
    "            tree_method = trial.suggest_categorical('tree_method', ['hist', 'gpu_hist'])\n",
    "            param = { 'objective':'multi:softmax', 'num_class':n_classes, 'eval_metric':'mlogloss','n_estimators':trial.suggest_int('n_estimators',200,2000,step=100), 'max_depth':trial.suggest_int('max_depth',3,15),'learning_rate':trial.suggest_float('learning_rate',0.005,0.3,log=True), 'subsample':trial.suggest_float('subsample',0.5,1.0),'colsample_bytree':trial.suggest_float('colsample_bytree',0.4,1.0), 'min_child_weight':trial.suggest_int('min_child_weight',1,12),'gamma':trial.suggest_float('gamma',1e-8,1.0,log=True), 'reg_alpha':trial.suggest_float('reg_alpha',1e-8,15.0,log=True),'reg_lambda':trial.suggest_float('reg_lambda',1e-8,15.0,log=True), 'random_state':42,'n_jobs': 1,'booster':'gbtree', 'tree_method':tree_method } # n_jobs=1 for wrapper\n",
    "            if tree_method == 'gpu_hist':\n",
    "                param['gpu_id'] = 0; use_gpu = True; param.pop('n_jobs', None)\n",
    "            else: param.pop('gpu_id', None)\n",
    "            model = XGBClassifier(**param); fit_params = {'verbose': False}\n",
    "        elif model_type == 'catboost':\n",
    "            task_type = trial.suggest_categorical('task_type', ['CPU', 'GPU'])\n",
    "            param = { 'iterations':trial.suggest_int('iterations',200,2000,step=100), 'depth':trial.suggest_int('depth',4,14),'learning_rate':trial.suggest_float('learning_rate',0.005,0.3,log=True), 'l2_leaf_reg':trial.suggest_float('l2_leaf_reg',1,20,log=True),'random_strength':trial.suggest_float('random_strength',1e-3,10.0,log=True), 'border_count':trial.suggest_categorical('border_count',[32,64,128,254]),'bagging_temperature':trial.suggest_float('bagging_temperature',0.0,1.0), 'loss_function':'MultiClass', 'eval_metric':'Accuracy','random_seed':42, 'thread_count':-1,'verbose':False, 'task_type':task_type }\n",
    "            if task_type == 'GPU': param['devices'] = '0'; use_gpu = True\n",
    "            model = CatBoostClassifier(**param); fit_params = {'early_stopping_rounds': KERAS_PATIENCE, 'verbose': False}\n",
    "        elif model_type == 'keras_mlp':\n",
    "            is_keras = True; optimizer_name = trial.suggest_categorical('optimizer', ['adam']); lr = trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True); dropout = trial.suggest_float('dropout_rate', 0.1, 0.6); activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'swish']); n_layers = trial.suggest_int('n_layers', 2, 4); units_list = []; last_units = n_features\n",
    "            for i in range(n_layers): max_units = max(32, int(last_units / 1.5)); min_units = max(16, int(last_units / 4)); units = trial.suggest_int(f'n_units_l{i}', min_units, max_units, step=4, log=True); units_list.append(units); last_units = units\n",
    "            model = KerasClassifier(model=build_keras_model, n_features=n_features, n_classes=n_classes, optimizer=optimizer_name, learning_rate=lr, hidden_units=units_list, dropout_rate=dropout, activation=activation, epochs=KERAS_EPOCHS, batch_size=trial.suggest_categorical('batch_size', [64, 128, 256, 512]), verbose=0)\n",
    "            keras_callbacks = [ EarlyStopping(monitor='val_accuracy', patience=KERAS_PATIENCE, restore_best_weights=True, verbose=0), ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=KERAS_PATIENCE // 2, min_lr=1e-6, verbose=0) ]; fit_params = {'callbacks': keras_callbacks, 'validation_split': 0.15}\n",
    "        elif model_type == 'mlp':\n",
    "            layer_choices = [(100,), (50, 50), (100, 50), (64, 32, 16), (128, 64), (256, 128)]; layers = trial.suggest_categorical('hidden_layer_sizes', layer_choices)\n",
    "            param = {'hidden_layer_sizes': layers, 'activation': trial.suggest_categorical('activation', ['relu', 'tanh']), 'solver': trial.suggest_categorical('solver', ['adam']), 'alpha': trial.suggest_float('alpha', 1e-6, 1e-2, log=True), 'learning_rate': 'adaptive', 'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True), 'max_iter': trial.suggest_int('max_iter', 400, 1200), 'early_stopping': True, 'n_iter_no_change': KERAS_PATIENCE + 10, 'validation_fraction': 0.15, 'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]), 'random_state': 42, 'warm_start': False }; model = MLPClassifier(**param)\n",
    "        elif model_type == 'randomforest':\n",
    "             param={'n_estimators':trial.suggest_int('n_estimators',100,2000,step=100), 'max_depth':trial.suggest_int('max_depth',5,40,step=5), 'min_samples_split':trial.suggest_int('min_samples_split',2,25), 'min_samples_leaf':trial.suggest_int('min_samples_leaf',1,20), 'max_features':trial.suggest_categorical('max_features',['sqrt','log2',0.6,0.8]), 'bootstrap':trial.suggest_categorical('bootstrap',[True,False]), 'class_weight':trial.suggest_categorical('class_weight',['balanced','balanced_subsample',None]), 'random_state':42, 'n_jobs':n_jobs_optuna}; model=RandomForestClassifier(**param)\n",
    "        elif model_type == 'extratrees':\n",
    "             param={'n_estimators':trial.suggest_int('n_estimators',100,2000,step=100), 'max_depth':trial.suggest_int('max_depth',5,45,step=5), 'min_samples_split':trial.suggest_int('min_samples_split',2,25), 'min_samples_leaf':trial.suggest_int('min_samples_leaf',1,20), 'max_features':trial.suggest_categorical('max_features',['sqrt','log2',0.6,0.8]), 'bootstrap':trial.suggest_categorical('bootstrap',[True,False]), 'class_weight':trial.suggest_categorical('class_weight',['balanced','balanced_subsample',None]), 'random_state':42, 'n_jobs':n_jobs_optuna}; model=ExtraTreesClassifier(**param)\n",
    "        elif model_type == 'logistic':\n",
    "            param={'C':trial.suggest_float('C',1e-4,1e3,log=True), 'penalty':trial.suggest_categorical('penalty',['l1','l2']),'solver':'liblinear','class_weight':'balanced', 'max_iter':trial.suggest_int('max_iter',100,1000), 'random_state':42}; model=LogisticRegression(**param)\n",
    "        elif model_type == 'gradientboosting':\n",
    "             param={'n_estimators':trial.suggest_int('n_estimators',100,1500,step=100), 'learning_rate':trial.suggest_float('learning_rate',0.005,0.3,log=True), 'max_depth':trial.suggest_int('max_depth',3,12), 'min_samples_split':trial.suggest_int('min_samples_split',2,25), 'min_samples_leaf':trial.suggest_int('min_samples_leaf',1,20), 'subsample':trial.suggest_float('subsample',0.5,1.0), 'max_features':trial.suggest_categorical('max_features',['sqrt','log2',None]),'random_state':42}; model=GradientBoostingClassifier(**param)\n",
    "        elif model_type == 'adaboost':\n",
    "            base_depth=trial.suggest_int('base_estimator_max_depth',1,6); param_ada={'n_estimators':trial.suggest_int('n_estimators',50,800,step=50), 'learning_rate':trial.suggest_float('learning_rate',0.01,1.5,log=True), 'algorithm':'SAMME', 'random_state':42}; base_est=DecisionTreeClassifier(max_depth=base_depth,random_state=42); model=AdaBoostClassifier(estimator=base_est,**param_ada); trial.set_user_attr(\"base_estimator_max_depth\",base_depth)\n",
    "        elif model_type == 'knn':\n",
    "            metric=trial.suggest_categorical('metric',['minkowski','manhattan','chebyshev']); param={'n_neighbors':trial.suggest_int('n_neighbors',3,65,step=2), 'weights':trial.suggest_categorical('weights',['uniform','distance']), 'metric':metric,'n_jobs':n_jobs_optuna};\n",
    "            if metric=='minkowski': param['p']=trial.suggest_int('p',1,3)\n",
    "            model=KNeighborsClassifier(**param)\n",
    "        elif model_type == 'svc':\n",
    "             kernel=trial.suggest_categorical('kernel',['rbf','poly','sigmoid', 'linear']); param={'C':trial.suggest_float('C',1e-3,1e4,log=True), 'kernel':kernel, 'probability':True, 'class_weight':'balanced', 'random_state':42, 'cache_size':700 };\n",
    "             if kernel=='poly': param['degree']=trial.suggest_int('degree',2,5)\n",
    "             if kernel in ['rbf','poly','sigmoid']: param['gamma']=trial.suggest_float('gamma',1e-5,1e1,log=True)\n",
    "             model=SVC(**param)\n",
    "        else: logger.error(f\"Unsupported model type: {model_type}\"); raise ValueError(f\"Unsupported: {model_type}\")\n",
    "\n",
    "        # --- Cross-validation ---\n",
    "        scores = []; is_dataframe = isinstance(X, pd.DataFrame)\n",
    "        try:\n",
    "            for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "                X_train = X.iloc[train_idx] if is_dataframe else X[train_idx]; X_valid = X.iloc[valid_idx] if is_dataframe else X[valid_idx]\n",
    "                y_train_fold = y_keras[train_idx] if is_keras else (y.iloc[train_idx] if isinstance(y, pd.Series) else y[train_idx])\n",
    "                y_valid_fold_orig = y.iloc[valid_idx] if isinstance(y, pd.Series) else y[valid_idx]\n",
    "                current_fit_params = fit_params.copy()\n",
    "                try:\n",
    "                    if model_type == 'xgboost': eval_set_xgb = [(X_valid, y_valid_fold_orig)]; model.fit(X_train, y_train_fold, eval_set=eval_set_xgb, early_stopping_rounds=KERAS_PATIENCE, verbose=False)\n",
    "                    elif model_type == 'catboost': current_fit_params['eval_set'] = [(X_valid, y_valid_fold_orig)]; model.fit(X_train, y_train_fold, **current_fit_params)\n",
    "                    elif model_type == 'keras_mlp': model.fit(X_train, y_train_fold, **current_fit_params)\n",
    "                    elif model_type == 'mlp': model.fit(X_train, y_train_fold)\n",
    "                    else: model.fit(X_train, y_train_fold, **current_fit_params)\n",
    "                    y_pred = model.predict(X_valid)\n",
    "                    if is_keras and y_pred.ndim > 1 and y_pred.shape[1] > 1: y_pred = np.argmax(y_pred, axis=1)\n",
    "                    score = accuracy_score(y_valid_fold_orig, y_pred); scores.append(score)\n",
    "                except ValueError as ve: logger.warning(f\"CV fold {fold+1} VAL ERROR {model_type} trial {trial.number}: {ve}\"); return 0.0\n",
    "                except Exception as e: logger.warning(f\"CV fold {fold+1} fail {model_type} trial {trial.number} (GPU:{use_gpu}, Keras:{is_keras}): {e}\", exc_info=False); scores = []; break\n",
    "        except Exception as outer_e: logger.error(f\"Outer CV error {model_type} trial {trial.number} (GPU:{use_gpu}, Keras:{is_keras}): {outer_e}\", exc_info=True); return 0.0\n",
    "        if not scores: logger.error(f\"Cross-validation failed completely for {model_type} trial {trial.number}\"); return 0.0\n",
    "        mean_score = np.mean(scores); logger.debug(f\"Trial {trial.number} ({model_type}) completed. Avg CV Score: {mean_score:.5f}\"); return mean_score\n",
    "\n",
    "    # --- Run Optuna Study ---\n",
    "    study_name = f\"{model_type}_opt_{timestamp}\"; storage_name = f\"sqlite:///optuna_trials/{study_name}.db\"; study = optuna.create_study(direction='maximize', study_name=study_name, storage=storage_name, load_if_exists=True, pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)); completed_trials = len([t for t in study.trials if t.state==optuna.trial.TrialState.COMPLETE]); trials_to_run = n_trials-completed_trials\n",
    "\n",
    "    if trials_to_run > 0:\n",
    "        logger.info(f\"Setting Optuna timeout for {model_type} to {OPTUNA_TIMEOUT_PER_MODEL} seconds.\")\n",
    "        try:\n",
    "            study.optimize(objective, n_trials=trials_to_run, timeout=OPTUNA_TIMEOUT_PER_MODEL, n_jobs=1) # Use n_jobs=1 for stability\n",
    "        except Exception as opt_e:\n",
    "            logger.error(f\"Optuna optimize call failed for {model_type}: {opt_e}\", exc_info=True)\n",
    "            return None, -1, {} # Indicate failure\n",
    "    else:\n",
    "        logger.info(f\"Study '{study_name}' already has {completed_trials} completed trials (target was {n_trials}). Skipping optimization run.\")\n",
    "\n",
    "    # --- Retrieve Results ---\n",
    "    try:\n",
    "        if not any(t.state == optuna.trial.TrialState.COMPLETE for t in study.trials):\n",
    "            logger.error(f\"Optuna study '{study_name}' for {model_type} has no successfully completed trials.\")\n",
    "            return None, -1, {}\n",
    "        best_trial = study.best_trial\n",
    "        best_params = best_trial.params\n",
    "        best_cv_score = best_trial.value\n",
    "    except ValueError:\n",
    "        logger.error(f\"Optuna study '{study_name}' for {model_type} has no best trial available.\")\n",
    "        return None, -1, {}\n",
    "    except Exception as res_e:\n",
    "        logger.error(f\"Error retrieving Optuna results for {model_type}: {res_e}\", exc_info=True)\n",
    "        return None, -1, {}\n",
    "\n",
    "    logger.info(f\"Optimization complete for {model_type}.\")\n",
    "    logger.info(f\"Best CV score: {best_cv_score:.5f}\")\n",
    "    logger.info(f\"Best params: {best_params}\")\n",
    "\n",
    "    # --- Save Study Summary ---\n",
    "    try:\n",
    "        summary_file = f'optuna_trials/{model_type}_study_summary_{timestamp}.txt'\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(f\"Optuna Summary: {model_type}\\nTS: {timestamp}\\nBest Trial: {best_trial.number}\\nScore: {best_cv_score:.5f}\\n\\nParams:\\n\"); params_json = best_params.copy()\n",
    "            if model_type=='adaboost' and \"base_estimator_max_depth\" in best_trial.user_attrs:\n",
    "                params_json['base_estimator_max_depth'] = best_trial.user_attrs[\"base_estimator_max_depth\"]; params_json['algorithm'] = 'SAMME'\n",
    "            if model_type=='xgboost' and 'tree_method' in best_params:\n",
    "                params_json['tree_method'] = best_params['tree_method']\n",
    "            if model_type=='catboost' and 'task_type' in best_params:\n",
    "                params_json['task_type'] = best_params['task_type']\n",
    "            if model_type=='keras_mlp' or model_type=='mlp':\n",
    "                 if 'hidden_layer_sizes' in params_json:\n",
    "                     params_json['hidden_layer_sizes'] = str(params_json['hidden_layer_sizes'])\n",
    "                 if 'n_layers' in best_params:\n",
    "                     units_list = [best_params.get(f'n_units_l{i}') for i in range(best_params['n_layers']) if best_params.get(f'n_units_l{i}')]\n",
    "                     params_json['hidden_units_structure'] = str(units_list)\n",
    "                 params_json = {k: v for k, v in params_json.items() if not k.startswith('n_units_l')}\n",
    "            json.dump(params_json, f, indent=4); logger.info(f\"Saved Optuna summary: {summary_file}\")\n",
    "    except Exception as file_e: logger.warning(f\"Could not save Optuna summary {model_type}: {file_e}\")\n",
    "\n",
    "    # --- Train final model ---\n",
    "    final_model = None; final_fit_params = {}\n",
    "    try:\n",
    "        logger.info(f\"Instantiating final {model_type} model...\")\n",
    "        # --- Instantiate model correctly based on its type (as previously corrected) ---\n",
    "        if model_type == 'adaboost':\n",
    "            best_d = best_trial.user_attrs.get('base_estimator_max_depth', 1); logger.info(f\"Reconstruct AdaBoost DT(max_depth={best_d}) using SAMME\")\n",
    "            base_est_inst = DecisionTreeClassifier(max_depth=best_d, random_state=42); final_p_ada = {k:v for k,v in best_params.items() if k != 'base_estimator_max_depth'}; final_p_ada['algorithm'] = 'SAMME'; final_model = AdaBoostClassifier(estimator=base_est_inst, **final_p_ada)\n",
    "        elif model_type == 'xgboost': final_params_xgb = best_params.copy(); final_params_xgb['objective']='multi:softmax'; final_params_xgb['num_class']=n_classes; final_params_xgb['n_jobs']=1; final_model = XGBClassifier(**final_params_xgb) # Use n_jobs=1 for wrapper\n",
    "        elif model_type == 'catboost': final_params_cat = best_params.copy(); final_params_cat['loss_function']='MultiClass'; final_params_cat['verbose']=False; final_model = CatBoostClassifier(**final_params_cat)\n",
    "        elif model_type == 'mlp': final_model = MLPClassifier(**best_params)\n",
    "        elif model_type == 'keras_mlp':\n",
    "            keras_build_params = { 'n_features': n_features, 'n_classes': n_classes, 'optimizer': best_params.get('optimizer', 'adam'), 'learning_rate': best_params.get('learning_rate', 0.001), 'dropout_rate': best_params.get('dropout_rate', 0.3), 'activation': best_params.get('activation', 'relu') }\n",
    "            if 'n_layers' in best_params:\n",
    "                hidden_units = []\n",
    "                for i in range(best_params['n_layers']):\n",
    "                   unit_val = best_params.get(f'n_units_l{i}')\n",
    "                   if unit_val is not None: hidden_units.append(unit_val)\n",
    "                keras_build_params['hidden_units'] = hidden_units if hidden_units else [64]\n",
    "            else:\n",
    "                hidden_units_final = best_params.get('hidden_layer_sizes', [128, 64])\n",
    "                if isinstance(hidden_units_final, str): \n",
    "                    try: \n",
    "                        hidden_units_final = eval(hidden_units_final)\n",
    "                    except: \n",
    "                        hidden_units_final = [128, 64]\n",
    "                keras_build_params['hidden_units'] = list(hidden_units_final)\n",
    "            final_model = KerasClassifier( model=build_keras_model, model__n_features=keras_build_params['n_features'], model__n_classes=keras_build_params['n_classes'], model__optimizer=keras_build_params['optimizer'], model__learning_rate=keras_build_params['learning_rate'], model__hidden_units=keras_build_params['hidden_units'], model__dropout_rate=keras_build_params['dropout_rate'], model__activation=keras_build_params['activation'], epochs=KERAS_EPOCHS, batch_size=best_params.get('batch_size', 128), verbose=0 )\n",
    "            final_callbacks = [ EarlyStopping(monitor='val_accuracy', patience=KERAS_PATIENCE, restore_best_weights=True, verbose=0), ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=KERAS_PATIENCE // 2, min_lr=1e-6, verbose=0) ]; final_fit_params = {'callbacks': final_callbacks, 'validation_split': 0.15}\n",
    "        elif model_type == 'randomforest': final_params_rf = best_params.copy(); final_params_rf['n_jobs'] = n_jobs_optuna; final_model = RandomForestClassifier(**final_params_rf) # Use defined n_jobs\n",
    "        elif model_type == 'extratrees': final_params_et = best_params.copy(); final_params_et['n_jobs'] = n_jobs_optuna; final_model = ExtraTreesClassifier(**final_params_et) # Use defined n_jobs\n",
    "        elif model_type == 'logistic': final_p_log = best_params.copy(); final_p_log['solver'] = 'liblinear'; \n",
    "        if 'class_weight' not in final_p_log: final_p_log['class_weight'] = 'balanced'; final_model = LogisticRegression(**final_p_log)\n",
    "        elif model_type == 'gradientboosting': final_model = GradientBoostingClassifier(**best_params)\n",
    "        elif model_type == 'knn': final_params_knn = best_params.copy(); final_params_knn['n_jobs'] = n_jobs_optuna; final_model = KNeighborsClassifier(**final_params_knn) # Use defined n_jobs\n",
    "        elif model_type == 'svc': svc_p = best_params.copy(); svc_p['probability'] = True; \n",
    "        if 'class_weight' not in svc_p: svc_p['class_weight'] = 'balanced'; final_model = SVC(**svc_p)\n",
    "\n",
    "        # --- Fit the final model (Check existence first) ---\n",
    "        if final_model is not None:\n",
    "            logger.info(f\"Fitting final {model_type} model...\")\n",
    "            start_fit_time = time.time()\n",
    "            try:\n",
    "                if model_type == 'keras_mlp':\n",
    "                    logger.info(\"Using validation_split for Keras final fit...\")\n",
    "                    final_model.fit(X, y_keras, **final_fit_params)\n",
    "                else:\n",
    "                    final_model.fit(X, y)\n",
    "                fit_duration = time.time() - start_fit_time\n",
    "                logger.info(f\"Final {model_type} fitted in {fit_duration:.2f}s.\")\n",
    "\n",
    "                # --- Save model and importance (AFTER fitting) ---\n",
    "                model_path = f'models/{model_type}_{timestamp}.joblib'\n",
    "                logger.info(f\"Saving final {model_type} model...\")\n",
    "                try:\n",
    "                    if isinstance(final_model, KerasClassifier):\n",
    "                        tf_model_save_path = f'models/{model_type}_tfmodel_{timestamp}'\n",
    "                        try:\n",
    "                            final_model.model_.save(tf_model_save_path)\n",
    "                            logger.info(f\"Saved Keras TF model: {tf_model_save_path}\")\n",
    "                        except Exception as keras_save_err:\n",
    "                            logger.warning(f\"Keras TF save failed ({keras_save_err}), attempt joblib wrapper...\")\n",
    "                            joblib.dump(final_model, model_path)\n",
    "                            logger.info(f\"Saved Keras wrapper via joblib: {model_path}\")\n",
    "                    else:\n",
    "                        joblib.dump(final_model, model_path)\n",
    "                        logger.info(f\"Saved final {model_type} via joblib: {model_path}\")\n",
    "                except Exception as save_err:\n",
    "                    logger.error(f\"Failed save model {model_type}: {save_err}\", exc_info=True)\n",
    "\n",
    "                feat_names = list(X.columns) if isinstance(X, pd.DataFrame) else None\n",
    "                if feat_names:\n",
    "                    logger.info(f\"Saving importance {model_type}...\")\n",
    "                    save_feature_importance(final_model, feat_names, timestamp, model_type)\n",
    "                else:\n",
    "                    logger.warning(f\"No feat names for importance {model_type}.\")\n",
    "            except Exception as fit_save_e:\n",
    "                logger.error(f\"Error during final fit/save {model_type}: {fit_save_e}\", exc_info=True)\n",
    "                return None, best_cv_score, best_params # Return score/params, but no model\n",
    "        else: # Instantiation failed\n",
    "            logger.error(f\"Could not instantiate final model {model_type}.\")\n",
    "            return None, best_cv_score, best_params\n",
    "    except Exception as final_e:\n",
    "        logger.error(f\"Failed final instantiate/fit/save process {model_type}: {final_e}\", exc_info=True)\n",
    "        return None, best_cv_score, best_params\n",
    "\n",
    "    return final_model, best_cv_score, best_params\n",
    "\n",
    "def create_ensemble(qualified_models_with_scores, X_train_ensemble, y_train_ensemble, timestamp, n_jobs_ensemble=18):\n",
    "    \"\"\"\n",
    "    Create ensemble models (Voting and Stacking) from qualified models.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting ensemble creation...\")\n",
    "\n",
    "    if not qualified_models_with_scores:\n",
    "        logger.error(\"No qualified models.\")\n",
    "        return None, None, None\n",
    "\n",
    "    sorted_models = sorted(qualified_models_with_scores, key=lambda x: x[2], reverse=True)\n",
    "    logger.info(f\"Qualified models: {[(m[0], f'{m[2]:.5f}') for m in sorted_models]}\")\n",
    "    N_ens = len(sorted_models)\n",
    "\n",
    "    if N_ens < 2:\n",
    "        logger.warning(f\"Less than 2 qualified models. Skipping ensembles.\")\n",
    "        if N_ens == 1:\n",
    "            n, m, s = sorted_models[0]\n",
    "            logger.info(f\"Returning best individual model: {n} (CV: {s:.5f})\")\n",
    "            return None, None, m\n",
    "        else:\n",
    "            return None, None, None\n",
    "\n",
    "    estimators_valid_for_ensemble = []\n",
    "    keras_models_excluded = []\n",
    "    for name, model, score in sorted_models:\n",
    "        is_keras_wrapper = isinstance(model, KerasClassifier)\n",
    "        model_saved_path = f'models/{name}_{timestamp}.joblib'\n",
    "        tf_model_path = f'models/{name}_tfmodel_{timestamp}'\n",
    "        if is_keras_wrapper and not os.path.exists(tf_model_path) and not os.path.exists(model_saved_path):\n",
    "            logger.warning(f\"Keras model {name} save files missing. Excluding from ensemble.\")\n",
    "            keras_models_excluded.append(name)\n",
    "        else:\n",
    "            estimators_valid_for_ensemble.append((name, model))\n",
    "\n",
    "    if len(estimators_valid_for_ensemble) < 2:\n",
    "        logger.warning(f\"Less than 2 models usable for ensemble. Skipping ensembles.\")\n",
    "        non_keras_models = [(n, m, s) for n, m, s in sorted_models if n not in keras_models_excluded]\n",
    "        if non_keras_models:\n",
    "            best_n, best_m, best_s = non_keras_models[0]\n",
    "            logger.info(f\"Returning best non-excluded model: {best_n} (CV: {best_s:.5f})\")\n",
    "            return None, None, best_m\n",
    "        elif sorted_models:\n",
    "            best_n, best_m, best_s = sorted_models[0]\n",
    "            logger.info(f\"Returning original best model: {best_n} (CV: {best_s:.5f})\")\n",
    "            return None, None, best_m\n",
    "        else:\n",
    "            return None, None, None\n",
    "\n",
    "    logger.info(f\"Using {len(estimators_valid_for_ensemble)} models for ensemble: {[n for n, _ in estimators_valid_for_ensemble]}\")\n",
    "\n",
    "    scores = [s for n, m, s in sorted_models if n in dict(estimators_valid_for_ensemble)]\n",
    "    min_s = min(scores) if scores else 0\n",
    "    shift_s = [s - min_s + 1e-6 for s in scores]\n",
    "    tot_s = sum(shift_s)\n",
    "    norm_w = [s / tot_s for s in shift_s] if tot_s > 0 else None\n",
    "\n",
    "    vote_clf = None\n",
    "    can_soft = all(hasattr(m, 'predict_proba') for _, m in estimators_valid_for_ensemble)\n",
    "\n",
    "    if can_soft:\n",
    "        logger.info(\"Attempting Soft Voting...\")\n",
    "        logger.info(f\"Weights: {list(np.round(norm_w, 3)) if norm_w else 'Uniform'}\")\n",
    "        try:\n",
    "            vote_clf = VotingClassifier(estimators=estimators_valid_for_ensemble, voting='soft', weights=norm_w, n_jobs=n_jobs_ensemble)\n",
    "            vote_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            vote_path = f'models/voting_ensemble_soft_{timestamp}.joblib'\n",
    "            joblib.dump(vote_clf, vote_path)\n",
    "            logger.info(f\"Saved Soft Voting Ensemble: {vote_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Soft Voting: {e}\", exc_info=True)\n",
    "            vote_clf = None\n",
    "            can_soft = False\n",
    "\n",
    "    if not can_soft:\n",
    "        logger.warning(\"Attempting Hard Voting...\")\n",
    "        try:\n",
    "            vote_clf = VotingClassifier(estimators=estimators_valid_for_ensemble, voting='hard', n_jobs=n_jobs_ensemble)\n",
    "            vote_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            vote_path = f'models/voting_ensemble_hard_{timestamp}.joblib'\n",
    "            joblib.dump(vote_clf, vote_path)\n",
    "            logger.info(f\"Saved Hard Voting Ensemble: {vote_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Hard Voting: {e}\", exc_info=True)\n",
    "            vote_clf = None\n",
    "\n",
    "    stack_clf = None\n",
    "    can_stack = all(hasattr(m, 'predict_proba') for _, m in estimators_valid_for_ensemble)\n",
    "    meta_learner = None\n",
    "    if can_stack:\n",
    "        logger.info(\"Attempting Stacking...\")\n",
    "        try:\n",
    "            meta_learner = LogisticRegression(random_state=42, class_weight='balanced', solver='liblinear', C=1.0, n_jobs=1)\n",
    "            stack_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "            stack_clf = StackingClassifier(estimators=estimators_valid_for_ensemble, final_estimator=meta_learner, cv=stack_cv, stack_method='predict_proba', n_jobs=n_jobs_ensemble, passthrough=False)\n",
    "            stack_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            stack_path = f'models/stacking_ensemble_{timestamp}.joblib'\n",
    "            joblib.dump(stack_clf, stack_path)\n",
    "            logger.info(f\"Saved Stacking Ensemble: {stack_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Stacking: {e}\", exc_info=True)\n",
    "            stack_clf = None\n",
    "    else:\n",
    "        logger.warning(\"Cannot create Stacking Ensemble.\")\n",
    "\n",
    "    best_ind_q_model = None\n",
    "    best_n = \"N/A\"\n",
    "    best_s = -1\n",
    "    if sorted_models:\n",
    "        best_n, best_m, best_s = sorted_models[0]\n",
    "        best_ind_q_model = best_m\n",
    "        logger.info(f\"Best individual qualified model: {best_n} (CV: {best_s:.5f})\")\n",
    "\n",
    "    try:\n",
    "        summary_path = f'results/ensemble_creation_summary_{timestamp}.txt'\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(\"Ensemble Summary\\n=============\\nQualified Models:\\n\")\n",
    "            for n, _, s in sorted_models:\n",
    "                f.write(f\"- {n}: CV {s:.5f} {'(Excluded)' if n in keras_models_excluded else '(Included)'}\\n\")\n",
    "            f.write(f\"\\nEnsembles ({len(estimators_valid_for_ensemble)} models):\\n\")\n",
    "            vote_t = 'Soft' if can_soft and vote_clf and vote_clf.voting == 'soft' else ('Hard' if vote_clf and vote_clf.voting == 'hard' else 'N/A')\n",
    "            f.write(f\"- Voting ({vote_t}): {'Saved' if vote_clf else 'Failed/Skipped'}.\\n\")\n",
    "            meta_name = meta_learner.__class__.__name__ if meta_learner and stack_clf else 'N/A'\n",
    "            f.write(f\"- Stacking (Meta: {meta_name}): {'Saved' if stack_clf else 'Failed/Skipped'}.\\n\")\n",
    "            if keras_models_excluded:\n",
    "                f.write(f\"\\nKeras Excluded: {', '.join(keras_models_excluded)}\\n\")\n",
    "            if best_ind_q_model: \n",
    "                f.write(f\"\\nBest individual: {best_n}\\n\")\n",
    "        logger.info(f\"Saved ensemble summary: {summary_path}\")\n",
    "    except Exception as file_e:\n",
    "        logger.warning(f\"Could not save ensemble summary: {file_e}\")\n",
    "\n",
    "    return vote_clf, stack_clf, best_ind_q_model\n",
    "\n",
    "def evaluate_model(model, X_eval, y_eval, model_name, timestamp, le):\n",
    "    if model is None:\n",
    "        logger.warning(f\"Skip eval {model_name}: model None.\")\n",
    "        return None, None\n",
    "        \n",
    "    if le is None:\n",
    "        logger.warning(f\"Skip eval {model_name}: LE None.\")\n",
    "        return None, None\n",
    "        \n",
    "    logger.info(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    if isinstance(y_eval, pd.Series):\n",
    "        y_eval = y_eval.values\n",
    "        \n",
    "    is_keras_wrapper = isinstance(model, KerasClassifier)\n",
    "    \n",
    "    try:\n",
    "        y_pred = model.predict(X_eval)\n",
    "        if is_keras_wrapper and hasattr(model, 'predict_proba') and y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            \n",
    "        acc = accuracy_score(y_eval, y_pred)\n",
    "        \n",
    "        try:\n",
    "            y_eval_lbls = le.inverse_transform(y_eval)\n",
    "            y_pred_lbls = le.inverse_transform(y_pred)\n",
    "            tg_names = le.classes_\n",
    "        except Exception as e_le:\n",
    "            logger.warning(f\"LE inverse fail {model_name}:{e_le}. Use numeric.\")\n",
    "            y_eval_lbls = y_eval\n",
    "            y_pred_lbls = y_pred\n",
    "            tg_names = [str(i) for i in sorted(np.unique(y_eval))]\n",
    "            \n",
    "        rpt_str = classification_report(y_eval_lbls, y_pred_lbls, target_names=tg_names, zero_division=0)\n",
    "        rpt_dict = classification_report(y_eval_lbls, y_pred_lbls, target_names=tg_names, output_dict=True, zero_division=0)\n",
    "        cm = confusion_matrix(y_eval_lbls, y_pred_lbls, labels=tg_names)\n",
    "        \n",
    "        logger.info(f\"{model_name} Eval Acc: {acc:.5f}\")\n",
    "        \n",
    "        eval_fname = f'results/{model_name}_evaluation_{timestamp}.txt'\n",
    "        with open(eval_fname, 'w') as f:\n",
    "            f.write(f\"Model Eval Summary\\n=============\\nModel:{model_name}\\nTS:{timestamp}\\nAcc:{acc:.5f}\\n\\nReport:\\n{rpt_str}\\n\\nCM:\\n{np.array2string(cm)}\\n\")\n",
    "            \n",
    "        logger.info(f\"Saved eval: {eval_fname}\")\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=tg_names, yticklabels=tg_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'CM - {model_name} (Acc:{acc:.3f})')\n",
    "        plt.tight_layout()\n",
    "        plot_fname = f'plots/{model_name}_confusion_matrix_{timestamp}.png'\n",
    "        plt.savefig(plot_fname)\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved CM plot: {plot_fname}\")\n",
    "        \n",
    "        return acc, rpt_dict\n",
    "        \n",
    "    except AttributeError as ae:\n",
    "        logger.error(f\"Eval error {model_name}: AttrErr: {ae}\", exc_info=True)\n",
    "        return None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error eval {model_name}: {e}\", exc_info=True)\n",
    "        return None, None\n",
    "    \n",
    "def make_test_predictions(model, X_test, test_obs, timestamp, model_name, le):\n",
    "    \"\"\"\n",
    "    Generate test predictions and save results to CSV and summary files.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating test predictions using {model_name}...\")\n",
    "\n",
    "    if model is None:\n",
    "        logger.error(f\"Prediction failed for {model_name}: model is None.\")\n",
    "        return None\n",
    "\n",
    "    if le is None:\n",
    "        logger.warning(f\"LabelEncoder is None for {model_name}. Attempting fallback...\")\n",
    "        enc_fs = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "        if enc_fs:\n",
    "            try:\n",
    "                le = joblib.load(f'features/{enc_fs[-1]}')\n",
    "                logger.info(f\"Loaded fallback LabelEncoder: {enc_fs[-1]}.\")\n",
    "            except Exception as load_e:\n",
    "                logger.error(f\"LabelEncoder fallback failed: {load_e}. Prediction aborted.\")\n",
    "                return None\n",
    "        else:\n",
    "            logger.error(f\"Prediction failed: LabelEncoder is None and no fallback available.\")\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        y_pred_enc = model.predict(X_test)\n",
    "\n",
    "        # Handle KerasClassifier predictions\n",
    "        is_keras_wrapper = isinstance(model, KerasClassifier)\n",
    "        if is_keras_wrapper and hasattr(model, 'predict_proba') and y_pred_enc.ndim > 1 and y_pred_enc.shape[1] > 1:\n",
    "            y_pred_enc = np.argmax(y_pred_enc, axis=1)\n",
    "\n",
    "        pred_lbls = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "        # Create submission DataFrame\n",
    "        sub = pd.DataFrame({'obs': test_obs, 'salary_category': pred_lbls})\n",
    "        safe_name = model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\").replace(\" \", \"_\")\n",
    "        sub_path = f'submissions/solution_{safe_name}_{timestamp}.csv'\n",
    "        sub.to_csv(sub_path, index=False)\n",
    "        logger.info(f\"Saved submission: {sub_path}\")\n",
    "\n",
    "        # Log prediction distribution\n",
    "        val_cts = sub['salary_category'].value_counts().to_dict()\n",
    "        logger.info(f\"Test prediction distribution for {model_name}: {val_cts}\")\n",
    "\n",
    "        # Save prediction summary\n",
    "        sum_fname = f'results/{safe_name}_test_prediction_summary_{timestamp}.txt'\n",
    "        with open(sum_fname, 'w') as f:\n",
    "            f.write(f\"Test Prediction Summary - {model_name}\\n\\n\")\n",
    "            try:\n",
    "                m_cls = model.__class__.__name__\n",
    "            except AttributeError:\n",
    "                m_cls = \"N/A\"\n",
    "            f.write(f\"Model Class: {m_cls}\\n\")\n",
    "            f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "            f.write(f\"Total Predictions: {len(pred_lbls)}\\n\")\n",
    "            f.write(\"Distribution:\\n\")\n",
    "            total = len(pred_lbls)\n",
    "            if total > 0:\n",
    "                for lbl, cnt in sorted(val_cts.items()):\n",
    "                    f.write(f\"- {lbl}: {cnt} ({cnt / total:.2%})\\n\")\n",
    "            else:\n",
    "                f.write(\"- No predictions.\\n\")\n",
    "        logger.info(f\"Saved prediction summary: {sum_fname}\")\n",
    "\n",
    "        return sub\n",
    "\n",
    "    except AttributeError as ae:\n",
    "        logger.error(f\"LabelEncoder or prediction error for {model_name}: {ae}.\", exc_info=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction for {model_name}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "# --- Main Pipeline (Uses modified preprocess_data) ---\n",
    "def run_complete_pipeline(perform_feature_selection=False, min_cv_score_threshold=0.72, fs_threshold='mean', n_jobs_sklearn=18):\n",
    "    \"\"\"Run the complete model training pipeline with combined FE logic.\"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    main_log_file = None\n",
    "    file_handler = None\n",
    "    try:\n",
    "        # 1. Setup\n",
    "        print(\"--- Starting Complete Pipeline Run (Combined FE) ---\")\n",
    "        create_directory_structure()\n",
    "        main_log_file = f'logs/pipeline_run_{timestamp}.log'\n",
    "        file_handler = logging.FileHandler(main_log_file)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        if file_handler not in logger.handlers:\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "        logger.info(f\"--- Starting Complete Pipeline Run --- Timestamp: {timestamp} ---\")\n",
    "        logger.info(f\"Pipeline Config: Combined FE, Scaling=True, FeatSelect={perform_feature_selection} (Thresh={fs_threshold}), CV Thresh={min_cv_score_threshold}, n_jobs={n_jobs_sklearn} used for Sklearn, Const Cols Kept in Preproc\")\n",
    "        logger.info(f\"Logging detailed output to: {main_log_file}\")\n",
    "\n",
    "        # 2. Load Data\n",
    "        logger.info(\"Loading data...\")\n",
    "        try:\n",
    "            train_df = pd.read_csv('train.csv')\n",
    "            test_df = pd.read_csv('test.csv')\n",
    "            logger.info(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "        except FileNotFoundError as e:\n",
    "            logger.error(f\"Data load error: {e}.\")\n",
    "            return False\n",
    "        if 'salary_category' not in train_df.columns:\n",
    "            logger.error(\"Target missing.\")\n",
    "            return False\n",
    "        if train_df.empty or test_df.empty:\n",
    "            logger.error(\"Data empty.\")\n",
    "            return False\n",
    "        if 'obs' not in test_df.columns:\n",
    "            logger.error(\"Column 'obs' missing in test.csv.\")\n",
    "            return False\n",
    "            \n",
    "        # Get unique values for manual OHE (CRITICAL: do this before preprocessing)\n",
    "        all_states = set(train_df['job_state'].dropna().unique()).union(set(test_df['job_state'].dropna().unique()))\n",
    "        all_feature1 = set(train_df['feature_1'].dropna().unique()).union(set(test_df['feature_1'].dropna().unique()))\n",
    "        logger.info(f\"Found {len(all_states)} unique states and {len(all_feature1)} unique feature_1 values across train/test.\")\n",
    "\n",
    "        # 3. Preprocess Training Data (Using the NEW preprocess_data)\n",
    "        logger.info(\"Preprocessing training data (using combined logic)...\")\n",
    "        X_train_orig, y_train_orig, feature_cols_initial, label_encoder = preprocess_data(\n",
    "            train_df, all_states, all_feature1, timestamp, is_training=True\n",
    "        )\n",
    "        if X_train_orig is None or y_train_orig is None or label_encoder is None or feature_cols_initial is None:\n",
    "            logger.error(\"Train preprocess failed.\")\n",
    "            return False\n",
    "        logger.info(f\"Train preprocess done. Initial Feats: {X_train_orig.shape[1]}\")\n",
    "        y_train_orig = pd.Series(y_train_orig)  # Ensure Series\n",
    "\n",
    "        # 4. Train/Validation Split\n",
    "        logger.info(\"Splitting data (80/20)...\")\n",
    "        X_train_full, X_val, y_train_full, y_val = train_test_split(X_train_orig, y_train_orig, test_size=0.20, random_state=42, stratify=y_train_orig)\n",
    "        logger.info(f\"Train (Pre-scale): {X_train_full.shape}, Val (Pre-scale): {X_val.shape}\")\n",
    "        y_train_full = pd.Series(y_train_full, index=X_train_full.index)\n",
    "        y_val = pd.Series(y_val, index=X_val.index)\n",
    "\n",
    "        # 5. SCALING STEP\n",
    "        logger.info(\"Applying StandardScaler...\")\n",
    "        scaler = StandardScaler()\n",
    "        # Fit scaler only on the columns that exist in the training partition\n",
    "        X_train_full_scaled = scaler.fit_transform(X_train_full[feature_cols_initial])\n",
    "        X_val_scaled = scaler.transform(X_val[feature_cols_initial])\n",
    "        X_train_full_scaled = pd.DataFrame(X_train_full_scaled, index=X_train_full.index, columns=feature_cols_initial)\n",
    "        X_val_scaled = pd.DataFrame(X_val_scaled, index=X_val.index, columns=feature_cols_initial)\n",
    "        scaler_path = f'scalers/scaler_{timestamp}.joblib'\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        logger.info(f\"Scaler saved: {scaler_path}\")\n",
    "        logger.info(f\"Scaled Train shape: {X_train_full_scaled.shape}, Scaled Val shape: {X_val_scaled.shape}\")\n",
    "\n",
    "        # 6. Preprocess & Scale Test Data (Using the NEW preprocess_data)\n",
    "        logger.info(\"Preprocessing test data (using combined logic)...\")\n",
    "        # Pass the *initial* feature list determined during training preprocessing\n",
    "        X_test_orig, _, _, _ = preprocess_data(\n",
    "            test_df, all_states, all_feature1, timestamp, is_training=False, feature_columns_to_use=feature_cols_initial\n",
    "        )\n",
    "        if X_test_orig is None:\n",
    "            logger.error(\"Test preprocess failed.\")\n",
    "            return False\n",
    "            \n",
    "        # Ensure columns match (should be handled by preprocess_data now)\n",
    "        try:\n",
    "            X_test_aligned = X_test_orig[feature_cols_initial]  # Align columns\n",
    "            logger.info(\"Test columns aligned.\")\n",
    "        except KeyError as ke:\n",
    "            logger.error(f\"Test col mismatch after preprocess: {ke}.\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(\"Scaling test data...\")\n",
    "        X_test_scaled = scaler.transform(X_test_aligned)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test_aligned.index, columns=feature_cols_initial)\n",
    "        logger.info(f\"Test preprocess & scale done. Shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        # Define Data Partitions\n",
    "        X_opt_train = X_train_full_scaled.copy()\n",
    "        y_opt_train = y_train_full.copy()\n",
    "        X_holdout_val = X_val_scaled.copy()\n",
    "        y_holdout_val = y_val.copy()\n",
    "        X_final_test = X_test_scaled.copy()\n",
    "        current_feature_cols = list(feature_cols_initial)  # Start with all features\n",
    "\n",
    "        # 7. Optional Feature Selection (On SCALED data)\n",
    "        if perform_feature_selection:\n",
    "            logger.info(f\"Performing feature selection (Threshold: {fs_threshold})...\")\n",
    "            try:\n",
    "                # Note: RF selector might rank constant columns low, effectively removing them here if threshold allows\n",
    "                selector_model = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=n_jobs_sklearn, class_weight='balanced', max_depth=20)  # Use param\n",
    "                logger.info(\"Fitting RF for feature selection...\")\n",
    "                selector_model.fit(X_opt_train, y_opt_train)\n",
    "                selector = SelectFromModel(selector_model, threshold=fs_threshold, prefit=True)  # Use specified threshold\n",
    "                selected_mask = selector.get_support()\n",
    "                selected_features = X_opt_train.columns[selected_mask]\n",
    "                num_orig = X_opt_train.shape[1]\n",
    "                num_sel = len(selected_features)\n",
    "                if num_sel < num_orig:\n",
    "                    num_removed = num_orig - num_sel\n",
    "                    logger.info(f\"Feat selection removed {num_removed} features. Selected {num_sel}.\")\n",
    "                    current_feature_cols = list(selected_features)\n",
    "                    X_opt_train = X_opt_train[current_feature_cols]\n",
    "                    X_holdout_val = X_holdout_val[current_feature_cols]\n",
    "                    X_final_test = X_final_test[current_feature_cols]\n",
    "                    logger.info(f\"Selection applied to train/val/test.\")\n",
    "                    joblib.dump(current_feature_cols, f'features/selected_feature_columns_{timestamp}.joblib')\n",
    "                else:\n",
    "                    logger.info(f\"Feature selection removed no features with threshold '{fs_threshold}'.\")\n",
    "                    perform_feature_selection = False  # Update flag\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error feature selection: {e}. Use all scaled.\", exc_info=True)\n",
    "                perform_feature_selection = False\n",
    "                current_feature_cols = list(feature_cols_initial)\n",
    "                X_opt_train = X_train_full_scaled[current_feature_cols]\n",
    "                X_holdout_val = X_val_scaled[current_feature_cols]\n",
    "                X_final_test = X_test_scaled[current_feature_cols]\n",
    "        else:\n",
    "            logger.info(\"Skipping feature selection.\")\n",
    "            X_opt_train = X_opt_train[current_feature_cols]\n",
    "            X_holdout_val = X_holdout_val[current_feature_cols]\n",
    "            X_final_test = X_final_test[current_feature_cols]\n",
    "\n",
    "        logger.info(f\"Data shapes post-scaling/selection: Train={X_opt_train.shape}, Val={X_holdout_val.shape}, Test={X_final_test.shape}\")\n",
    "        logger.info(f\"Number of features used in modeling: {len(current_feature_cols)}\")\n",
    "\n",
    "        # 8. Optimize, Train Base Models & Make Individual Predictions\n",
    "        models_to_optimize = [\n",
    "            ('logistic', 30), ('knn', 25), ('adaboost', 30),\n",
    "            ('randomforest', 40), ('extratrees', 40),\n",
    "            ('gradientboosting', 40), ('mlp', 25), ('keras_mlp', 20),\n",
    "            ('catboost', 40), ('xgboost', 50)\n",
    "        ]\n",
    "        qualified_models_with_scores = []\n",
    "        optimized_params_all = {}\n",
    "        logger.info(f\"--- Optimizing models & Making Individual Predictions (Thresh: {min_cv_score_threshold}) ---\")\n",
    "        for model_name, n_trials in models_to_optimize:\n",
    "            indiv_sub_df = None\n",
    "            try:\n",
    "                logger.info(f\"--- Optimizing {model_name} ---\")\n",
    "                final_model, best_cv_score, best_params = optimize_model(\n",
    "                    X_opt_train, y_opt_train, timestamp, model_name, n_trials=n_trials, n_jobs_optuna=n_jobs_sklearn\n",
    "                )\n",
    "                if final_model is not None and best_cv_score is not None and best_cv_score >= min_cv_score_threshold:\n",
    "                    logger.info(f\"+++ QUALIFIED: {model_name} (CV Score: {best_cv_score:.5f})\")\n",
    "                    qualified_models_with_scores.append((model_name, final_model, best_cv_score))\n",
    "                    if best_params:\n",
    "                        optimized_params_all[model_name] = best_params\n",
    "                    logger.info(f\"--- Evaluating {model_name} on HOLD-OUT set ---\")\n",
    "                    holdout_acc, _ = evaluate_model(final_model, X_holdout_val, y_holdout_val, f\"{model_name}_qualified_holdout_eval\", timestamp, label_encoder)\n",
    "                    if holdout_acc is not None:\n",
    "                        logger.info(f\"Hold-out Acc ({model_name}): {holdout_acc:.5f}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Hold-out Eval failed for {model_name}.\")\n",
    "                    logger.info(f\"--- Generating individual predictions for {model_name} ---\")\n",
    "                    indiv_sub_df = make_test_predictions(final_model, X_final_test, test_df['obs'], timestamp, f\"{model_name}_qual_individual_pred\", label_encoder)\n",
    "                    if indiv_sub_df is not None:\n",
    "                        logger.info(f\"Individual prediction file saved for {model_name}.\")\n",
    "                    else:\n",
    "                        logger.error(f\"Failed individual predictions for {model_name}.\")\n",
    "                elif best_cv_score is not None:\n",
    "                    logger.info(f\"--- NOT QUALIFIED: {model_name} (CV Score: {best_cv_score:.5f} {' - Final fit/save failed' if final_model is None else ''}) ---\")\n",
    "                    if best_params:\n",
    "                        optimized_params_all[model_name] = best_params\n",
    "                else:\n",
    "                    logger.warning(f\"Optimization failed for {model_name}. Skip.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in main loop for {model_name}: {e}\", exc_info=True)\n",
    "\n",
    "        logger.info(\"--- Model Optimization Phase Complete ---\")\n",
    "        if not qualified_models_with_scores:\n",
    "            logger.error(f\"CRITICAL: NO models met CV threshold {min_cv_score_threshold}. Abort.\")\n",
    "            if file_handler:\n",
    "                logger.removeHandler(file_handler)\n",
    "                file_handler.close()\n",
    "            return False\n",
    "            \n",
    "        logger.info(f\"--- {len(qualified_models_with_scores)} models qualified. ---\")\n",
    "        logger.info(f\"Qualified Models (Name, CV Score): {[(m[0], f'{m[2]:.5f}') for m in qualified_models_with_scores]}\")\n",
    "\n",
    "        # 9. Create Ensembles & Select FINAL Best Model\n",
    "        final_model = None\n",
    "        final_model_name = \"N/A\"\n",
    "        vote_ens = None\n",
    "        stack_ens = None\n",
    "        best_ind_q_model = None\n",
    "        if len(qualified_models_with_scores) == 1:\n",
    "            final_model_name, final_model, final_cv_score = qualified_models_with_scores[0]\n",
    "            logger.warning(f\"Only 1 qualified: {final_model_name} (CV:{final_cv_score:.5f}). Select it.\")\n",
    "            best_ind_q_model = final_model\n",
    "        elif len(qualified_models_with_scores) > 1:\n",
    "            logger.info(f\"--- Creating and Evaluating Ensembles ---\")\n",
    "            vote_ens, stack_ens, best_ind_q_model = create_ensemble(\n",
    "                qualified_models_with_scores, X_opt_train, y_opt_train, timestamp, n_jobs_ensemble=n_jobs_sklearn  # Pass n_jobs\n",
    "            )\n",
    "            logger.info(\"--- Evaluating candidate final models on HOLD-OUT validation set ---\")\n",
    "            candidates = {}\n",
    "            best_ind_name = None\n",
    "            if vote_ens:\n",
    "                vote_model_name = f\"voting_ensemble_{vote_ens.voting}_qualified\"\n",
    "                logger.info(f\"--- Eval {vote_model_name} ---\")\n",
    "                val_acc, _ = evaluate_model(vote_ens, X_holdout_val, y_holdout_val, f\"{vote_model_name}_holdout_eval\", timestamp, label_encoder)\n",
    "                if val_acc is not None:\n",
    "                    candidates[vote_model_name] = (val_acc, vote_ens)\n",
    "                    logger.info(f\"Hold-out Acc ({vote_model_name}): {val_acc:.5f}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Eval fail: {vote_model_name}\")\n",
    "            if stack_ens:\n",
    "                stack_model_name = \"stacking_ensemble_qualified\"\n",
    "                logger.info(f\"--- Eval {stack_model_name} ---\")\n",
    "                val_acc, _ = evaluate_model(stack_ens, X_holdout_val, y_holdout_val, f\"{stack_model_name}_holdout_eval\", timestamp, label_encoder)\n",
    "                if val_acc is not None:\n",
    "                    candidates[stack_model_name] = (val_acc, stack_ens)\n",
    "                    logger.info(f\"Hold-out Acc ({stack_model_name}): {val_acc:.5f}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Eval fail: {stack_model_name}\")\n",
    "            if best_ind_q_model:\n",
    "                best_ind_info = next((m for m in qualified_models_with_scores if m[1] == best_ind_q_model), None)\n",
    "                if best_ind_info:\n",
    "                    best_ind_name = best_ind_info[0]\n",
    "                    logger.info(f\"--- Eval Best Indiv ({best_ind_name}) ---\")\n",
    "                    eval_name = f\"{best_ind_name}_best_qual_holdout_eval\"\n",
    "                    val_acc, _ = evaluate_model(best_ind_q_model, X_holdout_val, y_holdout_val, eval_name, timestamp, label_encoder)\n",
    "                    if val_acc is not None:\n",
    "                        cand_name = f\"{best_ind_name}_best_qualified\"\n",
    "                        candidates[cand_name] = (val_acc, best_ind_q_model)\n",
    "                        logger.info(f\"Hold-out Acc ({best_ind_name}): {val_acc:.5f}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Eval fail: {best_ind_name}\")\n",
    "                else:\n",
    "                    logger.warning(\"Could not find name for best individual.\")\n",
    "            if candidates:\n",
    "                final_model_name = max(candidates, key=lambda k: candidates[k][0])\n",
    "                final_val_score, final_model = candidates[final_model_name]\n",
    "                logger.info(f\"--- FINAL MODEL: '{final_model_name}' (Hold-Out Acc: {final_val_score:.5f}) ---\")\n",
    "            else:\n",
    "                logger.error(\"Hold-out eval failed for all candidates.\")\n",
    "                if best_ind_q_model and best_ind_name:\n",
    "                    final_model = best_ind_q_model\n",
    "                    final_model_name = f\"{best_ind_name}_best_qualified_cv_fallback\"\n",
    "                    logger.warning(f\"FALLBACK: Using '{final_model_name}'.\")\n",
    "                else:\n",
    "                    logger.error(\"No final model fallback.\")\n",
    "                    final_model = None\n",
    "\n",
    "        if not final_model:\n",
    "            logger.error(\"No final model selected. Abort.\")\n",
    "            if file_handler:\n",
    "                logger.removeHandler(file_handler)\n",
    "                file_handler.close()\n",
    "            return False\n",
    "\n",
    "        # 10. Make FINAL Test Predictions\n",
    "        logger.info(f\"--- Generating FINAL predictions using: {final_model_name} ---\")\n",
    "        final_sub_df = make_test_predictions(final_model, X_final_test, test_df['obs'], timestamp, f\"{final_model_name}_FINAL\", label_encoder)\n",
    "        if final_sub_df is None:\n",
    "            logger.error(f\"Failed FINAL submission with {final_model_name}.\")\n",
    "        else:\n",
    "            logger.info(f\"FINAL submission file generated with {final_model_name}.\")\n",
    "\n",
    "        # 11. Final Summary\n",
    "        logger.info(\"--- Pipeline Run Summary ---\")\n",
    "        logger.info(f\"Timestamp: {timestamp}\")\n",
    "        logger.info(f\"Config: Combined FE, Scaling=True, FeatSelect={perform_feature_selection} (Thresh={fs_threshold}), CV Thresh={min_cv_score_threshold}, n_jobs={n_jobs_sklearn}, ConstCols Kept\")\n",
    "        logger.info(f\"Final # Features: {len(current_feature_cols)}\")\n",
    "        logger.info(\"Models Optimized: \" + \", \".join([m[0] for m in models_to_optimize]))\n",
    "        qual_details = [(m[0], f\"{m[2]:.5f}\") for m in qualified_models_with_scores] if qualified_models_with_scores else [\"None\"]\n",
    "        logger.info(\"Models Qualified (Name, CV Score): \" + \", \".join([f\"{n}({s})\" for n, s in qual_details]))\n",
    "        logger.info(f\"Ensembles Created: Voting={'Yes' if vote_ens else 'No'}, Stacking={'Yes' if stack_ens else 'No'}\")\n",
    "        logger.info(f\"Final model selected: {final_model_name}\")\n",
    "        logger.info(\"Individual predictions saved for qualified models.\")\n",
    "        if final_sub_df is not None:\n",
    "            safe_final_n = final_model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\").replace(\" \", \"_\")\n",
    "            final_sub_path = f\"submissions/solution_{safe_final_n}_FINAL_{timestamp}.csv\"\n",
    "            logger.info(f\"Final submission file: {final_sub_path}\")\n",
    "        else:\n",
    "            logger.warning(\"No FINAL submission file generated.\")\n",
    "        logger.info(f\"Logs in: {main_log_file}\")\n",
    "        logger.info(\"--- Pipeline Completed Successfully ---\")\n",
    "        if file_handler:\n",
    "            logger.removeHandler(file_handler)\n",
    "            file_handler.close()\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"--- Pipeline Failed Critically --- Error: {e}\", exc_info=True)\n",
    "        if file_handler and file_handler in logger.handlers:\n",
    "            logger.removeHandler(file_handler)\n",
    "            file_handler.close()\n",
    "        return False\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    N_CORES_TO_USE = 18 # Define the number of cores to use for scikit-learn tasks (adjust as needed)\n",
    "\n",
    "    start_time = time.time()\n",
    "    success = run_complete_pipeline(\n",
    "        perform_feature_selection=True,\n",
    "        fs_threshold='mean', # Use 'mean' threshold like simpler model, or keep 'median'\n",
    "        min_cv_score_threshold=0.72, # Adjust as needed\n",
    "        n_jobs_sklearn=N_CORES_TO_USE # Pass the core count\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    status_msg = f\"Pipeline execution {'succeeded' if success else 'failed'}.\"\n",
    "    duration_msg = f\"Total time: {duration:.2f} sec ({duration/60:.2f} min).\"\n",
    "    print(f\"\\n{'='*30}\\n{status_msg}\")\n",
    "    print(duration_msg)\n",
    "    print(f\"{'='*30}\")\n",
    "    try: # Log final status if possible\n",
    "        logger.info(status_msg)\n",
    "        logger.info(duration_msg)\n",
    "    except Exception as log_final_e:\n",
    "        print(f\"Final logging failed: {log_final_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f360e16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ce4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281d52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f7dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04956581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
