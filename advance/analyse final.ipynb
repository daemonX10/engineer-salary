{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2497f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61729fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in .\\supernova\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in .\\supernova\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in .\\supernova\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in .\\supernova\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in .\\supernova\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: tensorflow in .\\supernova\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: scikeras in .\\supernova\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: category-encoders in .\\supernova\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: optuna in .\\supernova\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: joblib in .\\supernova\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: xgboost in .\\supernova\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: catboost in .\\supernova\\lib\\site-packages (1.2.8)\n",
      "Requirement already satisfied: lightgbm in .\\supernova\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: imblearn in .\\supernova\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\supernova\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\supernova\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\supernova\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\supernova\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\supernova\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\supernova\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\supernova\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\supernova\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\supernova\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in .\\supernova\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in .\\supernova\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in .\\supernova\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in .\\supernova\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in .\\supernova\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in .\\supernova\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in .\\supernova\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in .\\supernova\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in .\\supernova\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in .\\supernova\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in .\\supernova\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in .\\supernova\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in .\\supernova\\lib\\site-packages (from tensorflow) (79.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in .\\supernova\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in .\\supernova\\lib\\site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in .\\supernova\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in .\\supernova\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in .\\supernova\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in .\\supernova\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in .\\supernova\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in .\\supernova\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in .\\supernova\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: patsy>=0.5.1 in .\\supernova\\lib\\site-packages (from category-encoders) (1.0.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in .\\supernova\\lib\\site-packages (from category-encoders) (0.14.4)\n",
      "Requirement already satisfied: alembic>=1.5.0 in .\\supernova\\lib\\site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in .\\supernova\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in .\\supernova\\lib\\site-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in .\\supernova\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in .\\supernova\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: graphviz in .\\supernova\\lib\\site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: plotly in .\\supernova\\lib\\site-packages (from catboost) (6.0.1)\n",
      "Requirement already satisfied: imbalanced-learn in .\\supernova\\lib\\site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: Mako in .\\supernova\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in .\\supernova\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in .\\supernova\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in .\\supernova\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in .\\supernova\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\supernova\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\supernova\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\supernova\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\supernova\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in .\\supernova\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in .\\supernova\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in .\\supernova\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in .\\supernova\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: colorama in .\\supernova\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in .\\supernova\\lib\\site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in .\\supernova\\lib\\site-packages (from plotly->catboost) (1.36.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in .\\supernova\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in .\\supernova\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in .\\supernova\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in .\\supernova\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn tensorflow scikeras category-encoders optuna joblib xgboost catboost lightgbm imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab21e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-25 18:18:47,999 - INFO - Set LOKY_MAX_CPU_COUNT to 20\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,\n",
    "    AdaBoostClassifier, VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# SVC import is removed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA # Keep PCA if used in FE\n",
    "# TruncatedSVD import removed (can be added back if needed)\n",
    "from category_encoders import TargetEncoder # Keep if used (e.g., for job title)\n",
    "# CatBoostEncoder import removed (can be added back if needed)\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb # Added LightGBM\n",
    "import optuna\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import logging\n",
    "import subprocess\n",
    "import math\n",
    "from sklearn.calibration import CalibratedClassifierCV # <-- Added for Step 2/3 plan\n",
    "\n",
    "# Configure logging (Ensure this runs before logger is used)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Define the global logger instance\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "# Optional: Set LOKY env var if needed for Windows parallelism issues with joblib\n",
    "try:\n",
    "    cpu_count = os.cpu_count()\n",
    "    if cpu_count: os.environ[\"LOKY_MAX_CPU_COUNT\"] = str(cpu_count)\n",
    "    logger.info(f\"Set LOKY_MAX_CPU_COUNT to {os.environ.get('LOKY_MAX_CPU_COUNT')}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not set LOKY_MAX_CPU_COUNT: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3850ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Assume logger is defined globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Creates the necessary directory structure for the project.\"\"\"\n",
    "    directories = ['models', 'features', 'results', 'submissions', 'logs', 'plots', 'optuna_trials', 'scalers', 'calibrated_models'] # Added calibrated_models\n",
    "    logger.info(\"Creating directory structure...\")\n",
    "    for directory in directories:\n",
    "        try:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "                logger.info(f\"Created directory: {directory}\")\n",
    "            # else: # Optional: log if directory already exists\n",
    "                logger.debug(f\"Directory already exists: {directory}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating directory {directory}: {e}\")\n",
    "            # Re-raise the exception to halt execution if directory creation fails,\n",
    "            # as it's likely critical for the pipeline.\n",
    "            raise\n",
    "    logger.info(\"Directory structure verified/created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bab0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Returns the current timestamp in YYYYMMDD_HHMMSS format.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6100ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume logger is defined globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def save_feature_importance(model, feature_names, timestamp, model_name):\n",
    "    \"\"\"\n",
    "    Saves feature importances for compatible models (Tree-based, Linear).\n",
    "    Logs a message for models where standard importance isn't directly available.\n",
    "    \"\"\"\n",
    "    if not feature_names:\n",
    "        logger.warning(f\"No feature names provided for {model_name}. Skipping feature importance.\")\n",
    "        return\n",
    "\n",
    "    importances = None\n",
    "    importance_type = None\n",
    "    is_fitted = True # Assume fitted unless checked otherwise\n",
    "\n",
    "    # --- Model Type Specific Handling ---\n",
    "    if isinstance(model, KerasClassifier):\n",
    "        try:\n",
    "            _ = model.model_ # Check if internal model exists\n",
    "            logger.info(f\"Standard feature importance plot not generated for Keras model {model_name}.\")\n",
    "            logger.info(\"Consider using techniques like Permutation Importance or SHAP.\")\n",
    "        except AttributeError:\n",
    "            logger.warning(f\"Keras model {model_name} not fitted. Skip importance.\")\n",
    "        return # Exit for Keras models\n",
    "\n",
    "    elif isinstance(model, (VotingClassifier, StackingClassifier)):\n",
    "        logger.info(f\"Importance plot not generated for ensemble {model_name}.\")\n",
    "        return # Exit for ensembles\n",
    "\n",
    "    elif isinstance(model, MLPClassifier):\n",
    "        logger.info(f\"Standard feature importance not directly available for MLPClassifier {model_name}.\")\n",
    "        return # Exit for MLP\n",
    "\n",
    "    # Check for standard attributes AFTER handling special cases\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        importance_type = 'Importance'\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        if model.coef_.ndim > 1:\n",
    "            importances = np.abs(model.coef_).mean(axis=0)\n",
    "        else:\n",
    "            importances = np.abs(model.coef_)\n",
    "        importance_type = 'Coefficient Magnitude'\n",
    "    elif hasattr(model, 'estimator_') and hasattr(model.estimator_, 'feature_importances_'):\n",
    "        # Handle cases like AdaBoost where the base estimator holds importance\n",
    "        # Ensure estimator_ exists and has the attribute\n",
    "        if getattr(model, 'estimator_', None) and hasattr(model.estimator_, 'feature_importances_'):\n",
    "             logger.info(f\"Using importance from base estimator ({model.estimator_.__class__.__name__}) of {model_name}.\")\n",
    "             importances = model.estimator_.feature_importances_\n",
    "             importance_type = 'Base Estimator Importance'\n",
    "        else:\n",
    "             logger.warning(f\"Base estimator not found or lacks importance for {model_name}.\")\n",
    "             return\n",
    "\n",
    "    # Add check for LightGBM specifically if feature_importances_ isn't present on fitted model sometimes\n",
    "    elif isinstance(model, lgb.LGBMClassifier) and hasattr(model, 'booster_'):\n",
    "         try:\n",
    "             importances = model.booster_.feature_importance(importance_type='gain') # Or 'split'\n",
    "             importance_type = 'LGBM Gain'\n",
    "             logger.info(f\"Using booster_.feature_importance() for {model_name}.\")\n",
    "         except Exception as lgbm_imp_err:\n",
    "              logger.warning(f\"Could not get LGBM importance via booster_: {lgbm_imp_err}\")\n",
    "              return\n",
    "    else:\n",
    "        logger.info(f\"Model {model_name} ({model.__class__.__name__}) lacks standard importance attributes (feature_importances_, coef_, relevant estimator_).\")\n",
    "        return\n",
    "\n",
    "    # --- Process and Save Importances (if found) ---\n",
    "    if importances is None:\n",
    "        logger.warning(f\"Could not retrieve importances for {model_name}.\")\n",
    "        return\n",
    "\n",
    "    if isinstance(importances, list): # Ensure numpy array\n",
    "        importances = np.array(importances)\n",
    "\n",
    "    if importances.ndim > 1:\n",
    "        logger.warning(f\"Importances shape {importances.shape} for {model_name}. Taking mean over axis 0.\")\n",
    "        importances = importances.mean(axis=0)\n",
    "\n",
    "    if len(importances) != len(feature_names):\n",
    "        logger.warning(f\"Importance length ({len(importances)}) vs names ({len(feature_names)}) mismatch for {model_name}.\")\n",
    "        return\n",
    "\n",
    "    # --- Create DataFrame and Plot ---\n",
    "    try:\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "        # Handle potential NaN/Inf in importance values before sorting\n",
    "        importance_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        importance_df.dropna(subset=['Importance'], inplace=True)\n",
    "        if importance_df.empty:\n",
    "             logger.warning(f\"Importance DataFrame became empty after dropping NaN/Inf for {model_name}.\")\n",
    "             return\n",
    "\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_n = min(30, len(importance_df))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df.head(top_n), palette='viridis')\n",
    "        plt.title(f'Top {top_n} Feature Importances - {model_name}')\n",
    "        plt.xlabel(f'Relative {importance_type}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Ensure directories exist before saving\n",
    "        plot_dir = 'plots'\n",
    "        results_dir = 'results'\n",
    "        if not os.path.exists(plot_dir): os.makedirs(plot_dir)\n",
    "        if not os.path.exists(results_dir): os.makedirs(results_dir)\n",
    "\n",
    "        plot_filename = os.path.join(plot_dir, f'{model_name}_feature_importance_{timestamp}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved importance plot: {plot_filename}\")\n",
    "\n",
    "        csv_filename = os.path.join(results_dir, f'{model_name}_feature_importance_{timestamp}.csv')\n",
    "        importance_df.to_csv(csv_filename, index=False)\n",
    "        logger.info(f\"Saved importance csv: {csv_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not save importance plot/CSV for {model_name}: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c9c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_keras_model(n_features, n_classes, optimizer='adam', learning_rate=0.001,\n",
    "                      hidden_units=[128, 64], dropout_rate=0.3, activation='relu', l2_reg=1e-4):\n",
    "    \"\"\"Builds a Keras MLP model with specified architecture and hyperparameters.\"\"\"\n",
    "\n",
    "    model = keras.Sequential(name=\"keras_mlp_tabular\")\n",
    "    model.add(layers.Input(shape=(n_features,)))\n",
    "\n",
    "    # Optional: Batch Norm before the first layer\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Hidden Layers\n",
    "    for units in hidden_units:\n",
    "        model.add(layers.Dense(\n",
    "            units,\n",
    "            kernel_regularizer=keras.regularizers.l2(l2_reg) # Add L2 regularization\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization()) # Batch Norm after Dense layer\n",
    "        model.add(layers.Activation(activation)) # Activation after Batch Norm\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    # Select Optimizer\n",
    "    if optimizer.lower() == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer.lower() == 'sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        logger.warning(f\"Unsupported optimizer '{optimizer}'. Defaulting to Adam.\")\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Log model summary details\n",
    "    logger.info(f\"Keras model built: Input({n_features}), Hidden({len(hidden_units)} layers, units={hidden_units}), Output({n_classes})\")\n",
    "    logger.info(f\" Activation: {activation}, Dropout: {dropout_rate}, L2 Reg: {l2_reg}, Optimizer: {optimizer}, LR: {learning_rate}\")\n",
    "    # Optional detailed summary:\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    short_model_summary = \"\\n\".join(stringlist)\n",
    "    logger.debug(f\"Keras Model Summary:\\n{short_model_summary}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e9b3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_data(df, all_states, all_feature1, timestamp, is_training=True, feature_columns_to_use=None):\n",
    "    \"\"\"\n",
    "    Preprocesses data using combined FE logic and robust categorical handling.\n",
    "    Warns about constant columns in training data but does not drop them.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting preprocessing (Combined Logic). Is training: {is_training}\")\n",
    "    start_time = time.time()\n",
    "    data = df.copy()\n",
    "    y = None\n",
    "    le = None\n",
    "    target_column = 'salary_category'\n",
    "\n",
    "    # 1. Handle Target Variable\n",
    "    if target_column in data.columns and is_training:\n",
    "        target = data[target_column]\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(target)\n",
    "        logger.info(f\"Target '{target_column}' label encoded.\")\n",
    "        # Save encoder and mapping\n",
    "        if not os.path.exists('features'): os.makedirs('features')\n",
    "        joblib.dump(le, f'features/label_encoder_{timestamp}.joblib')\n",
    "        mapping = {int(v): k for k, v in zip(le.classes_, le.transform(le.classes_))}\n",
    "        mapping_file = f'features/target_mapping_{timestamp}.json'\n",
    "        with open(mapping_file, 'w') as f: json.dump(mapping, f, indent=4)\n",
    "        logger.info(\"Saved label encoder and mapping.\")\n",
    "    elif not is_training:\n",
    "        # Load encoder\n",
    "        try:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "            if encoder_files: le = joblib.load(f'features/{encoder_files[-1]}'); logger.info(f\"Loaded LE: {encoder_files[-1]}\")\n",
    "            else: logger.warning(\"No LE file found!\"); le = None\n",
    "        except Exception as e: logger.error(f\"Failed load LE: {e}\"); le = None\n",
    "    elif is_training: # Training mode but no target\n",
    "        logger.error(f\"Target column '{target_column}' missing in training data!\")\n",
    "        raise ValueError(f\"Target column '{target_column}' not found.\")\n",
    "\n",
    "    # 2. Define Feature Groups\n",
    "    boolean_features = [f for f in ['feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_10', 'feature_11'] if f in data.columns]\n",
    "    numerical_features = [f for f in ['feature_2', 'feature_9', 'feature_12'] if f in data.columns]\n",
    "    job_desc_cols = [col for col in data.columns if col.startswith('job_desc_')]\n",
    "    all_numerical_features = numerical_features + job_desc_cols\n",
    "\n",
    "    # 3. Initial Cleaning\n",
    "    logger.info(\"Initial cleaning: Numerical and Boolean Features...\")\n",
    "    for col in all_numerical_features:\n",
    "        if col in data.columns:\n",
    "            if data[col].dtype == 'object': data[col] = data[col].replace(['', ' ', 'NA', 'None', 'NULL'], np.nan)\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "            median_val = data[col].median(); fill_value = median_val if not pd.isna(median_val) else 0\n",
    "            data[col] = data[col].fillna(fill_value)\n",
    "    for col in boolean_features:\n",
    "        if col in data.columns:\n",
    "            numeric_view = pd.to_numeric(data[col], errors='coerce'); is_boolean_like = numeric_view.dropna().isin([0, 1]).all()\n",
    "            if is_boolean_like: data[col] = numeric_view.fillna(0).astype(int)\n",
    "            else:\n",
    "                num_non_bool = numeric_view.dropna().loc[~numeric_view.dropna().isin([0, 1])].count()\n",
    "                logger.warning(f\"Col '{col}' has non-0/1 vals ({num_non_bool}). Treat as numeric, impute median.\")\n",
    "                median_val = numeric_view.median(); fill_value = median_val if not pd.isna(median_val) else 0\n",
    "                data[col] = numeric_view.fillna(fill_value)\n",
    "\n",
    "    logger.info(\"Starting Feature Engineering...\")\n",
    "    engineered_feature_names = []\n",
    "    target_encoded_title = 'job_title_encoded' # Define expected name\n",
    "\n",
    "    # --- Feature Engineering Steps (Combined logic from analysis) ---\n",
    "    if 'job_title' in data.columns:\n",
    "        data['job_title'] = data['job_title'].fillna('Unknown')\n",
    "        title_flags = ['is_senior', 'is_junior', 'is_developer', 'is_specialist']\n",
    "        data['is_senior'] = data['job_title'].str.lower().str.contains('senior|sr|lead|principal').fillna(False).astype(int)\n",
    "        data['is_junior'] = data['job_title'].str.lower().str.contains('junior|jr|associate|entry').fillna(False).astype(int)\n",
    "        data['is_developer'] = data['job_title'].str.lower().str.contains('develop|programmer|coder|engineer').fillna(False).astype(int)\n",
    "        data['is_specialist'] = data['job_title'].str.lower().str.contains('special|expert|consult').fillna(False).astype(int)\n",
    "        engineered_feature_names.extend(title_flags)\n",
    "        title_counts = data['job_title'].value_counts(); rare_titles = title_counts[title_counts < 10].index\n",
    "        data['job_title_grouped'] = data['job_title'].apply(lambda x: 'Other_Title' if x in rare_titles else x)\n",
    "        title_encoder_col = 'job_title_grouped'; engineered_feature_names.append(target_encoded_title)\n",
    "        if is_training:\n",
    "            job_encoder = TargetEncoder(cols=[title_encoder_col], handle_missing='value', handle_unknown='value')\n",
    "            data[target_encoded_title] = job_encoder.fit_transform(data[[title_encoder_col]], y) # Use encoded y\n",
    "            joblib.dump(job_encoder, f'features/job_title_encoder_{timestamp}.joblib')\n",
    "            logger.info(f\"Fit/saved TE for {title_encoder_col}\")\n",
    "        else:\n",
    "            encoder_path = f'features/job_title_encoder_{timestamp}.joblib'\n",
    "            fallback_files = sorted([f for f in os.listdir('features') if f.startswith('job_title_encoder_')])\n",
    "            loaded = False\n",
    "            \n",
    "            if os.path.exists(encoder_path):\n",
    "                try:\n",
    "                    job_encoder = joblib.load(encoder_path)\n",
    "                    data[target_encoded_title] = job_encoder.transform(data[[title_encoder_col]])\n",
    "                    loaded = True\n",
    "                    logger.info(f\"Loaded TE: {encoder_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed load TE '{encoder_path}': {e}. Try fallback.\")\n",
    "                    \n",
    "            if not loaded and fallback_files:\n",
    "                latest_encoder = fallback_files[-1]\n",
    "                try:\n",
    "                    job_encoder = joblib.load(f'features/{latest_encoder}')\n",
    "                    data[target_encoded_title] = job_encoder.transform(data[[title_encoder_col]])\n",
    "                    loaded = True\n",
    "                    logger.info(f\"Loaded fallback TE: {latest_encoder}\")\n",
    "                except Exception as e_fb:\n",
    "                    logger.error(f\"Fallback TE failed: {e_fb}. Fill 0.5\")\n",
    "            if not loaded: logger.error(\"No TE found. Fill 0.5\"); data[target_encoded_title] = 0.5\n",
    "        data = data.drop(['job_title', 'job_title_grouped'], axis=1, errors='ignore'); logger.info(\"Processed 'job_title'.\")\n",
    "\n",
    "    if 'job_posted_date' in data.columns:\n",
    "        data['job_posted_date'] = data['job_posted_date'].fillna('2000/01')\n",
    "        def extract_year(d): \n",
    "            try: return int(str(d)[:4]) \n",
    "            except: return 2000\n",
    "        def extract_month(d): \n",
    "            try: return int(str(d).split('/')[1]) \n",
    "            except: return 1\n",
    "        data['job_posted_year'] = data['job_posted_date'].apply(extract_year); data['job_posted_month'] = data['job_posted_date'].apply(extract_month); data['job_posted_month'] = data['job_posted_month'].clip(1, 12)\n",
    "        date_features = ['month_sin', 'month_cos', 'job_recency', 'job_posted_year_norm']\n",
    "        data['month_sin'] = np.sin(2 * np.pi * data['job_posted_month'] / 12); data['month_cos'] = np.cos(2 * np.pi * data['job_posted_month'] / 12); data['job_recency'] = data['job_posted_year'] * 12 + data['job_posted_month']\n",
    "        mean_year = 2022; data['job_posted_year_norm'] = data['job_posted_year'] - mean_year\n",
    "        engineered_feature_names.extend(date_features)\n",
    "        data = data.drop(['job_posted_date', 'job_posted_year', 'job_posted_month'], axis=1, errors='ignore'); logger.info(\"Processed 'job_posted_date'.\")\n",
    "\n",
    "    num_transform_features = []\n",
    "    # Feature 9 processing\n",
    "    if 'feature_9' in data.columns:\n",
    "        try: data['feature_9_bin'] = pd.qcut(data['feature_9'].rank(method='first'), q=5, labels=[0, 1, 2, 3, 4], duplicates='drop').astype(int)\n",
    "        except ValueError: logger.warning(\"qcut fail f9, use cut.\")\n",
    "        try: \n",
    "            data['feature_9_bin'] = pd.cut(data['feature_9'], bins=5, labels=[0, 1, 2, 3, 4], include_lowest=True, duplicates='drop').astype(int) \n",
    "        except Exception as e_cut: logger.error(f\"Cut fail f9: {e_cut}. Bin 0.\"); data['feature_9_bin'] = 0\n",
    "        data['feature_9_bin'] = data['feature_9_bin'].fillna(int(data['feature_9_bin'].median())) # Fill potential NaNs from cut\n",
    "        num_transform_features.append('feature_9_bin'); logger.info(\"Added bin f9.\")\n",
    "        if 'feature_2' in data.columns: interaction_name = 'feature_2_9_interaction'; data[interaction_name] = data['feature_2'] * data['feature_9']; num_transform_features.append(interaction_name); logger.info(f\"Added: {interaction_name}\")\n",
    "\n",
    "    # Feature 2 processing\n",
    "    if 'feature_2' in data.columns:\n",
    "        # Log transform (handle potential zeros/negatives if necessary)\n",
    "        data['feature_2_log'] = np.log1p(data['feature_2']) # Add log transform\n",
    "        num_transform_features.append('feature_2_log')\n",
    "\n",
    "        data['feature_2_squared'] = data['feature_2'] ** 2\n",
    "        data['feature_2_sqrt'] = np.sqrt(np.abs(data['feature_2']))\n",
    "        num_transform_features.extend(['feature_2_squared', 'feature_2_sqrt'])\n",
    "        try: \n",
    "            data['feature_2_bin'] = pd.qcut(data['feature_2'].rank(method='first'), q=5, labels=[0, 1, 2, 3, 4], duplicates='drop').astype(int)\n",
    "        except ValueError: \n",
    "            logger.warning(\"qcut fail f2, use cut.\"); \n",
    "            try: \n",
    "                data['feature_2_bin'] = pd.cut(data['feature_2'], bins=5, labels=[0, 1, 2, 3, 4], include_lowest=True, duplicates='drop').astype(int) \n",
    "            except Exception as e_cut: \n",
    "                logger.error(f\"Cut fail f2: {e_cut}. Bin 0.\"); \n",
    "                data['feature_2_bin'] = 0\n",
    "        data['feature_2_bin'] = data['feature_2_bin'].fillna(int(data['feature_2_bin'].median())) # Fill potential NaNs from cut\n",
    "        num_transform_features.append('feature_2_bin')\n",
    "        logger.info(\"Added transforms f2 (sq, sqrt, bin).\")\n",
    "    engineered_feature_names.extend(num_transform_features)\n",
    "\n",
    "    bool_agg_features = []; actual_boolean_cols = [col for col in boolean_features if col in data.columns]\n",
    "    if actual_boolean_cols: data['boolean_sum'] = data[actual_boolean_cols].sum(axis=1); data['boolean_sum_squared'] = data['boolean_sum'] ** 2; bool_agg_features.extend(['boolean_sum', 'boolean_sum_squared']); logger.info(\"Added bool aggs.\")\n",
    "    else: data['boolean_sum'] = 0; data['boolean_sum_squared'] = 0; logger.info(\"No bool features for agg.\")\n",
    "    engineered_feature_names.extend(bool_agg_features)\n",
    "\n",
    "    if 'feature_10' in data.columns and 'feature_8' in data.columns: interaction_name = 'feature_10_8_interaction'; data[interaction_name] = data['feature_10'] * data['feature_8']; engineered_feature_names.append(interaction_name); logger.info(f\"Added: {interaction_name}\")\n",
    "\n",
    "    # --- NEW Interactions based on Analysis ---\n",
    "    new_interactions = []\n",
    "    if 'feature_2' in data.columns:\n",
    "        if target_encoded_title in data.columns:\n",
    "             int_name = f'feat2_{target_encoded_title}'; data[int_name] = data['feature_2'] * data[target_encoded_title]; new_interactions.append(int_name)\n",
    "        if 'boolean_sum' in data.columns:\n",
    "             int_name = 'feat2_boolsum'; data[int_name] = data['feature_2'] * data['boolean_sum']; new_interactions.append(int_name)\n",
    "        if 'job_recency' in data.columns:\n",
    "             int_name = 'feat2_recency'; data[int_name] = data['feature_2'] * data['job_recency']; new_interactions.append(int_name)\n",
    "        # Interaction with top PCA component (assuming pca_0 exists)\n",
    "        if 'job_desc_pca_0' in data.columns:\n",
    "            int_name = 'feat2_pca0'; data[int_name] = data['feature_2'] * data['job_desc_pca_0']; new_interactions.append(int_name)\n",
    "    if target_encoded_title in data.columns and 'job_recency' in data.columns:\n",
    "        int_name = f'{target_encoded_title}_recency'; data[int_name] = data[target_encoded_title] * data['job_recency']; new_interactions.append(int_name)\n",
    "    if new_interactions: logger.info(f\"Added new interactions: {new_interactions}\"); engineered_feature_names.extend(new_interactions)\n",
    "    # --- End New Interactions ---\n",
    "\n",
    "    job_desc_eng_features = []\n",
    "    if job_desc_cols:\n",
    "        desc_agg = ['job_desc_mean', 'job_desc_std', 'job_desc_min', 'job_desc_max', 'job_desc_sum', 'job_desc_q25', 'job_desc_q75', 'job_desc_iqr']\n",
    "        data['job_desc_mean'] = data[job_desc_cols].mean(axis=1); data['job_desc_std'] = data[job_desc_cols].std(axis=1).fillna(0); data['job_desc_min'] = data[job_desc_cols].min(axis=1); data['job_desc_max'] = data[job_desc_cols].max(axis=1); data['job_desc_sum'] = data[job_desc_cols].sum(axis=1); data['job_desc_q25'] = data[job_desc_cols].quantile(0.25, axis=1); data['job_desc_q75'] = data[job_desc_cols].quantile(0.75, axis=1); data['job_desc_iqr'] = data['job_desc_q75'] - data['job_desc_q25']; job_desc_eng_features.extend(desc_agg)\n",
    "        n_pca_components = 15 # Keep default or adjust based on experiments\n",
    "        if len(job_desc_cols) > n_pca_components:\n",
    "            logger.info(f\"Applying PCA (n={n_pca_components}) to job desc...\")\n",
    "            pca_names = [f'job_desc_pca_{i}' for i in range(n_pca_components)]; job_desc_eng_features.extend(pca_names); job_desc_pca_result = None\n",
    "            if is_training: pca = PCA(n_components=n_pca_components, random_state=42); job_desc_pca_result = pca.fit_transform(data[job_desc_cols]); joblib.dump(pca, f'features/job_desc_pca_{timestamp}.joblib'); logger.info(\"Fit/saved PCA.\")\n",
    "            else:\n",
    "                pca_path = f'features/job_desc_pca_{timestamp}.joblib'; fallback_files = sorted([f for f in os.listdir('features') if f.startswith('job_desc_pca_')]); pca_loaded = False; pca = None\n",
    "                if os.path.exists(pca_path): \n",
    "                    try: \n",
    "                        pca = joblib.load(pca_path)\n",
    "                        pca_loaded=True; logger.info(f\"Loaded PCA: {pca_path}\") \n",
    "                    except Exception as e: \n",
    "                        logger.error(f\"Fail load PCA: {e}. Try fallback.\")\n",
    "                if not pca_loaded and fallback_files:\n",
    "                    latest_pca = fallback_files[-1]\n",
    "                    try: \n",
    "                        pca = joblib.load(f'features/{latest_pca}')\n",
    "                        pca_loaded = True\n",
    "                    except Exception as e_fb:\n",
    "                        logger.error(f\"Fallback PCA load failed: {e_fb}.\")\n",
    "                else:\n",
    "                    latest_pca = None\n",
    "                if pca_loaded and pca is not None: \n",
    "                    try: \n",
    "                        job_desc_pca_result = pca.transform(data[job_desc_cols]) \n",
    "                    except Exception as e_trans: \n",
    "                        logger.error(f\"PCA transform fail: {e_trans}. Fill 0.\")\n",
    "                if job_desc_pca_result is None: logger.error(\"PCA result None. Fill 0.\")\n",
    "                job_desc_pca_result = np.zeros((data.shape[0], n_pca_components))\n",
    "            for i in range(min(n_pca_components, job_desc_pca_result.shape[1])): data[pca_names[i]] = job_desc_pca_result[:, i]\n",
    "        else: logger.warning(f\"Skip PCA: Not enough features.\")\n",
    "        data = data.drop(columns=job_desc_cols, errors='ignore'); logger.info(\"Finished job desc features.\")\n",
    "    else: logger.info(\"No job desc features.\")\n",
    "    engineered_feature_names.extend(job_desc_eng_features)\n",
    "\n",
    "    # --- Robust Categorical Handling ---\n",
    "    if 'job_state' in data.columns: data['job_state'] = data['job_state'].fillna('Unknown')\n",
    "    if 'feature_1' in data.columns: data['feature_1'] = data['feature_1'].fillna('Unknown')\n",
    "\n",
    "    manual_ohe_features = []\n",
    "    logger.info(f\"Applying manual OHE for 'job_state' ({len(all_states)} unique).\")\n",
    "    if 'job_state' in data.columns:\n",
    "        for state in all_states: col_name = f'state_{state}'; data[col_name] = (data['job_state'] == state).astype(int); manual_ohe_features.append(col_name)\n",
    "        data = data.drop('job_state', axis=1, errors='ignore')\n",
    "    else: logger.warning(\"'job_state' not found for OHE.\")\n",
    "\n",
    "    logger.info(f\"Applying manual OHE for 'feature_1' ({len(all_feature1)} unique).\")\n",
    "    if 'feature_1' in data.columns:\n",
    "        for feat in all_feature1: col_name = f'feat1_{feat}'; data[col_name] = (data['feature_1'] == feat).astype(int); manual_ohe_features.append(col_name)\n",
    "        data = data.drop('feature_1', axis=1, errors='ignore')\n",
    "    else: logger.warning(\"'feature_1' not found for OHE.\")\n",
    "    engineered_feature_names.extend(manual_ohe_features)\n",
    "    # --- End FE ---\n",
    "\n",
    "    # 5. Final Cleanup and Column Management\n",
    "    logger.info(\"Final cleanup and column alignment...\")\n",
    "    columns_to_exclude = ['obs']\n",
    "    if is_training and target_column in df.columns: columns_to_exclude.append(target_column)\n",
    "    potential_feature_cols = [col for col in data.columns if col not in columns_to_exclude]\n",
    "\n",
    "    inf_cols_handled = []; nan_cols_handled = []\n",
    "    for col in potential_feature_cols:\n",
    "        if pd.api.types.is_numeric_dtype(data[col]):\n",
    "            if np.isinf(data[col]).any(): inf_cols_handled.append(col); data[col] = data[col].replace([np.inf, -np.inf], np.nan)\n",
    "            if data[col].isnull().any(): nan_cols_handled.append(col); data[col] = data[col].fillna(0)\n",
    "    if inf_cols_handled: logger.warning(f\"Replaced Inf: {inf_cols_handled}\")\n",
    "    final_nan_cols = list(set(nan_cols_handled) - set(inf_cols_handled))\n",
    "    if final_nan_cols: logger.info(f\"Filled NaNs with 0: {final_nan_cols}\")\n",
    "    for col in potential_feature_cols:\n",
    "        if data[col].dtype == 'bool': data[col] = data[col].astype(int)\n",
    "\n",
    "    if is_training:\n",
    "        constant_cols_found = []\n",
    "        for col in potential_feature_cols:\n",
    "            nunique_val = data[col].nunique(dropna=False)\n",
    "            if nunique_val <= 1: logger.warning(f\"Col '{col}' constant (nunique={nunique_val}) in train. Engineered: {col in engineered_feature_names}. Keeping col.\") ; constant_cols_found.append(col)\n",
    "            elif nunique_val <= 3 and col in engineered_feature_names: logger.info(f\"Eng col '{col}' low card (nunique={nunique_val}) in train.\")\n",
    "\n",
    "        final_feature_columns = potential_feature_cols\n",
    "        joblib.dump(final_feature_columns, f'features/feature_columns_{timestamp}.joblib')\n",
    "        logger.info(f\"Saved {len(final_feature_columns)} feature names (const cols NOT dropped).\")\n",
    "        X = data[final_feature_columns]\n",
    "        logger.info(f\"Preprocessing train done. Shape: {X.shape}. Time: {time.time() - start_time:.2f}s\")\n",
    "        try: X.head().to_csv(f'features/processed_features_head_{timestamp}.csv', index=False)\n",
    "        except Exception as e: logger.warning(f\"Could not save head: {e}\")\n",
    "        return X, y, final_feature_columns, le\n",
    "    else: # Test Data\n",
    "        if feature_columns_to_use is None:\n",
    "            try:\n",
    "                col_files = sorted([f for f in os.listdir('features') if f.startswith('feature_columns_')])\n",
    "                if col_files:\n",
    "                    latest_col = col_files[-1]\n",
    "                    feature_columns_to_use = joblib.load(f'features/{latest_col}')\n",
    "                    logger.info(f\"Loaded {len(feature_columns_to_use)} cols from: {latest_col}\")\n",
    "                else:\n",
    "                    logger.error(\"CRITICAL: No feature_columns file.\")\n",
    "                    raise FileNotFoundError(\"feature_columns_*.joblib missing.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load feature columns: {e}.\")\n",
    "                raise\n",
    "\n",
    "        X = pd.DataFrame(columns=feature_columns_to_use); missing_cols = []; processed_cols = list(data.columns); extra_cols = list(set(processed_cols) - set(feature_columns_to_use) - set(columns_to_exclude))\n",
    "        for col in feature_columns_to_use:\n",
    "            if col in data.columns: X[col] = data[col]\n",
    "            else: X[col] = 0; missing_cols.append(col)\n",
    "        if missing_cols: logger.warning(f\"Cols missing in test (filled 0): {missing_cols}\")\n",
    "        if extra_cols: logger.warning(f\"Cols extra in test (dropped align): {extra_cols}\")\n",
    "        X = X[feature_columns_to_use]; logger.info(f\"Preprocessing test done. Shape: {X.shape}. Time: {time.time() - start_time:.2f}s\")\n",
    "        return X, y, feature_columns_to_use, le # y is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a0de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight  # Added compute_sample_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "# Imports for models used within the function\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              ExtraTreesClassifier, AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import joblib  # For saving models\n",
    "from sklearn.calibration import CalibratedClassifierCV  # For calibration step\n",
    "\n",
    "# Assume build_keras_model function is defined elsewhere\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "# Assume save_feature_importance function is defined\n",
    "\n",
    "def optimize_model(X, y, timestamp, model_type, n_trials=30, n_jobs_optuna=1):\n",
    "    \"\"\"\n",
    "    Optimizes hyperparameters for a given model type using Optuna,\n",
    "    then trains and saves the final model with best parameters.\n",
    "    Includes class_weight='balanced' or equivalent strategies.\n",
    "    Correctly handles Optuna-specific trial parameters during final instantiation.\n",
    "    Attempts calibration after successful model saving.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting {model_type} optimization ({n_trials} trials)...\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    if not isinstance(y, (np.ndarray, pd.Series)):\n",
    "        y = np.array(y)\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)  # Ensure DataFrame for consistent .iloc\n",
    "\n",
    "    n_classes = len(np.unique(y))\n",
    "    n_features = X.shape[1]\n",
    "    y_keras = to_categorical(y, num_classes=n_classes) if model_type == 'keras_mlp' else y\n",
    "\n",
    "    KERAS_EPOCHS = 150  # Reduced Keras epochs slightly\n",
    "    KERAS_PATIENCE = 25  # Increased Keras patience slightly\n",
    "    OPTUNA_TIMEOUT_PER_MODEL = 3600  # Default 1 hour\n",
    "    # Increase timeout for complex models\n",
    "    if model_type in ['xgboost', 'catboost', 'randomforest', 'gradientboosting', 'keras_mlp', 'mlp', 'extratrees', 'lightgbm']:\n",
    "        OPTUNA_TIMEOUT_PER_MODEL = 7200  # Increase to 2 hours\n",
    "    logger.info(f\"Optuna timeout for {model_type}: {OPTUNA_TIMEOUT_PER_MODEL}s.\")\n",
    "\n",
    "    # --- Optuna Objective Function ---\n",
    "    def objective(trial):\n",
    "        model = None\n",
    "        fit_params = {}\n",
    "        use_gpu = False\n",
    "        is_keras = False\n",
    "\n",
    "        # --- Model Definitions for Optuna Trial (Includes custom weights/params) ---\n",
    "        if model_type == 'xgboost':\n",
    "            tree_method = trial.suggest_categorical('tree_method', ['hist', 'gpu_hist'])\n",
    "            param = {'objective': 'multi:softprob',  # Use softprob for calibration later\n",
    "                    'num_class': n_classes, 'eval_metric': 'mlogloss',\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 200, 2000, step=100),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 12),  # Slightly reduced max depth range\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),  # Narrower LR range\n",
    "                    'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                    'gamma': trial.suggest_float('gamma', 1e-6, 0.5, log=True),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 1e-6, 10.0, log=True),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 1e-6, 10.0, log=True),\n",
    "                    'random_state': 42, 'n_jobs': 1, 'booster': 'gbtree', 'tree_method': tree_method}\n",
    "            if tree_method == 'gpu_hist':\n",
    "                param['gpu_id'] = 0\n",
    "                use_gpu = True\n",
    "                param.pop('n_jobs', None)\n",
    "            else:\n",
    "                param.pop('gpu_id', None)\n",
    "            # Optuna doesn't directly support sample_weight in suggest, handle balancing via other params or data prep\n",
    "            model = XGBClassifier(**param)\n",
    "            fit_params = {'verbose': False}  # Early stopping handled separately below\n",
    "\n",
    "        elif model_type == 'catboost':\n",
    "            task_type = trial.suggest_categorical('task_type', ['CPU', 'GPU'])\n",
    "            # Define custom class weights to prioritize Medium-High distinction\n",
    "            class_weight_options = [None, 'Balanced', {0: 1.2, 1: 1.0, 2: 1.4}, {0: 1.3, 1: 1.0, 2: 1.5}]\n",
    "            chosen_class_weight = trial.suggest_categorical('class_weight_option', class_weight_options)  # Directly suggest value\n",
    "\n",
    "            param = {'iterations': trial.suggest_int('iterations', 200, 2000, step=100),\n",
    "                    'depth': trial.suggest_int('depth', 4, 12),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "                    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 15, log=True),\n",
    "                    'random_strength': trial.suggest_float('random_strength', 1e-2, 5.0, log=True),\n",
    "                    'border_count': trial.suggest_categorical('border_count', [64, 128, 254]),\n",
    "                    'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 0.8),\n",
    "                    'loss_function': 'MultiClass', 'eval_metric': 'Accuracy',\n",
    "                    'random_seed': 42, 'thread_count': -1, 'verbose': False, 'task_type': task_type}\n",
    "            if isinstance(chosen_class_weight, dict):\n",
    "                param['class_weights'] = chosen_class_weight\n",
    "            elif chosen_class_weight == 'Balanced':\n",
    "                param['auto_class_weights'] = 'Balanced'\n",
    "            if task_type == 'GPU':\n",
    "                param['devices'] = '0'\n",
    "                use_gpu = True\n",
    "            model = CatBoostClassifier(**param)\n",
    "            fit_params = {'early_stopping_rounds': KERAS_PATIENCE, 'verbose': False}\n",
    "\n",
    "        elif model_type == 'keras_mlp':\n",
    "            is_keras = True\n",
    "            optimizer_name = trial.suggest_categorical('optimizer', ['adam'])\n",
    "            lr = trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True)\n",
    "            dropout = trial.suggest_float('dropout_rate', 0.1, 0.6)\n",
    "            activation = trial.suggest_categorical('activation', ['relu', 'swish'])\n",
    "            n_layers = trial.suggest_int('n_layers', 2, 5)\n",
    "            units_list = []\n",
    "            last_units = n_features\n",
    "            for i in range(n_layers):\n",
    "                max_units = max(32, int(last_units * 1.2))\n",
    "                min_units = max(16, int(last_units / 3))\n",
    "                units = trial.suggest_int(f'n_units_l{i}', min_units, max_units, log=True)\n",
    "                units_list.append(units)\n",
    "                last_units = units\n",
    "            l2_reg = trial.suggest_float('l2_reg', 1e-6, 1e-3, log=True)\n",
    "            # Class weights handled in fit_params below, not model constructor\n",
    "            model_params = {'model__n_features': n_features, 'model__n_classes': n_classes, \n",
    "                           'model__optimizer': optimizer_name, 'model__learning_rate': lr, \n",
    "                           'model__hidden_units': units_list, 'model__dropout_rate': dropout, \n",
    "                           'model__activation': activation, 'model__l2_reg': l2_reg, \n",
    "                           'epochs': KERAS_EPOCHS, \n",
    "                           'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]), \n",
    "                           'verbose': 0}\n",
    "            model = KerasClassifier(model=build_keras_model, **model_params)\n",
    "            keras_callbacks = [\n",
    "                EarlyStopping(monitor='val_accuracy', patience=KERAS_PATIENCE, restore_best_weights=True, verbose=0),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=KERAS_PATIENCE // 2, min_lr=1e-6, verbose=0)\n",
    "            ]\n",
    "            # Calculate and add class weights for this trial\n",
    "            class_weights_values = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "            class_weights_dict = dict(enumerate(class_weights_values))\n",
    "            # Optional: Add custom emphasis here if needed based on trial param, e.g.,\n",
    "            # emphasis = trial.suggest_float('medium_emphasis', 1.0, 1.5)\n",
    "            # class_weights_dict[medium_encoded_label] *= emphasis\n",
    "            fit_params = {'callbacks': keras_callbacks, 'validation_split': 0.15, 'class_weight': class_weights_dict}\n",
    "\n",
    "        elif model_type == 'mlp':\n",
    "            layer_choices = [(100,), (100, 50), (128, 64), (256, 128), (128, 64, 32)]\n",
    "            layers = trial.suggest_categorical('hidden_layer_sizes', layer_choices)\n",
    "            param = {'hidden_layer_sizes': layers,\n",
    "                    'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "                    'solver': trial.suggest_categorical('solver', ['adam']),\n",
    "                    'alpha': trial.suggest_float('alpha', 1e-6, 1e-2, log=True),\n",
    "                    'learning_rate': 'adaptive',\n",
    "                    'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True),\n",
    "                    'max_iter': trial.suggest_int('max_iter', 500, 1500),\n",
    "                    'early_stopping': True,\n",
    "                    'n_iter_no_change': KERAS_PATIENCE + 10,\n",
    "                    'validation_fraction': 0.15,\n",
    "                    'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "                    'random_state': 42,\n",
    "                    'warm_start': False}\n",
    "            model = MLPClassifier(**param)\n",
    "            # Sample weights handled during fit below\n",
    "\n",
    "        elif model_type == 'randomforest':\n",
    "            class_weight_choices = ['balanced', 'balanced_subsample', {0: 1.3, 1: 1.0, 2: 1.2}, {0: 1.6, 1: 1.0, 2: 1.3},{0: 1.5, 1: 1.0, 2: 1.5}]  # Use dict directly if supported\n",
    "            class_weight = trial.suggest_categorical('class_weight', class_weight_choices)\n",
    "            param = {'n_estimators': trial.suggest_int('n_estimators', 200, 1500, step=100),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 8, 50, step=2),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 15),\n",
    "                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.7]),\n",
    "                    'bootstrap': True,\n",
    "                    'class_weight': class_weight,\n",
    "                    'random_state': 42,\n",
    "                    'n_jobs': n_jobs_optuna,\n",
    "                    'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])}\n",
    "            model = RandomForestClassifier(**param)\n",
    "\n",
    "        elif model_type == 'extratrees':\n",
    "            class_weight_choices = ['balanced', 'balanced_subsample', {0: 1.2, 1: 1.0, 2: 1.3}, {0: 1.7, 1: 1.0, 2: 1.4},{0: 1.9, 1: 1.0, 2: 1.7}]\n",
    "            class_weight = trial.suggest_categorical('class_weight', class_weight_choices)\n",
    "            param = {'n_estimators': trial.suggest_int('n_estimators', 200, 1500, step=100),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 10, 60, step=2),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 15),\n",
    "                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.7]),\n",
    "                    'bootstrap': trial.suggest_categorical('bootstrap', [False, True]),\n",
    "                    'class_weight': class_weight,\n",
    "                    'random_state': 42,\n",
    "                    'n_jobs': n_jobs_optuna,\n",
    "                    'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])}\n",
    "            model = ExtraTreesClassifier(**param)\n",
    "\n",
    "        elif model_type == 'logistic':\n",
    "            class_weight_choices = ['balanced', {0: 1.2, 1: 1.0, 2: 1.3}, {0: 1.4, 1: 1.0, 2: 1.4},{0: 2, 1: 1.0, 2: 1.7}]\n",
    "            class_weight = trial.suggest_categorical('class_weight', class_weight_choices)\n",
    "            penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "            solver = 'liblinear'  # Needed for L1/L2\n",
    "            param = {'C': trial.suggest_float('C', 1e-3, 1e3, log=True),\n",
    "                    'penalty': penalty,\n",
    "                    'solver': solver,\n",
    "                    'class_weight': class_weight,\n",
    "                    'max_iter': trial.suggest_int('max_iter', 100, 1000),\n",
    "                    'random_state': 42,\n",
    "                    'multi_class': 'ovr'}  # Ovr often works well with liblinear\n",
    "            model = LogisticRegression(**param)\n",
    "\n",
    "        elif model_type == 'gradientboosting':\n",
    "            param = {'n_estimators': trial.suggest_int('n_estimators', 150, 1500, step=100),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 5, 30),\n",
    "                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 3, 25),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                    'random_state': 42,\n",
    "                    'loss': 'log_loss',\n",
    "                    'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.1)}\n",
    "            model = GradientBoostingClassifier(**param)\n",
    "            # Sample weights handled during fit below\n",
    "\n",
    "        elif model_type == 'adaboost':\n",
    "            base_depth = trial.suggest_int('base_estimator_max_depth', 1, 4)\n",
    "            class_weights_options = ['balanced', {0: 1.2, 1: 1.0, 2: 1.3}, {0: 1.7, 1: 1.0, 2: 1.4},{0: 2, 1: 1.0, 2: 1.9}]\n",
    "            weight_choice = trial.suggest_categorical('class_weight_choice', class_weights_options)  # Suggest actual value\n",
    "            param_ada = {'n_estimators': trial.suggest_int('n_estimators', 50, 1000, step=50),\n",
    "                        'learning_rate': trial.suggest_float('learning_rate', 0.05, 1.5, log=True),\n",
    "                        'algorithm': 'SAMME',\n",
    "                        'random_state': 42}\n",
    "            base_est = DecisionTreeClassifier(max_depth=base_depth, random_state=42, class_weight=weight_choice)  # Apply choice\n",
    "            model = AdaBoostClassifier(estimator=base_est, **param_ada)\n",
    "            trial.set_user_attr(\"base_estimator_max_depth\", base_depth)\n",
    "            trial.set_user_attr(\"class_weight_info\", weight_choice if isinstance(weight_choice, dict) else 'balanced' if weight_choice=='balanced' else 'None')  # Log info\n",
    "\n",
    "        elif model_type == 'knn':\n",
    "            metric = trial.suggest_categorical('metric', ['minkowski', 'manhattan'])\n",
    "            param = {'n_neighbors': trial.suggest_int('n_neighbors', 5, 35, step=2),  # Focus on smaller K\n",
    "                    'weights': trial.suggest_categorical('weights', ['distance']),  # Distance usually better\n",
    "                    'metric': metric,\n",
    "                    'n_jobs': n_jobs_optuna,\n",
    "                    'leaf_size': trial.suggest_int('leaf_size', 20, 40),\n",
    "                    'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree'])}\n",
    "            if metric == 'minkowski':\n",
    "                param['p'] = trial.suggest_int('p', 1, 2)\n",
    "            model = KNeighborsClassifier(**param)\n",
    "            # Sample weights handled during fit below\n",
    "\n",
    "        elif model_type == 'lightgbm':\n",
    "            class_weight_options = [None, 'balanced', {0: 1.4, 1: 1.0, 2: 1.3}, {0: 1.6, 1: 1.0, 2: 1.4},{0: 2, 1: 1.0, 2: 1.7}]\n",
    "            class_weight = trial.suggest_categorical('class_weight_option', class_weight_options)\n",
    "            param = {'objective': 'multiclass',\n",
    "                    'num_class': n_classes,\n",
    "                    'metric': 'multi_logloss',\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 200, 1500, step=100),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "                    'num_leaves': trial.suggest_int('num_leaves', 20, 100, step=5),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 5, 14),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 1e-7, 5.0, log=True),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 1e-7, 5.0, log=True),\n",
    "                    'min_child_samples': trial.suggest_int('min_child_samples', 5, 40),\n",
    "                    'class_weight': class_weight,  # Apply selected weight\n",
    "                    'random_state': 42,\n",
    "                    'n_jobs': n_jobs_optuna,\n",
    "                    'verbose': -1,\n",
    "                    'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart'])}  # GBDT and DART\n",
    "            model = lgb.LGBMClassifier(**param)\n",
    "            fit_params = {'callbacks': [lgb.early_stopping(KERAS_PATIENCE, verbose=False)]}\n",
    "\n",
    "        else:\n",
    "            logger.error(f\"Unsupported model type: {model_type}\")\n",
    "            raise ValueError(f\"Unsupported: {model_type}\")\n",
    "\n",
    "        # --- Cross-validation ---\n",
    "        scores = []\n",
    "        is_dataframe = isinstance(X, pd.DataFrame)\n",
    "        try:\n",
    "            for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "                # --- Use .iloc consistently for pandas objects ---\n",
    "                if is_dataframe: # If X is a DataFrame, assume y is a Series\n",
    "                    X_train_fold = X.iloc[train_idx]\n",
    "                    X_valid_fold = X.iloc[valid_idx]\n",
    "                    # Select training labels based on position\n",
    "                    y_train_fold = y_keras[train_idx] if is_keras else y.iloc[train_idx] # ***MODIFIED***\n",
    "                    # Select validation labels based on position\n",
    "                    y_valid_fold_orig = y.iloc[valid_idx] # ***MODIFIED***\n",
    "                else: # If X is a numpy array, assume y is also numpy array\n",
    "                    X_train_fold = X[train_idx]\n",
    "                    X_valid_fold = X[valid_idx]\n",
    "                    y_train_fold = y_keras[train_idx] if is_keras else y[train_idx]\n",
    "                    y_valid_fold_orig = y[valid_idx]\n",
    "                current_fit_params = fit_params.copy()\n",
    "\n",
    "                # --- Handle Sample Weights for Models That Need It in Fit ---\n",
    "                fold_sample_weight = None\n",
    "                if model_type in ['gradientboosting']:  \n",
    "                    # Calculate balanced weights\n",
    "                    sample_weight = compute_sample_weight('balanced', y=y_train_fold)\n",
    "                    # Apply custom emphasis based on strategy (e.g., boost High/Medium)\n",
    "                    emphasis_weights = {0: 1.5, 1: 1.0, 2: 1.5}  # Example emphasis\n",
    "                    for cls_idx, weight_multiplier in emphasis_weights.items():\n",
    "                         # Ensure y_train_fold is numpy for boolean indexing if it was a Series\n",
    "                         y_train_fold_np = y_train_fold.values if isinstance(y_train_fold, pd.Series) else y_train_fold\n",
    "                         sample_weight[y_train_fold_np == cls_idx] *= weight_multiplier\n",
    "                    fold_sample_weight = sample_weight\n",
    "                    current_fit_params['sample_weight'] = fold_sample_weight\n",
    "                    logger.debug(f\"Trial {trial.number} Fold {fold+1}: Applied sample weights for {model_type}\")\n",
    "\n",
    "                try:\n",
    "                    # Pass eval_set for models that use it with callbacks/early stopping\n",
    "                    eval_set = [(X_valid_fold, y_valid_fold_orig)]\n",
    "                    \n",
    "                    # --- XGBoost Specific Fit Call ---\n",
    "                    if model_type == 'xgboost':\n",
    "                         model.fit(X_train_fold, y_train_fold,\n",
    "                                   eval_set=eval_set,\n",
    "                                   early_stopping_rounds=KERAS_PATIENCE, # Pass directly\n",
    "                                   verbose=False) # Pass other relevant args directly if needed\n",
    "                    # --- LightGBM Specific Fit Call (already seemed correct) ---\n",
    "                    elif model_type == 'lightgbm':\n",
    "                         # Note: LGBM uses callbacks for early stopping, passed via fit_params\n",
    "                         current_fit_params['eval_set'] = eval_set\n",
    "                         current_fit_params['eval_metric'] = 'multi_logloss' # Or match objective metric\n",
    "                         model.fit(X_train_fold, y_train_fold, **current_fit_params)\n",
    "                    # --- CatBoost Specific Fit Call (already seemed correct) ---\n",
    "                    elif model_type == 'catboost':\n",
    "                         current_fit_params['eval_set'] = eval_set\n",
    "                         # Early stopping rounds already part of CatBoost init/params\n",
    "                         model.fit(X_train_fold, y_train_fold, **current_fit_params)\n",
    "                    # --- Default Fit Call for other models ---\n",
    "                    else:\n",
    "                         # Pass sample_weight if applicable (e.g., for GB)\n",
    "                         model.fit(X_train_fold, y_train_fold, **current_fit_params)\n",
    "                    \n",
    "                    # Predict and score\n",
    "                    y_pred = model.predict(X_valid_fold)\n",
    "                    if is_keras and y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "                        y_pred = np.argmax(y_pred, axis=1)\n",
    "                    score = accuracy_score(y_valid_fold_orig, y_pred)\n",
    "                    scores.append(score)\n",
    "                    logger.debug(f\"Trial {trial.number} Fold {fold+1} Score: {score:.5f}\")\n",
    "\n",
    "                except ValueError as ve:\n",
    "                    logger.warning(f\"CV fold {fold+1} VAL ERROR {model_type} trial {trial.number}: {ve}\")\n",
    "                    return 0.0\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"CV fold {fold+1} EXCEPTION {model_type} trial {trial.number}: {e}\", exc_info=True)\n",
    "                    scores = []\n",
    "                    break  # Log full traceback\n",
    "        except Exception as outer_e:\n",
    "            logger.error(f\"Outer CV error {model_type} trial {trial.number}: {outer_e}\", exc_info=True)\n",
    "            return 0.0\n",
    "        if not scores:\n",
    "            logger.error(f\"Cross-validation failed completely for {model_type} trial {trial.number}\")\n",
    "            return 0.0\n",
    "        mean_score = np.mean(scores)\n",
    "        logger.debug(f\"Trial {trial.number} ({model_type}) completed. Avg CV Score: {mean_score:.5f}\")\n",
    "        return mean_score\n",
    "\n",
    "    # --- Run Optuna Study ---\n",
    "    study_name = f\"{model_type}_opt_{timestamp}\"\n",
    "    storage_name = f\"sqlite:///optuna_trials/{study_name}.db\"\n",
    "    study = optuna.create_study(direction='maximize', study_name=study_name, storage=storage_name, \n",
    "                              load_if_exists=True, pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "    completed_trials = len([t for t in study.trials if t.state==optuna.trial.TrialState.COMPLETE])\n",
    "    trials_to_run = n_trials-completed_trials\n",
    "    \n",
    "    if trials_to_run > 0:\n",
    "        logger.info(f\"Setting Optuna timeout {OPTUNA_TIMEOUT_PER_MODEL}s.\")\n",
    "        try:\n",
    "            study.optimize(objective, n_trials=trials_to_run, timeout=OPTUNA_TIMEOUT_PER_MODEL, n_jobs=1)\n",
    "        except Exception as opt_e:\n",
    "            logger.error(f\"Optuna optimize fail {model_type}: {opt_e}\", exc_info=True)\n",
    "            return None, -1, {}\n",
    "    else:\n",
    "        logger.info(f\"Study {study_name} has {completed_trials} trials. Skip optimize.\")\n",
    "\n",
    "    # --- Retrieve Results ---\n",
    "    try:\n",
    "        if not any(t.state == optuna.trial.TrialState.COMPLETE for t in study.trials):\n",
    "            logger.error(f\"Optuna study {model_type} no successful trials.\")\n",
    "            return None, -1, {}\n",
    "        best_trial = study.best_trial\n",
    "        best_params = best_trial.params\n",
    "        best_cv_score = best_trial.value\n",
    "    except ValueError:\n",
    "        logger.error(f\"Optuna study {model_type} no best trial.\")\n",
    "        return None, -1, {}\n",
    "    except Exception as res_e:\n",
    "        logger.error(f\"Error get Optuna results {model_type}: {res_e}\", exc_info=True)\n",
    "        return None, -1, {}\n",
    "    logger.info(f\"Opt complete {model_type}. Best CV score: {best_cv_score:.5f}. Best params: {best_params}\")\n",
    "\n",
    "    # --- Save Study Summary ---\n",
    "    try:\n",
    "        summary_file = f'optuna_trials/{model_type}_study_summary_{timestamp}.txt'\n",
    "        params_json = best_params.copy()\n",
    "        if model_type=='adaboost' and \"base_estimator_max_depth\" in best_trial.user_attrs:\n",
    "            params_json['base_estimator_max_depth'] = best_trial.user_attrs[\"base_estimator_max_depth\"]\n",
    "            params_json['class_weight_info'] = best_trial.user_attrs.get(\"class_weight_info\", \"N/A\")\n",
    "        if model_type=='xgboost' and 'tree_method' in best_params:\n",
    "            params_json['tree_method'] = best_params['tree_method']\n",
    "        if model_type=='catboost' and 'task_type' in best_params:\n",
    "            params_json['task_type'] = best_params['task_type']\n",
    "        if model_type=='keras_mlp' or model_type=='mlp':\n",
    "            if 'hidden_layer_sizes' in params_json:\n",
    "                params_json['hidden_layer_sizes'] = str(params_json['hidden_layer_sizes'])\n",
    "            if 'n_layers' in best_params:\n",
    "                units_list = [best_params.get(f'n_units_l{i}') for i in range(best_params['n_layers']) \n",
    "                             if best_params.get(f'n_units_l{i}')]\n",
    "                params_json['hidden_units_structure'] = str(units_list)\n",
    "            params_json = {k: v for k, v in params_json.items() if not k.startswith('n_units_l')}\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(f\"Optuna Summary: {model_type}\\nTS: {timestamp}\\nBest Trial: {best_trial.number}\\nScore: {best_cv_score:.5f}\\n\\nParams:\\n\")\n",
    "            json.dump(params_json, f, indent=4)\n",
    "        logger.info(f\"Saved Optuna summary: {summary_file}\")\n",
    "    except Exception as file_e:\n",
    "        logger.warning(f\"Could not save Optuna summary {model_type}: {file_e}\")\n",
    "\n",
    "    # --- Train final model ---\n",
    "    final_model = None\n",
    "    final_fit_params = {}  # Reset for final fit\n",
    "    try:\n",
    "        logger.info(f\"Instantiating final {model_type} model...\")\n",
    "        # Clean best_params from Optuna-specific args before final instantiation\n",
    "        params_for_final = best_params.copy()\n",
    "        optuna_internal_params = ['class_weight_option', 'class_weight_choice', 'class_weight_idx', \n",
    "                                 'class_weight_strategy', 'use_smote', 'smote_k', \n",
    "                                 'use_focal_loss', 'focal_gamma']  # Params used only in objective logic\n",
    "        for p in optuna_internal_params:\n",
    "            params_for_final.pop(p, None)\n",
    "\n",
    "        # Inside optimize_model, after Optuna, in elif model_type == 'adaboost':\n",
    "        if model_type == 'adaboost':\n",
    "            # Clean best_params from Optuna-specific args before final instantiation\n",
    "            params_for_final = best_params.copy()\n",
    "            # List internal params used only during Optuna trials\n",
    "            optuna_internal_params = ['class_weight_choice', 'base_estimator_max_depth']\n",
    "            for p in optuna_internal_params:\n",
    "                # --- THESE LINES REMOVE THE BAD PARAMETERS ---\n",
    "                params_for_final.pop(p, None)\n",
    "                # --- END OF REMOVAL ---\n",
    "\n",
    "            # Retrieve correct values from Optuna trial attributes\n",
    "            best_d = best_trial.user_attrs.get('base_estimator_max_depth', 1)\n",
    "            weight_info_raw = best_trial.user_attrs.get(\"class_weight_info\", 'balanced')\n",
    "            \n",
    "            # --- *** ADDED: Convert dictionary keys if needed *** ---\n",
    "            weight_info_processed = weight_info_raw\n",
    "            if isinstance(weight_info_raw, dict):\n",
    "                try:\n",
    "                    # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                    weight_info_processed = {int(k): v for k, v in weight_info_raw.items()}\n",
    "                    logger.info(f\"Converted AdaBoost class_weight keys to int: {weight_info_processed}\")\n",
    "                except ValueError as e:\n",
    "                     logger.error(f\"Error converting AdaBoost class_weight keys: {e}. Using raw: {weight_info_raw}\")\n",
    "                     weight_info_processed = weight_info_raw # Fallback to raw if conversion fails\n",
    "            # --- *** END KEY CONVERSION *** ---\n",
    "            \n",
    "            logger.info(f\"Reconstruct AdaBoost DT(max_depth={best_d}, class_weight={weight_info_processed}) using SAMME\")\n",
    "            # Create the base estimator correctly\n",
    "            base_est_inst = DecisionTreeClassifier(max_depth=best_d, random_state=42, class_weight=weight_info_processed)\n",
    "\n",
    "            final_p_ada = params_for_final # Use the cleaned dictionary for AdaBoost itself\n",
    "            final_p_ada['algorithm'] = 'SAMME'\n",
    "            # --- FINAL MODEL TRAINING WILL STILL HAPPEN USING base_est_inst and final_p_ada ---\n",
    "            final_model = AdaBoostClassifier(estimator=base_est_inst, **final_p_ada)\n",
    "\n",
    "        elif model_type == 'xgboost':\n",
    "            final_params_xgb = params_for_final.copy()\n",
    "            final_params_xgb['objective'] = 'multi:softprob'\n",
    "            final_params_xgb['num_class'] = n_classes\n",
    "            final_params_xgb['n_jobs'] = 1\n",
    "            logger.info(\"XGBoost final model - balancing via sample_weight in fit.\")\n",
    "            final_model = XGBClassifier(**final_params_xgb)\n",
    "            # Prepare sample weights for fit step\n",
    "            sample_weights_xgb = compute_sample_weight('balanced', y=y)  # Start with balanced\n",
    "            emphasis_weights = {0: 2, 1: 1.0, 2: 2}  # Emphasize High/Medium\n",
    "            for cls_idx, weight_multiplier in emphasis_weights.items():\n",
    "                sample_weights_xgb[y == cls_idx] *= weight_multiplier\n",
    "            final_fit_params['sample_weight'] = sample_weights_xgb\n",
    "\n",
    "        elif model_type == 'catboost':\n",
    "            final_params_cat = params_for_final.copy()\n",
    "            final_params_cat['loss_function'] = 'MultiClass'\n",
    "            final_params_cat['verbose'] = False\n",
    "            # Re-apply class weight strategy based on best trial's choice\n",
    "            chosen_weight = best_params.get('class_weight_option')\n",
    "            if isinstance(chosen_weight, dict):\n",
    "                final_params_cat['class_weights'] = chosen_weight\n",
    "                logger.info(f\"CatBoost using custom weights: {chosen_weight}\")\n",
    "            elif chosen_weight == 'Balanced':\n",
    "                final_params_cat['auto_class_weights'] = 'Balanced'\n",
    "                logger.info(\"CatBoost using auto_class_weights=Balanced\")\n",
    "            else:\n",
    "                logger.info(\"CatBoost using default balancing or no weights.\")\n",
    "            final_model = CatBoostClassifier(**final_params_cat)\n",
    "\n",
    "        elif model_type == 'mlp':\n",
    "            final_params_mlp = params_for_final.copy()\n",
    "            logger.info(\"MLP final model - using sample_weight in fit step\")\n",
    "            final_model = MLPClassifier(**final_params_mlp)\n",
    "            # sample_weights_mlp = compute_sample_weight('balanced', y=y)\n",
    "            # final_fit_params['sample_weight'] = sample_weights_mlp\n",
    "            final_fit_params = {}\n",
    "\n",
    "        elif model_type == 'keras_mlp':\n",
    "            keras_build_params = {\n",
    "                'n_features': n_features,\n",
    "                'n_classes': n_classes,\n",
    "                'optimizer': best_params.get('optimizer', 'adam'),\n",
    "                'learning_rate': best_params.get('learning_rate', 0.001),\n",
    "                'dropout_rate': best_params.get('dropout_rate', 0.3),\n",
    "                'activation': best_params.get('activation', 'relu'),\n",
    "                'l2_reg': best_params.get('l2_reg', 1e-4)\n",
    "            }\n",
    "            if 'n_layers' in best_params:\n",
    "                hidden_units = [best_params.get(f'n_units_l{i}') for i in range(best_params['n_layers']) \n",
    "                              if best_params.get(f'n_units_l{i}') is not None]\n",
    "                keras_build_params['hidden_units'] = hidden_units if hidden_units else [64]\n",
    "            else:\n",
    "                hidden_units_final = best_params.get('hidden_layer_sizes', [128, 64])\n",
    "                if isinstance(hidden_units_final, str):\n",
    "                    try:\n",
    "                        hidden_units_final = eval(hidden_units_final)\n",
    "                    except:\n",
    "                        hidden_units_final = [128, 64]\n",
    "                keras_build_params['hidden_units'] = list(hidden_units_final)\n",
    "            final_model = KerasClassifier(\n",
    "                model=build_keras_model,\n",
    "                **keras_build_params,\n",
    "                epochs=KERAS_EPOCHS,\n",
    "                batch_size=best_params.get('batch_size', 128),\n",
    "                verbose=0\n",
    "            )\n",
    "            final_callbacks = [\n",
    "                EarlyStopping(monitor='val_accuracy', patience=KERAS_PATIENCE, restore_best_weights=True, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=KERAS_PATIENCE // 2, min_lr=1e-7, verbose=1)\n",
    "            ]\n",
    "            # Get class weights based on best trial's suggestion or default\n",
    "            class_weights_values = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "            class_weights_dict = dict(enumerate(class_weights_values))\n",
    "            emphasis_weights = {0: 1.6, 1: 1.0, 2: 1.5}  # Example emphasis\n",
    "            for cls_idx, mult in emphasis_weights.items():\n",
    "                class_weights_dict[cls_idx] *= mult\n",
    "            logger.info(f\"Keras final fit using class_weights: {class_weights_dict}\")\n",
    "            final_fit_params = {'callbacks': final_callbacks, 'validation_split': 0.15, 'class_weight': class_weights_dict}\n",
    "\n",
    "        elif model_type == 'randomforest':\n",
    "            final_params_rf = params_for_final.copy()\n",
    "            final_params_rf['n_jobs'] = n_jobs_optuna\n",
    "            \n",
    "            # --- *** ADDED: Process class_weight dictionary keys *** ---\n",
    "            class_weight_raw = best_params.get('class_weight', 'balanced')\n",
    "            class_weight_processed = class_weight_raw\n",
    "            if isinstance(class_weight_raw, dict):\n",
    "                 try:\n",
    "                     # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                     class_weight_processed = {int(k): v for k, v in class_weight_raw.items()}\n",
    "                     logger.info(f\"Converted RF class_weight keys to int: {class_weight_processed}\")\n",
    "                 except ValueError as e:\n",
    "                      logger.error(f\"Error converting RF class_weight keys: {e}. Using raw: {class_weight_raw}\")\n",
    "                      class_weight_processed = class_weight_raw # Fallback\n",
    "            # --- *** END KEY CONVERSION *** ---\n",
    "            \n",
    "            final_params_rf['class_weight'] = best_params.get('class_weight', 'balanced')  # Use optimized or default balanced\n",
    "            logger.info(f\"RF final model using class_weight={final_params_rf['class_weight']}\")\n",
    "            final_model = RandomForestClassifier(**final_params_rf)\n",
    "\n",
    "        elif model_type == 'extratrees':\n",
    "            final_params_et = params_for_final.copy()\n",
    "            final_params_et['n_jobs'] = n_jobs_optuna\n",
    "            \n",
    "            # --- *** ADDED: Process class_weight dictionary keys *** ---\n",
    "            class_weight_raw = best_params.get('class_weight', 'balanced')\n",
    "            class_weight_processed = class_weight_raw\n",
    "            if isinstance(class_weight_raw, dict):\n",
    "                 try:\n",
    "                     # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                     class_weight_processed = {int(k): v for k, v in class_weight_raw.items()}\n",
    "                     logger.info(f\"Converted ET class_weight keys to int: {class_weight_processed}\")\n",
    "                 except ValueError as e:\n",
    "                      logger.error(f\"Error converting ET class_weight keys: {e}. Using raw: {class_weight_raw}\")\n",
    "                      class_weight_processed = class_weight_raw # Fallback\n",
    "            # --- *** END KEY CONVERSION *** ---\n",
    "            \n",
    "            final_params_et['class_weight'] = best_params.get('class_weight', 'balanced')  # Use optimized or default balanced\n",
    "            logger.info(f\"ET final model using class_weight={final_params_et['class_weight']}\")\n",
    "            final_model = ExtraTreesClassifier(**final_params_et)\n",
    "\n",
    "        elif model_type == 'logistic':\n",
    "            final_p_log = params_for_final.copy()\n",
    "            final_p_log['solver'] = 'liblinear'\n",
    "            \n",
    "            # --- *** ADDED: Process class_weight dictionary keys *** ---\n",
    "            class_weight_raw = best_params.get('class_weight', 'balanced')\n",
    "            class_weight_processed = class_weight_raw\n",
    "            if isinstance(class_weight_raw, dict):\n",
    "                 try:\n",
    "                     # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                     class_weight_processed = {int(k): v for k, v in class_weight_raw.items()}\n",
    "                     logger.info(f\"Converted LogReg class_weight keys to int: {class_weight_processed}\")\n",
    "                 except ValueError as e:\n",
    "                      logger.error(f\"Error converting LogReg class_weight keys: {e}. Using raw: {class_weight_raw}\")\n",
    "                      class_weight_processed = class_weight_raw # Fallback\n",
    "            # --- *** END KEY CONVERSION *** ---\n",
    "            \n",
    "            final_p_log['class_weight'] = best_params.get('class_weight', 'balanced')  # Use optimized or default balanced\n",
    "            logger.info(f\"LogReg final model using class_weight={final_p_log['class_weight']}\")\n",
    "            final_model = LogisticRegression(**final_p_log)\n",
    "\n",
    "        elif model_type == 'gradientboosting':\n",
    "            final_params_gb = params_for_final.copy()\n",
    "            logger.info(\"GradientBoosting final model - applying sample_weight in fit\")\n",
    "            final_model = GradientBoostingClassifier(**final_params_gb)\n",
    "            sample_weights_gb = compute_sample_weight('balanced', y=y)\n",
    "            emphasis_weights = {0: 1.6, 1: 1.0, 2: 1.5}\n",
    "            for cls_idx, mult in emphasis_weights.items():\n",
    "                sample_weights_gb[y == cls_idx] *= mult\n",
    "            final_fit_params['sample_weight'] = sample_weights_gb\n",
    "\n",
    "        # Inside optimize_model, after Optuna, in elif model_type == 'knn':\n",
    "        elif model_type == 'knn':\n",
    "            # Clean best_params from Optuna-specific args before final instantiation\n",
    "            params_for_final = best_params.copy()\n",
    "            optuna_internal_params = ['use_smote', 'smote_k'] # List of params ONLY for Optuna logic\n",
    "            for p in optuna_internal_params:\n",
    "                # --- THIS LINE REMOVES THE BAD PARAMETER ---\n",
    "                params_for_final.pop(p, None)\n",
    "                # --- END OF REMOVAL ---\n",
    "\n",
    "            final_params_knn = params_for_final # Use the cleaned dictionary\n",
    "            final_params_knn['n_jobs'] = n_jobs_optuna\n",
    "            logger.info(\"KNN final model - balancing requires SMOTE or different approach (not applied in final fit here).\")\n",
    "            # --- FINAL MODEL TRAINING WILL STILL HAPPEN USING final_params_knn ---\n",
    "            final_model = KNeighborsClassifier(**final_params_knn)\n",
    "\n",
    "        elif model_type == 'lightgbm':\n",
    "            final_params_lgbm = params_for_final.copy()\n",
    "            final_params_lgbm['objective'] = 'multiclass'\n",
    "            final_params_lgbm['num_class'] = n_classes\n",
    "            final_params_lgbm['n_jobs'] = n_jobs_optuna\n",
    "            \n",
    "            # --- *** ADDED: Refined Key Conversion for LGBM *** ---\n",
    "            class_weight_value_to_use = 'balanced' # Default\n",
    "            # Use 'class_weight_option' key from Optuna params for LGBM\n",
    "            if 'class_weight_option' in best_params:\n",
    "                class_weight_raw = best_params['class_weight_option'] # Get raw value from Optuna result\n",
    "                class_weight_processed = class_weight_raw\n",
    "\n",
    "                if isinstance(class_weight_raw, dict):\n",
    "                    logger.info(f\"Raw class_weight dict found for LGBM: {class_weight_raw}\")\n",
    "                    try:\n",
    "                        if all(isinstance(k, int) for k in class_weight_raw.keys()):\n",
    "                            logger.info(\"LGBM class_weight keys appear to be integers already.\")\n",
    "                            class_weight_processed = class_weight_raw\n",
    "                        else:\n",
    "                            logger.info(\"Attempting conversion of LGBM class_weight keys to int...\")\n",
    "                            class_weight_processed = {int(k): v for k, v in class_weight_raw.items()}\n",
    "                            logger.info(f\"Successfully converted LGBM class_weight keys to int: {class_weight_processed}\")\n",
    "                    except Exception as e_gen:\n",
    "                         logger.error(f\"Error processing LGBM class_weight dict: {e_gen}. Using 'balanced'.\")\n",
    "                         class_weight_processed = 'balanced'\n",
    "                class_weight_value_to_use = class_weight_processed\n",
    "            else:\n",
    "                 logger.info(\"No 'class_weight_option' found in best_params for LGBM, using default 'balanced'.\")\n",
    "                 class_weight_value_to_use = 'balanced'\n",
    "            \n",
    "            final_params_lgbm['class_weight'] = class_weight_value_to_use\n",
    "            logger.info(f\"LGBM final model using class_weight={final_params_lgbm['class_weight']}\")\n",
    "            final_model = lgb.LGBMClassifier(**final_params_lgbm)\n",
    "\n",
    "        # --- Fit the final model ---\n",
    "        if final_model is not None:\n",
    "            logger.info(f\"Fitting final {model_type} model...\")\n",
    "            start_fit_time = time.time()\n",
    "            model_fitted_successfully = False\n",
    "            try:\n",
    "                # Fit using specific params if they exist (like sample_weight)\n",
    "                if final_fit_params:\n",
    "                    logger.info(f\"Fitting {model_type} with additional fit parameters: {list(final_fit_params.keys())}\")\n",
    "                    if model_type == 'keras_mlp':\n",
    "                        final_model.fit(X, y_keras, **final_fit_params)  # Keras needs one-hot y\n",
    "                    else:\n",
    "                        final_model.fit(X, y, **final_fit_params)  # Pass original y and weights dict\n",
    "                else:\n",
    "                    final_model.fit(X, y)  # Fit standard models\n",
    "\n",
    "                fit_duration = time.time() - start_fit_time\n",
    "                logger.info(f\"Final {model_type} fitted in {fit_duration:.2f}s.\")\n",
    "                model_fitted_successfully = True\n",
    "\n",
    "            except Exception as fit_e:\n",
    "                logger.error(f\"Error during final fit for {model_type}: {fit_e}\", exc_info=True)\n",
    "                # Keep going to return score/params, but model will be None\n",
    "\n",
    "            # --- Save model and importance only if fit succeeded ---\n",
    "            if model_fitted_successfully:\n",
    "                model_path = f'models/{model_type}_{timestamp}.joblib'\n",
    "                logger.info(f\"Saving final {model_type} model...\")\n",
    "                try:\n",
    "                    if isinstance(final_model, KerasClassifier):\n",
    "                        tf_model_save_path = f'models/{model_type}_tfmodel_{timestamp}'\n",
    "                        try:\n",
    "                            final_model.model_.save(tf_model_save_path)\n",
    "                            logger.info(f\"Saved Keras TF model: {tf_model_save_path}\")\n",
    "                        except Exception as k_save_err:\n",
    "                            logger.warning(f\"Keras TF save fail ({k_save_err}), try joblib...\")\n",
    "                            joblib.dump(final_model, model_path)\n",
    "                            logger.info(f\"Saved Keras wrapper: {model_path}\")\n",
    "                    else:\n",
    "                        joblib.dump(final_model, model_path)\n",
    "                        logger.info(f\"Saved final {model_type} via joblib: {model_path}\")\n",
    "                except Exception as save_err:\n",
    "                    logger.error(f\"Failed save model {model_type}: {save_err}\", exc_info=True)\n",
    "\n",
    "                # --- Attempt Calibration AFTER saving base model ---\n",
    "                if model_type not in ['knn', 'mlp']:  # Models less suitable or needing sample_weight for calibration fit\n",
    "                    try:\n",
    "                        logger.info(f\"Attempting calibration for {model_type}...\")\n",
    "                        # Use 'estimator' argument, not 'base_estimator'\n",
    "                        calibrated_model = CalibratedClassifierCV(\n",
    "                            estimator=final_model,\n",
    "                            cv=3,\n",
    "                            method='isotonic',\n",
    "                            n_jobs=n_jobs_optuna,\n",
    "                            ensemble=False\n",
    "                        )\n",
    "                        calibrated_model.fit(X, y)  # Calibrate on the full training data\n",
    "                        calibrated_path = f'calibrated_models/{model_type}_calibrated_{timestamp}.joblib'\n",
    "                        if not os.path.exists('calibrated_models'):\n",
    "                            os.makedirs('calibrated_models')\n",
    "                        joblib.dump(calibrated_model, calibrated_path)\n",
    "                        logger.info(f\"Saved calibrated model: {calibrated_path}\")\n",
    "                    except Exception as cal_err:\n",
    "                        logger.warning(f\"Calibration failed for {model_type}: {cal_err}\", exc_info=False)\n",
    "\n",
    "                # --- Save Importance ---\n",
    "                feat_names = list(X.columns) if isinstance(X, pd.DataFrame) else None\n",
    "                if feat_names:\n",
    "                    logger.info(f\"Saving importance {model_type}...\")\n",
    "                    save_feature_importance(final_model, feat_names, timestamp, model_type)\n",
    "                else:\n",
    "                    logger.warning(f\"No feat names for importance {model_type}.\")\n",
    "\n",
    "            else:  # Fit failed\n",
    "                final_model = None  # Ensure model is None if fit failed\n",
    "\n",
    "        else:  # Instantiation failed\n",
    "            logger.error(f\"Could not instantiate final model {model_type}.\")\n",
    "            return None, best_cv_score, best_params\n",
    "\n",
    "    except Exception as final_e:\n",
    "        logger.error(f\"Failed final instantiate/fit/save process {model_type}: {final_e}\", exc_info=True)\n",
    "        # Return score/params from Optuna, but model is None\n",
    "        return None, best_cv_score, best_params\n",
    "\n",
    "    # Return potentially None model if fit/save failed, but score/params if Optuna succeeded\n",
    "    return final_model, best_cv_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8951a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import logging\n",
    "import os\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "# Meta-learner imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb # For new default meta-learner\n",
    "# Calibration import (needed if implementing Step 3)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scikeras.wrappers import KerasClassifier # For type checking\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_ensemble(qualified_models_with_scores, X_train_ensemble, y_train_ensemble, timestamp, n_jobs_ensemble=1):\n",
    "    \"\"\"\n",
    "    Creates Voting and Stacking ensembles from a list of qualified models.\n",
    "    Default Stacking meta-learner is now LightGBM with class weights.\n",
    "    Includes comments for adding model calibration.\n",
    "\n",
    "    Args:\n",
    "        qualified_models_with_scores (list): List of tuples: (name, fitted_model, cv_score).\n",
    "                                             Assumes models were potentially trained with class weights.\n",
    "        X_train_ensemble (pd.DataFrame or np.ndarray): Training features for fitting ensembles.\n",
    "        y_train_ensemble (pd.Series or np.ndarray): Training target for fitting ensembles.\n",
    "        timestamp (str): Timestamp string for saving files.\n",
    "        n_jobs_ensemble (int): Number of parallel jobs for ensemble fitting.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fitted_voting_classifier, fitted_stacking_classifier, best_individual_model_object)\n",
    "               Models can be None if creation failed or skipped.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting ensemble creation...\")\n",
    "\n",
    "    if not qualified_models_with_scores:\n",
    "        logger.error(\"No qualified models provided for ensemble creation.\")\n",
    "        return None, None, None\n",
    "\n",
    "    sorted_models = sorted(qualified_models_with_scores, key=lambda x: x[2], reverse=True)\n",
    "    logger.info(f\"Qualified models for ensembling (Name, CV Score): {[(m[0], f'{m[2]:.5f}') for m in sorted_models]}\")\n",
    "    N_ens = len(sorted_models)\n",
    "\n",
    "    if N_ens < 2:\n",
    "        logger.warning(f\"Less than 2 qualified models ({N_ens}). Skipping ensembles.\")\n",
    "        if N_ens == 1: n, m, s = sorted_models[0]; logger.info(f\"Returning single best model: {n} (CV: {s:.5f})\"); return None, None, m\n",
    "        else: return None, None, None\n",
    "\n",
    "    # Filter usable models (as before)\n",
    "    estimators_valid_for_ensemble = []\n",
    "    keras_models_excluded = []\n",
    "    for name, model, score in sorted_models:\n",
    "        is_keras_wrapper = isinstance(model, KerasClassifier); model_saved_path = f'models/{name}_{timestamp}.joblib'; tf_model_path = f'models/{name}_tfmodel_{timestamp}'\n",
    "        if is_keras_wrapper and not os.path.exists(tf_model_path) and not os.path.exists(model_saved_path): logger.warning(f\"Keras model {name} save files missing. Exclude.\"); keras_models_excluded.append(name)\n",
    "        else: estimators_valid_for_ensemble.append((name, model))\n",
    "\n",
    "    if len(estimators_valid_for_ensemble) < 2:\n",
    "        logger.warning(f\"<2 models usable for ensemble. Skipping.\"); non_keras_models = [(n, m, s) for n, m, s in sorted_models if n not in keras_models_excluded]\n",
    "        if non_keras_models: best_n, best_m, best_s = non_keras_models[0]; logger.info(f\"Return best non-excluded: {best_n} ({best_s:.5f})\"); return None, None, best_m\n",
    "        elif sorted_models: best_n, best_m, best_s = sorted_models[0]; logger.info(f\"Return original best: {best_n} ({best_s:.5f})\"); return None, None, best_m\n",
    "        else: return None, None, None\n",
    "\n",
    "    logger.info(f\"Using {len(estimators_valid_for_ensemble)} models for ensemble: {[n for n,m in estimators_valid_for_ensemble]}\")\n",
    "    est_ens = estimators_valid_for_ensemble\n",
    "\n",
    "    # --- Optional Calibration Step (Implement if needed based on Step 3 plan) ---\n",
    "    # Example: Calibrate base models before putting them in est_ens_calibrated\n",
    "    est_ens_calibrated = []\n",
    "    logger.info(\"Calibrating base models for ensemble...\")\n",
    "    for name, model in est_ens:\n",
    "        try:\n",
    "            # Use isotonic calibration, fit on the same training data used for ensemble fitting\n",
    "            # CV within CalibratedClassifierCV helps prevent overfitting during calibration itself\n",
    "            calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=3, n_jobs=n_jobs_ensemble, ensemble=False) # Fit base estimator on each fold\n",
    "            calibrated_model.fit(X_train_ensemble, y_train_ensemble)\n",
    "            calibrated_model_path = f'calibrated_models/{name}_calibrated_{timestamp}.joblib'\n",
    "            joblib.dump(calibrated_model, calibrated_model_path)\n",
    "            est_ens_calibrated.append((name + \"_calibrated\", calibrated_model)) # Use new name and calibrated model\n",
    "            logger.info(f\"Calibrated model {name} and saved to {calibrated_model_path}\")\n",
    "        except Exception as cal_err:\n",
    "            logger.error(f\"Failed to calibrate model {name}: {cal_err}. Skipping calibration for this model.\", exc_info=True)\n",
    "            est_ens_calibrated.append((name, model)) # Use original model if calibration fails\n",
    "    est_ens_to_use = est_ens_calibrated # Use the calibrated list for subsequent steps\n",
    "    logger.info(f\"Using {len(est_ens_to_use)} models (calibrated where possible) for final ensemble.\")\n",
    "    # --- End Optional Calibration Step ---\n",
    "\n",
    "    # Use original (potentially uncalibrated) estimators for now\n",
    "    est_ens_to_use = est_ens\n",
    "\n",
    "    # --- Voting Classifier ---\n",
    "    vote_clf = None\n",
    "    can_soft = all(hasattr(m, 'predict_proba') for _, m in est_ens_to_use)\n",
    "    weights_used = None\n",
    "\n",
    "    if can_soft:\n",
    "        logger.info(\"Attempting Soft Voting...\")\n",
    "        # Weighting based on original CV scores (even if models are calibrated)\n",
    "        scores_map = {name: score for name, model, score in qualified_models_with_scores}\n",
    "        # Use original names to get scores, handle potentially calibrated names\n",
    "        scores = [scores_map.get(name.replace('_calibrated','')) for name, model in est_ens_to_use if name.replace('_calibrated','') in scores_map]\n",
    "\n",
    "        if scores and len(scores) == len(est_ens_to_use): # Ensure weights align\n",
    "            min_s = min(scores); shift_s = [s - min_s + 1e-6 for s in scores]; tot_s = sum(shift_s)\n",
    "            weights_used = [s / tot_s for s in shift_s] if tot_s > 0 else None\n",
    "            logger.info(f\"Weights:{list(np.round(weights_used,3)) if weights_used else 'Uniform'}\")\n",
    "        else:\n",
    "            logger.warning(\"Could not align scores for weighting. Using uniform weights.\")\n",
    "            weights_used = None\n",
    "\n",
    "        try:\n",
    "            vote_clf = VotingClassifier(estimators=est_ens_to_use, voting='soft', weights=weights_used, n_jobs=n_jobs_ensemble)\n",
    "            vote_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            vote_path = f'models/voting_ensemble_soft_{timestamp}.joblib'\n",
    "            joblib.dump(vote_clf, vote_path)\n",
    "            logger.info(f\"Saved Soft Voting Ensemble: {vote_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Soft Voting: {e}\", exc_info=True); vote_clf = None; can_soft = False\n",
    "    else:\n",
    "        logger.warning(\"Not all base models support predict_proba for Soft Voting.\")\n",
    "\n",
    "    if not can_soft: # Fallback or initial choice\n",
    "        logger.warning(\"Attempting Hard Voting...\")\n",
    "        weights_used = None # No weights for hard voting\n",
    "        try:\n",
    "            vote_clf = VotingClassifier(estimators=est_ens_to_use, voting='hard', n_jobs=n_jobs_ensemble)\n",
    "            vote_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            vote_path = f'models/voting_ensemble_hard_{timestamp}.joblib'\n",
    "            joblib.dump(vote_clf, vote_path)\n",
    "            logger.info(f\"Saved Hard Voting Ensemble: {vote_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Hard Voting: {e}\", exc_info=True); vote_clf = None\n",
    "\n",
    "    # --- Stacking Classifier ---\n",
    "    stack_clf = None\n",
    "    # Check predict_proba on the models actually used in the ensemble list\n",
    "    can_stack = all(hasattr(m, 'predict_proba') for _, m in est_ens_to_use)\n",
    "    meta_learner = None\n",
    "\n",
    "    if can_stack:\n",
    "        logger.info(\"Attempting Stacking...\")\n",
    "        try:\n",
    "            # *** Use LightGBM as meta-learner with class weights ***\n",
    "            meta_learner = lgb.LGBMClassifier(\n",
    "                n_estimators=150, # Reasonably more estimators for meta\n",
    "                learning_rate=0.05, # Slightly lower LR\n",
    "                num_leaves=20,      # Limit complexity\n",
    "                # max_depth=5,       # Optional: limit depth further\n",
    "                class_weight='balanced', # Crucial for imbalanced meta-predictions\n",
    "                random_state=42,\n",
    "                n_jobs=1 # Meta learner should run on single core\n",
    "            )\n",
    "            # Alternative: Simpler Logistic Regression\n",
    "            # meta_learner = LogisticRegression(random_state=42, class_weight='balanced', solver='liblinear', C=1.0, n_jobs=1)\n",
    "\n",
    "            logger.info(f\"Using {meta_learner.__class__.__name__} as stacking meta-learner.\")\n",
    "            stack_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "            stack_clf = StackingClassifier(\n",
    "                estimators=est_ens_to_use, # Use potentially calibrated models\n",
    "                final_estimator=meta_learner,\n",
    "                cv=stack_cv,\n",
    "                stack_method='predict_proba',\n",
    "                n_jobs=1,\n",
    "                passthrough=False\n",
    "            )\n",
    "            stack_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            stack_path = f'models/stacking_ensemble_{timestamp}.joblib'\n",
    "            joblib.dump(stack_clf, stack_path)\n",
    "            logger.info(f\"Saved Stacking Ensemble (Meta: {meta_learner.__class__.__name__}): {stack_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Stacking: {e}\", exc_info=True)\n",
    "            stack_clf = None\n",
    "    else:\n",
    "        logger.warning(\"Cannot create Stacking Ensemble (requires predict_proba).\")\n",
    "\n",
    "    # Determine best individual model from the original qualified list\n",
    "    best_ind_q_model = None; best_n = \"N/A\"; best_s = -1.0\n",
    "    if sorted_models:\n",
    "        best_n, best_m, best_s = sorted_models[0]\n",
    "        best_ind_q_model = best_m\n",
    "        logger.info(f\"Best individual qualified model identified: {best_n} (CV Score: {best_s:.5f})\")\n",
    "\n",
    "    # Save summary (remains the same logic)\n",
    "    try:\n",
    "        summary_path = f'results/ensemble_creation_summary_{timestamp}.txt'\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(\"Ensemble Summary\\n=============\\nQualified Models:\\n\")\n",
    "            for n, _, s in sorted_models: f.write(f\"- {n}: CV {s:.5f} {'(Excl.)' if n in keras_models_excluded else '(Incl.)'}\\n\")\n",
    "            f.write(f\"\\nEnsembles ({len(est_ens_to_use)} models):\\n\") # Reflect models used\n",
    "            vote_t = 'Soft' if vote_clf and vote_clf.voting == 'soft' else ('Hard' if vote_clf and vote_clf.voting == 'hard' else 'N/A')\n",
    "            f.write(f\"- Voting ({vote_t}): {'Saved' if vote_clf else 'Failed/Skipped'}.\\n\")\n",
    "            meta_name = meta_learner.__class__.__name__ if meta_learner and stack_clf else 'N/A'\n",
    "            f.write(f\"- Stacking (Meta:{meta_name}): {'Saved' if stack_clf else 'Failed/Skipped'}.\\n\")\n",
    "            if keras_models_excluded: f.write(f\"\\nKeras Excluded: {', '.join(keras_models_excluded)}\\n\")\n",
    "            if best_ind_q_model: f.write(f\"\\nBest individual model overall (from qualified list): {best_n}\\n\")\n",
    "        logger.info(f\"Saved ensemble summary: {summary_path}\")\n",
    "    except Exception as file_e: logger.warning(f\"Could not save ensemble summary: {file_e}\")\n",
    "\n",
    "    return vote_clf, stack_clf, best_ind_q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c12018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scikeras.wrappers import KerasClassifier # Keep for isinstance check\n",
    "import joblib # Keep if fallback LE loading is needed\n",
    "import os # For checking plot/results dirs\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def evaluate_model(model, X_eval, y_eval, model_name, timestamp, le):\n",
    "    \"\"\"Evaluates a trained model on a given dataset (e.g., validation or hold-out).\"\"\"\n",
    "    if model is None:\n",
    "        logger.warning(f\"Skip eval {model_name}: model None.\")\n",
    "        return None, None # Return None for accuracy and report dictionary\n",
    "\n",
    "    # Handle LabelEncoder loading if missing\n",
    "    if le is None:\n",
    "        logger.warning(f\"Skip eval {model_name}: LabelEncoder (le) is None. Attempting fallback.\")\n",
    "        try:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "            if encoder_files:\n",
    "                le = joblib.load(f'features/{encoder_files[-1]}')\n",
    "                logger.info(f\"Loaded fallback LE for evaluation: {encoder_files[-1]}\")\n",
    "            else:\n",
    "                logger.error(\"Evaluation cannot proceed without LabelEncoder.\")\n",
    "                return None, None\n",
    "        except Exception as load_err:\n",
    "             logger.error(f\"Evaluation failed: Could not load fallback LE: {load_err}\")\n",
    "             return None, None\n",
    "\n",
    "    logger.info(f\"Evaluating model '{model_name}'...\")\n",
    "    # Ensure y_eval is a numpy array for consistency\n",
    "    if isinstance(y_eval, pd.Series):\n",
    "        y_eval = y_eval.values\n",
    "\n",
    "    is_keras_wrapper = isinstance(model, KerasClassifier)\n",
    "\n",
    "    try:\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_eval)\n",
    "        \n",
    "        # --- ADDED: Handle CatBoost prediction type ---\n",
    "        # Check if the model is CatBoost AND predictions are not integers\n",
    "        is_catboost = 'CatBoostClassifier' in str(type(model)) # Simple check\n",
    "        if is_catboost and not np.issubdtype(y_pred.dtype, np.integer):\n",
    "            logger.warning(f\"CatBoost predictions seem non-integer ({y_pred.dtype}): {y_pred[:5]}. Attempting mapping to encoded labels.\")\n",
    "            # Create a mapping from original string labels -> encoded integers\n",
    "            label_to_encoded = {label: i for i, label in enumerate(le.classes_)}\n",
    "            try:\n",
    "                # Attempt direct mapping (if CatBoost predicts original labels like 'low')\n",
    "                y_pred_mapped = np.array([label_to_encoded.get(str(p), -1) for p in y_pred.flatten()])\n",
    "                if np.any(y_pred_mapped == -1): # Check if direct mapping failed\n",
    "                    logger.warning(\"Direct label mapping failed, trying string-to-int mapping (assuming '0', '1', '2').\")\n",
    "                    # Attempt string-to-int mapping (if CatBoost predicts '0', '1', '2')\n",
    "                    y_pred_mapped = np.array([int(p) for p in y_pred.flatten()])\n",
    "                y_pred = y_pred_mapped # Use the mapped integer predictions\n",
    "                logger.info(f\"Successfully mapped CatBoost predictions to integers: {y_pred[:5]}\")\n",
    "            except Exception as map_err:\n",
    "                logger.error(f\"Failed to map CatBoost predictions to integers: {map_err}. Evaluation/Prediction may fail.\")\n",
    "                # Let the original y_pred pass through, error will likely occur later\n",
    "        # --- END CatBoost Handling ---\n",
    "\n",
    "        # Ensure predictions are class indices (for Keras primarily, now redundant for CatBoost if mapped)\n",
    "        if isinstance(model, KerasClassifier) and y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "            # ... (argmax logic) ...\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        elif not np.issubdtype(y_pred.dtype, np.integer):\n",
    "             logger.error(f\"Predictions for {model_name} are not integers after processing: {y_pred.dtype}. Evaluation may fail.\")\n",
    "             # Optionally try to force conversion, but it's risky\n",
    "             # y_pred = y_pred.astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_eval, y_pred)\n",
    "\n",
    "        # Generate classification report and confusion matrix\n",
    "        try:\n",
    "            y_eval_labels = le.inverse_transform(y_eval)\n",
    "            y_pred_labels = le.inverse_transform(y_pred)\n",
    "            target_names = le.classes_ # Get class names in correct order\n",
    "        except Exception as le_error:\n",
    "            logger.warning(f\"LabelEncoder inverse_transform failed for '{model_name}': {le_error}. Using numeric labels for report.\")\n",
    "            y_eval_labels = y_eval\n",
    "            y_pred_labels = y_pred\n",
    "            target_names = [str(i) for i in sorted(np.unique(y_eval))]\n",
    "\n",
    "        # Classification Report\n",
    "        report_str = classification_report(y_eval_labels, y_pred_labels, target_names=target_names, zero_division=0)\n",
    "        report_dict = classification_report(y_eval_labels, y_pred_labels, target_names=target_names, output_dict=True, zero_division=0)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_eval_labels, y_pred_labels, labels=target_names)\n",
    "\n",
    "        logger.info(f\"Evaluation Results for '{model_name}':\")\n",
    "        logger.info(f\"  Accuracy: {accuracy:.5f}\")\n",
    "\n",
    "        # Ensure results directory exists\n",
    "        results_dir = 'results'\n",
    "        if not os.path.exists(results_dir): os.makedirs(results_dir)\n",
    "\n",
    "        # Save evaluation results to a file\n",
    "        eval_filename = os.path.join(results_dir, f'{model_name}_evaluation_{timestamp}.txt')\n",
    "        with open(eval_filename, 'w') as f:\n",
    "            f.write(f\"Model Evaluation Summary\\n=========================\\n\")\n",
    "            f.write(f\"Model Name: {model_name}\\nTimestamp: {timestamp}\\nAccuracy: {accuracy:.5f}\\n\\n\")\n",
    "            f.write(\"Classification Report:\\n\"); f.write(report_str)\n",
    "            f.write(\"\\n\\nConfusion Matrix:\\n\"); f.write(np.array2string(cm))\n",
    "        logger.info(f\"Saved evaluation summary: {eval_filename}\")\n",
    "\n",
    "        # Ensure plots directory exists\n",
    "        plot_dir = 'plots'\n",
    "        if not os.path.exists(plot_dir): os.makedirs(plot_dir)\n",
    "\n",
    "        # Save confusion matrix plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "        plt.xlabel('Predicted Label'); plt.ylabel('True Label')\n",
    "        plt.title(f'Confusion Matrix - {model_name} (Accuracy: {accuracy:.3f})')\n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(plot_dir, f'{model_name}_confusion_matrix_{timestamp}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close() # Close the plot\n",
    "        logger.info(f\"Saved confusion matrix plot: {plot_filename}\")\n",
    "\n",
    "        return accuracy, report_dict\n",
    "\n",
    "    except AttributeError as ae:\n",
    "         if 'predict' in str(ae): logger.error(f\"Evaluation error '{model_name}': Model not fitted? AttrErr: {ae}\", exc_info=True)\n",
    "         else: logger.error(f\"AttributeError during eval '{model_name}': {ae}\", exc_info=True)\n",
    "         return None, None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during eval '{model_name}': {e}\", exc_info=True)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab564437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import joblib\n",
    "import os\n",
    "from scikeras.wrappers import KerasClassifier # Keep for isinstance check\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def make_test_predictions(model, X_test, test_obs, timestamp, model_name, le):\n",
    "    \"\"\"Generates predictions on the test set and saves results.\"\"\"\n",
    "    logger.info(f\"Generating test predictions using {model_name}...\")\n",
    "\n",
    "    if model is None:\n",
    "        logger.error(f\"Predict fail {model_name}: model None.\")\n",
    "        return None\n",
    "\n",
    "    # Handle missing LabelEncoder (with fallback)\n",
    "    if le is None:\n",
    "        logger.warning(f\"LE None for {model_name}. Try fallback...\")\n",
    "        try:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "            if encoder_files:\n",
    "                le=joblib.load(f'features/{encoder_files[-1]}')\n",
    "                logger.info(f\"Loaded fallback LE: {encoder_files[-1]}.\")\n",
    "            else:\n",
    "                logger.error(f\"Predict fail: LE None, no fallback.\")\n",
    "                return None\n",
    "        except Exception as load_e:\n",
    "            logger.error(f\"LE None, fallback load fail:{load_e}. No predict.\")\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Make predictions (potentially encoded integers or strings from CatBoost)\n",
    "        y_pred_raw = model.predict(X_test)\n",
    "\n",
    "        # --- ADDED: Handle CatBoost prediction type ---\n",
    "        y_pred_enc = y_pred_raw # Default assumption\n",
    "        is_catboost = 'CatBoostClassifier' in str(type(model))\n",
    "        if is_catboost and not np.issubdtype(y_pred_raw.dtype, np.integer):\n",
    "            logger.warning(f\"CatBoost test predictions non-integer ({y_pred_raw.dtype}): {y_pred_raw[:5]}. Attempting mapping.\")\n",
    "            label_to_encoded = {label: i for i, label in enumerate(le.classes_)}\n",
    "            try:\n",
    "                y_pred_mapped = np.array([label_to_encoded.get(str(p), -1) for p in y_pred_raw.flatten()])\n",
    "                if np.any(y_pred_mapped == -1):\n",
    "                    logger.warning(\"Direct label mapping failed, trying string-to-int mapping.\")\n",
    "                    y_pred_mapped = np.array([int(p) for p in y_pred_raw.flatten()])\n",
    "                y_pred_enc = y_pred_mapped # Use mapped integer predictions\n",
    "                logger.info(f\"Mapped CatBoost test predictions to integers: {y_pred_enc[:5]}\")\n",
    "            except Exception as map_err:\n",
    "                logger.error(f\"Failed map CatBoost test predictions: {map_err}. Prediction may fail.\")\n",
    "                # Use original predictions; inverse_transform will likely fail\n",
    "                y_pred_enc = y_pred_raw\n",
    "        # --- END CatBoost Handling ---\n",
    "\n",
    "        # Ensure predictions are class indices (for Keras)\n",
    "        if isinstance(model, KerasClassifier) and y_pred_enc.ndim > 1 and y_pred_enc.shape[1] > 1:\n",
    "            y_pred_enc = np.argmax(y_pred_enc, axis=1)\n",
    "        elif not np.issubdtype(y_pred_enc.dtype, np.integer):\n",
    "             logger.error(f\"Test predictions for {model_name} not integers after processing: {y_pred_enc.dtype}. Inverse transform likely to fail.\")\n",
    "             # Cannot proceed reliably if predictions aren't integer encoded labels here\n",
    "\n",
    "        # Inverse transform to get original salary category labels\n",
    "        predicted_labels = le.inverse_transform(y_pred_enc) # This now expects integer y_pred_enc\n",
    "\n",
    "\n",
    "        # Create submission DataFrame\n",
    "        submission_df = pd.DataFrame({'obs': test_obs, 'salary_category': predicted_labels})\n",
    "\n",
    "        # Sanitize model name for filename\n",
    "        safe_model_name = model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "        # Ensure directories exist\n",
    "        submission_dir = 'submissions'\n",
    "        results_dir = 'results'\n",
    "        if not os.path.exists(submission_dir): os.makedirs(submission_dir)\n",
    "        if not os.path.exists(results_dir): os.makedirs(results_dir)\n",
    "\n",
    "        submission_path = os.path.join(submission_dir, f'solution_{safe_model_name}_{timestamp}.csv')\n",
    "\n",
    "        # Save submission file\n",
    "        submission_df.to_csv(submission_path, index=False)\n",
    "        logger.info(f\"Saved submission: {submission_path}\")\n",
    "\n",
    "        # Log value counts of predictions for analysis\n",
    "        pred_value_counts = submission_df['salary_category'].value_counts().to_dict()\n",
    "        logger.info(f\"Test prediction distribution for '{model_name}': {pred_value_counts}\")\n",
    "\n",
    "        # Save prediction summary\n",
    "        summary_filename = os.path.join(results_dir, f'{safe_model_name}_test_prediction_summary_{timestamp}.txt')\n",
    "        with open(summary_filename, 'w') as f:\n",
    "            f.write(f\"Test Prediction Summary\\n======================\\n\")\n",
    "            f.write(f\"Model Name: {model_name}\\n\")\n",
    "            try: f.write(f\"Model Class: {model.__class__.__name__}\\n\")\n",
    "            except: f.write(f\"Model Class: N/A\\n\")\n",
    "            f.write(f\"Timestamp: {timestamp}\\nTotal Predictions: {len(predicted_labels)}\\n\\nDistribution:\\n\")\n",
    "            total_preds = len(predicted_labels)\n",
    "            if total_preds > 0:\n",
    "                for label, count in sorted(pred_value_counts.items()):\n",
    "                    percentage = (count / total_preds) * 100\n",
    "                    f.write(f\"- {label}: {count} ({percentage:.2f}%)\\n\")\n",
    "            else:\n",
    "                f.write(\"- No predictions were generated.\\n\")\n",
    "        logger.info(f\"Saved test prediction summary: {summary_filename}\")\n",
    "\n",
    "        return submission_df\n",
    "\n",
    "    except AttributeError as ae:\n",
    "        logger.error(f\"AttributeError during predict/inverse_transform '{model_name}': {ae}.\", exc_info=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during test prediction '{model_name}': {e}\", exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc317202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier # Keep for FS\n",
    "# Imports for models used within the pipeline are assumed to be at the top of the file\n",
    "# Imports for utility functions are assumed to be at the top of the file\n",
    "import os\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "# Assume utility functions (create_directory_structure, get_timestamp) are defined\n",
    "# Assume preprocessing function (preprocess_data) is defined\n",
    "# Assume model optimization function (optimize_model) is defined\n",
    "# Assume ensemble function (create_ensemble) is defined\n",
    "# Assume evaluation function (evaluate_model) is defined\n",
    "# Assume prediction function (make_test_predictions) is defined\n",
    "\n",
    "def run_complete_pipeline(perform_feature_selection=False, min_cv_score_threshold=0.72, fs_threshold='mean', n_jobs_sklearn=1):\n",
    "    \"\"\"\n",
    "    Run the complete model training pipeline with combined FE logic,\n",
    "    updated model list, and class weight balancing strategy integrated\n",
    "    into optimize_model.\n",
    "    \"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    main_log_file = None\n",
    "    file_handler = None\n",
    "    pipeline_success = False # Track overall success\n",
    "\n",
    "    try:\n",
    "        # 1. Setup\n",
    "        print(\"--- Starting Complete Pipeline Run (Class Weights, LGBM added) ---\")\n",
    "        create_directory_structure()\n",
    "        main_log_file = f'logs/pipeline_run_{timestamp}.log'\n",
    "        file_handler = logging.FileHandler(main_log_file)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        if file_handler.baseFilename not in [h.baseFilename for h in logger.handlers if isinstance(h, logging.FileHandler)]:\n",
    "             logger.addHandler(file_handler)\n",
    "\n",
    "        logger.info(f\"--- Starting Complete Pipeline Run --- Timestamp: {timestamp} ---\")\n",
    "        logger.info(f\"Pipeline Config: Combined FE, Scaling=True, FeatSelect={perform_feature_selection} (Thresh={fs_threshold}), CV Thresh={min_cv_score_threshold}, n_jobs={n_jobs_sklearn} used, Class Weights Enabled, Const Cols Kept\")\n",
    "        logger.info(f\"Logging detailed output to: {main_log_file}\")\n",
    "\n",
    "        # 2. Load Data\n",
    "        logger.info(\"Loading data...\")\n",
    "        try: train_df = pd.read_csv('train.csv'); test_df = pd.read_csv('test.csv'); logger.info(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "        except FileNotFoundError as e: logger.error(f\"Data load error: {e}.\"); raise\n",
    "        if 'salary_category' not in train_df.columns: logger.error(\"Target missing.\"); raise ValueError(\"Missing target\")\n",
    "        if train_df.empty or test_df.empty: logger.error(\"Data empty.\"); raise ValueError(\"Empty data\")\n",
    "        if 'obs' not in test_df.columns: logger.error(\"Column 'obs' missing in test.csv.\"); raise ValueError(\"Missing obs\")\n",
    "        all_states = set(train_df['job_state'].dropna().unique()).union(set(test_df['job_state'].dropna().unique()))\n",
    "        all_feature1 = set(train_df['feature_1'].dropna().unique()).union(set(test_df['feature_1'].dropna().unique()))\n",
    "        logger.info(f\"Found {len(all_states)} unique states and {len(all_feature1)} unique feature_1 values.\")\n",
    "\n",
    "        # 3. Preprocess Training Data\n",
    "        logger.info(\"Preprocessing training data (using combined logic)...\")\n",
    "        X_train_orig, y_train_orig, feature_cols_initial, label_encoder = preprocess_data(train_df, all_states, all_feature1, timestamp, is_training=True)\n",
    "        if X_train_orig is None or y_train_orig is None or label_encoder is None or feature_cols_initial is None: logger.error(\"Train preprocess failed.\"); raise RuntimeError(\"Preprocessing train failed\")\n",
    "        logger.info(f\"Train preprocess done. Initial Feats: {X_train_orig.shape[1]}\"); y_train_orig = pd.Series(y_train_orig)\n",
    "\n",
    "        # 4. Train/Validation Split\n",
    "        logger.info(\"Splitting data (80/20)...\")\n",
    "        X_train_full, X_val, y_train_full, y_val = train_test_split(X_train_orig, y_train_orig, test_size=0.20, random_state=42, stratify=y_train_orig)\n",
    "        logger.info(f\"Train (Pre-scale): {X_train_full.shape}, Val (Pre-scale): {X_val.shape}\"); y_train_full = pd.Series(y_train_full, index=X_train_full.index); y_val = pd.Series(y_val, index=X_val.index)\n",
    "\n",
    "        # 5. SCALING STEP\n",
    "        logger.info(\"Applying StandardScaler...\")\n",
    "        scaler = StandardScaler(); X_train_full_scaled = scaler.fit_transform(X_train_full[feature_cols_initial]); X_val_scaled = scaler.transform(X_val[feature_cols_initial])\n",
    "        X_train_full_scaled = pd.DataFrame(X_train_full_scaled, index=X_train_full.index, columns=feature_cols_initial)\n",
    "        X_val_scaled = pd.DataFrame(X_val_scaled, index=X_val.index, columns=feature_cols_initial)\n",
    "        scaler_path = f'scalers/scaler_{timestamp}.joblib'; joblib.dump(scaler, scaler_path); logger.info(f\"Scaler saved: {scaler_path}\")\n",
    "\n",
    "        # 6. Preprocess & Scale Test Data\n",
    "        logger.info(\"Preprocessing test data (using combined logic)...\")\n",
    "        X_test_orig, _, _, _ = preprocess_data(test_df, all_states, all_feature1, timestamp, is_training=False, feature_columns_to_use=feature_cols_initial)\n",
    "        if X_test_orig is None: logger.error(\"Test preprocess failed.\"); raise RuntimeError(\"Preprocessing test failed\")\n",
    "        try: X_test_aligned = X_test_orig[feature_cols_initial]; logger.info(\"Test columns aligned.\")\n",
    "        except KeyError as ke: logger.error(f\"Test col mismatch after preprocess: {ke}.\"); raise RuntimeError(f\"Test column mismatch: {ke}\")\n",
    "        logger.info(\"Scaling test data...\"); X_test_scaled = scaler.transform(X_test_aligned)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test_aligned.index, columns=feature_cols_initial)\n",
    "        logger.info(f\"Test preprocess & scale done. Shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        # Define Data Partitions\n",
    "        X_opt_train = X_train_full_scaled.copy(); y_opt_train = y_train_full.copy(); X_holdout_val = X_val_scaled.copy(); y_holdout_val = y_val.copy(); X_final_test = X_test_scaled.copy()\n",
    "        current_feature_cols = list(feature_cols_initial)\n",
    "\n",
    "        # 7. Optional Feature Selection\n",
    "        if perform_feature_selection:\n",
    "            logger.info(f\"Performing feature selection (Threshold: {fs_threshold})...\")\n",
    "            try:\n",
    "                selector_model = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=n_jobs_sklearn, class_weight='balanced', max_depth=20)\n",
    "                logger.info(\"Fitting RF for feature selection...\"); selector_model.fit(X_opt_train, y_opt_train)\n",
    "                selector = SelectFromModel(selector_model, threshold=fs_threshold, prefit=True)\n",
    "                selected_mask = selector.get_support(); selected_features = X_opt_train.columns[selected_mask]\n",
    "                num_orig = X_opt_train.shape[1]; num_sel = len(selected_features)\n",
    "                if num_sel == 0: logger.error(\"FS removed ALL features!\"); raise RuntimeError(\"FS removed all features.\")\n",
    "                elif num_sel < num_orig:\n",
    "                    num_removed = num_orig - num_sel; logger.info(f\"Feat selection removed {num_removed} features. Selected {num_sel}.\")\n",
    "                    current_feature_cols = list(selected_features)\n",
    "                    X_opt_train = X_opt_train[current_feature_cols]; X_holdout_val = X_holdout_val[current_feature_cols]; X_final_test = X_final_test[current_feature_cols]\n",
    "                    logger.info(f\"Selection applied to train/val/test partitions.\")\n",
    "                    joblib.dump(current_feature_cols, f'features/selected_feature_columns_{timestamp}.joblib')\n",
    "                else: logger.info(f\"Feature selection removed no features with threshold '{fs_threshold}'.\"); perform_feature_selection = False\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error feature selection: {e}. Use all scaled.\", exc_info=True); perform_feature_selection = False\n",
    "                current_feature_cols = list(feature_cols_initial); X_opt_train=X_train_full_scaled[current_feature_cols]; X_holdout_val=X_val_scaled[current_feature_cols]; X_final_test=X_test_scaled[current_feature_cols]\n",
    "        else:\n",
    "            logger.info(\"Skipping feature selection.\")\n",
    "            X_opt_train = X_opt_train[current_feature_cols]; X_holdout_val = X_holdout_val[current_feature_cols]; X_final_test = X_final_test[current_feature_cols]\n",
    "\n",
    "        logger.info(f\"Data shapes post-scaling/selection: Train={X_opt_train.shape}, Val={X_holdout_val.shape}, Test={X_final_test.shape}\")\n",
    "        logger.info(f\"Number of features used in modeling: {len(current_feature_cols)}\")\n",
    "\n",
    "        # 8. Optimize, Train Base Models & Make Individual Predictions\n",
    "        # --- UPDATED LIST (No SVC, Added LightGBM) ---\n",
    "        models_to_optimize = [\n",
    "            ('logistic', 30), ('knn', 25), ('adaboost', 30),\n",
    "            ('randomforest', 40), ('extratrees', 40),\n",
    "            ('gradientboosting', 40), ('mlp', 25), ('keras_mlp', 20),\n",
    "            ('catboost', 40), ('xgboost', 50), ('lightgbm', 50)\n",
    "        ]\n",
    "        qualified_models_with_scores = [] # Stores (name, fitted_model, cv_score)\n",
    "        optimized_params_all = {} # Stores best params found by Optuna\n",
    "\n",
    "        logger.info(f\"--- Optimizing models (Class Weights Enabled Where Applicable) ---\")\n",
    "        logger.info(f\"Minimum CV score threshold for qualification: {min_cv_score_threshold}\")\n",
    "        logger.info(f\"Optimization Order: {[m[0] for m in models_to_optimize]}\")\n",
    "\n",
    "        for model_name, n_trials in models_to_optimize:\n",
    "            indiv_sub_df = None\n",
    "            try:\n",
    "                logger.info(f\"--- Optimizing {model_name} ({n_trials} trials) ---\")\n",
    "                # Optimize_model now handles final training with class_weight where applicable\n",
    "                final_model, best_cv_score, best_params = optimize_model(\n",
    "                    X_opt_train, y_opt_train, timestamp, model_name,\n",
    "                    n_trials=n_trials, n_jobs_optuna=n_jobs_sklearn\n",
    "                )\n",
    "\n",
    "                # Log details for qualification debugging\n",
    "                log_score = best_cv_score if best_cv_score is not None else -1.0\n",
    "                comparison_result = best_cv_score >= min_cv_score_threshold if best_cv_score is not None else False\n",
    "                logger.info(f\"Qualification Check for {model_name}: \"\n",
    "                            f\"final_model exists? {final_model is not None}, \"\n",
    "                            f\"best_cv_score={log_score:.7f}, \"\n",
    "                            f\"threshold={min_cv_score_threshold}, \"\n",
    "                            f\"comparison result: {comparison_result}\")\n",
    "\n",
    "                # Qualification Check\n",
    "                if final_model is not None and best_cv_score is not None and best_cv_score >= min_cv_score_threshold:\n",
    "                    logger.info(f\"+++ QUALIFIED: {model_name} (CV Score: {best_cv_score:.5f})\")\n",
    "                    # Evaluate on Holdout\n",
    "                    holdout_acc, _ = evaluate_model(final_model, X_holdout_val, y_holdout_val, f\"{model_name}_qualified_holdout_eval\", timestamp, label_encoder)\n",
    "                    if holdout_acc is not None:\n",
    "                        logger.info(f\"Hold-out Acc ({model_name}): {holdout_acc:.5f}\")\n",
    "                        # Store model with HOLD-OUT score for potential ensemble weighting later\n",
    "                        qualified_models_with_scores.append((model_name, final_model, holdout_acc)) # Store HOLD-OUT acc\n",
    "                    else:\n",
    "                        logger.warning(f\"Hold-out Eval failed for {model_name}. Cannot use its score for weighting.\")\n",
    "                        # Still add model? Maybe add with CV score as fallback weight? Or exclude?\n",
    "                        # Let's add with CV score for now, but this case needs consideration.\n",
    "                        qualified_models_with_scores.append((model_name, final_model, best_cv_score))\n",
    "\n",
    "                    if best_params: optimized_params_all[model_name] = best_params\n",
    "\n",
    "                    # Generate Individual Predictions\n",
    "                    logger.info(f\"--- Generating individual predictions for {model_name} ---\")\n",
    "                    indiv_sub_df = make_test_predictions(final_model, X_final_test, test_df['obs'], timestamp, f\"{model_name}_qual_individual_pred\", label_encoder)\n",
    "                    if indiv_sub_df is None: logger.error(f\"Failed individual predictions for {model_name}.\")\n",
    "                    else: logger.info(f\"Individual prediction file saved for {model_name}.\")\n",
    "\n",
    "                elif best_cv_score is not None: # Didn't meet threshold or final fit failed\n",
    "                     logger.info(f\"--- NOT QUALIFIED: {model_name} (CV Score: {best_cv_score:.5f} {' - Final model fit/save failed' if final_model is None else ''}) ---\")\n",
    "                     if best_params: optimized_params_all[model_name] = best_params # Still save params\n",
    "                else: # Optuna itself failed or returned None score\n",
    "                    logger.warning(f\"Optimization failed or returned invalid score for {model_name}. Skip.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in main optimization loop for {model_name}: {e}\", exc_info=True)\n",
    "\n",
    "        # --- Post-Optimization Check ---\n",
    "        logger.info(\"--- Model Optimization Phase Complete ---\")\n",
    "        if not qualified_models_with_scores:\n",
    "            logger.error(f\"CRITICAL: NO models met CV threshold {min_cv_score_threshold}. Aborting.\")\n",
    "            if file_handler: logger.removeHandler(file_handler); file_handler.close()\n",
    "            return False\n",
    "        logger.info(f\"--- {len(qualified_models_with_scores)} models qualified. ---\")\n",
    "        # Log based on HOLD-OUT score now stored\n",
    "        logger.info(f\"Qualified Models (Name, Holdout Acc/CV Score): {[(m[0], f'{m[2]:.5f}') for m in qualified_models_with_scores]}\")\n",
    "\n",
    "        # 9. Create Ensembles & Select FINAL Best Model\n",
    "        final_model = None; final_model_name = \"N/A\"; vote_ens = None; stack_ens = None; best_ind_q_model = None\n",
    "\n",
    "        if len(qualified_models_with_scores) == 1:\n",
    "            # Use holdout score here too\n",
    "            final_model_name, final_model, final_holdout_score = qualified_models_with_scores[0]\n",
    "            logger.warning(f\"Only 1 qualified: {final_model_name} (Holdout Acc: {final_holdout_score:.5f}). Select it.\")\n",
    "            best_ind_q_model = final_model\n",
    "        elif len(qualified_models_with_scores) > 1:\n",
    "            logger.info(f\"--- Creating and Evaluating Ensembles (using balanced models if applicable) ---\")\n",
    "            # Pass potentially balanced models to create_ensemble\n",
    "            # create_ensemble now uses LGBM meta-learner by default\n",
    "            vote_ens, stack_ens, best_ind_q_model_obj = create_ensemble(\n",
    "                qualified_models_with_scores, X_opt_train, y_opt_train, timestamp, n_jobs_ensemble=n_jobs_sklearn\n",
    "            )\n",
    "\n",
    "            # Evaluate candidate ensembles and best individual on HOLD-OUT set\n",
    "            logger.info(\"--- Evaluating candidate final models on HOLD-OUT validation set ---\")\n",
    "            candidates = {} # Store {name: (holdout_accuracy, model_object)}\n",
    "            best_ind_name_from_ensemble = None # Track name from ensemble function\n",
    "\n",
    "            if vote_ens:\n",
    "                vote_model_name = f\"voting_ensemble_{vote_ens.voting}_qualified\"\n",
    "                logger.info(f\"--- Eval {vote_model_name} ---\")\n",
    "                val_acc, _ = evaluate_model(vote_ens, X_holdout_val, y_holdout_val, f\"{vote_model_name}_holdout_eval\", timestamp, label_encoder)\n",
    "                if val_acc is not None: candidates[vote_model_name] = (val_acc, vote_ens); logger.info(f\"Hold-out Acc ({vote_model_name}): {val_acc:.5f}\")\n",
    "                else: logger.warning(f\"Eval fail: {vote_model_name}\")\n",
    "\n",
    "            if stack_ens:\n",
    "                stack_meta_name = stack_ens.final_estimator_.__class__.__name__\n",
    "                stack_model_name = f\"stacking_ensemble_{stack_meta_name}_qualified\"\n",
    "                logger.info(f\"--- Eval {stack_model_name} ---\")\n",
    "                val_acc, _ = evaluate_model(stack_ens, X_holdout_val, y_holdout_val, f\"{stack_model_name}_holdout_eval\", timestamp, label_encoder)\n",
    "                if val_acc is not None: candidates[stack_model_name] = (val_acc, stack_ens); logger.info(f\"Hold-out Acc ({stack_model_name}): {val_acc:.5f}\")\n",
    "                else: logger.warning(f\"Eval fail: {stack_model_name}\")\n",
    "\n",
    "            # Find the best individual model's info again to evaluate it\n",
    "            if qualified_models_with_scores:\n",
    "                 # Sort by stored score (holdout acc or fallback CV score)\n",
    "                 best_ind_info = max(qualified_models_with_scores, key=lambda item: item[2])\n",
    "                 best_ind_name_from_list = best_ind_info[0]\n",
    "                 best_ind_q_model = best_ind_info[1] # The actual best model object\n",
    "                 best_ind_stored_score = best_ind_info[2]\n",
    "                 logger.info(f\"--- Eval Best Individual ({best_ind_name_from_list}, Stored Score: {best_ind_stored_score:.5f}) ---\")\n",
    "                 eval_name = f\"{best_ind_name_from_list}_best_qual_holdout_eval\"\n",
    "                 val_acc, _ = evaluate_model(best_ind_q_model, X_holdout_val, y_holdout_val, eval_name, timestamp, label_encoder)\n",
    "                 if val_acc is not None:\n",
    "                     cand_name = f\"{best_ind_name_from_list}_best_qualified\"\n",
    "                     candidates[cand_name] = (val_acc, best_ind_q_model)\n",
    "                     logger.info(f\"Hold-out Acc ({best_ind_name_from_list}): {val_acc:.5f}\")\n",
    "                 else: logger.warning(f\"Hold-out Eval failed for {best_ind_name_from_list}\")\n",
    "\n",
    "            # Select FINAL model based on highest HOLD-OUT accuracy among candidates\n",
    "            if candidates:\n",
    "                final_model_name = max(candidates, key=lambda k: candidates[k][0])\n",
    "                final_val_score, final_model = candidates[final_model_name]\n",
    "                logger.info(f\"--- FINAL MODEL SELECTION ---\")\n",
    "                logger.info(f\"Selected '{final_model_name}' as FINAL model (Hold-Out Acc: {final_val_score:.5f})\")\n",
    "            else:\n",
    "                logger.error(\"Hold-out evaluation failed for all candidates.\")\n",
    "                # Fallback: Use best individual based on stored score (holdout or CV)\n",
    "                if best_ind_q_model and best_ind_name_from_list:\n",
    "                    final_model = best_ind_q_model\n",
    "                    final_model_name = f\"{best_ind_name_from_list}_best_qualified_fallback\"\n",
    "                    logger.warning(f\"FALLBACK: Using best individual model '{final_model_name}' based on its stored score.\")\n",
    "                else:\n",
    "                    logger.error(\"Could not determine final model even as fallback. Aborting.\")\n",
    "                    raise RuntimeError(\"Final model selection failed.\")\n",
    "\n",
    "        # Check if a final model was successfully selected\n",
    "        if not final_model:\n",
    "            logger.error(\"No final model could be selected. Aborting.\")\n",
    "            raise RuntimeError(\"Final model selection failed.\")\n",
    "\n",
    "        # 10. Make FINAL Test Predictions\n",
    "        logger.info(f\"--- Generating FINAL predictions using: {final_model_name} ---\")\n",
    "        final_sub_df = make_test_predictions(final_model, X_final_test, test_df['obs'], timestamp, f\"{final_model_name}_FINAL\", label_encoder)\n",
    "        if final_sub_df is None:\n",
    "            logger.error(f\"Failed FINAL submission with {final_model_name}.\")\n",
    "            # Consider if pipeline should fail here\n",
    "            pipeline_success = False # Mark as failed if submission fails\n",
    "        else:\n",
    "            logger.info(f\"FINAL submission file generated with {final_model_name}.\")\n",
    "            pipeline_success = True # Mark successful only if prediction works\n",
    "\n",
    "        # 11. Final Summary\n",
    "        logger.info(\"--- Pipeline Run Summary ---\")\n",
    "        logger.info(f\"Timestamp: {timestamp}\")\n",
    "        logger.info(f\"Config: Combined FE, Scaling=True, FeatSelect={perform_feature_selection} (Thresh={fs_threshold}), CV Thresh={min_cv_score_threshold}, n_jobs={n_jobs_sklearn}, Class Weights Enabled, Const Cols Kept\")\n",
    "        logger.info(f\"Final # Features: {len(current_feature_cols)}\")\n",
    "        logger.info(\"Models Optimized: \" + \", \".join([m[0] for m in models_to_optimize]))\n",
    "        qual_details = [(m[0], f\"{m[2]:.5f}\") for m in qualified_models_with_scores] if qualified_models_with_scores else [\"None\"]\n",
    "        logger.info(\"Models Qualified (Name, Holdout Acc/CV Score): \" + \", \".join([f\"{n}({s})\" for n, s in qual_details]))\n",
    "        logger.info(f\"Ensembles Created: Voting={'Yes' if vote_ens else 'No'}, Stacking={'Yes' if stack_ens else 'No'} (Meta: {stack_ens.final_estimator_.__class__.__name__ if stack_ens else 'N/A'})\")\n",
    "        logger.info(f\"Final model selected: {final_model_name}\")\n",
    "        logger.info(\"Individual predictions saved for qualified models.\")\n",
    "        if final_sub_df is not None:\n",
    "            safe_final_n = final_model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\").replace(\" \", \"_\")\n",
    "            final_sub_path = f\"submissions/solution_{safe_final_n}_FINAL_{timestamp}.csv\"\n",
    "            logger.info(f\"Final submission file: {final_sub_path}\")\n",
    "        else:\n",
    "            logger.warning(\"No FINAL submission file was generated.\")\n",
    "        logger.info(f\"Logs in: {main_log_file}\")\n",
    "        logger.info(f\"--- Pipeline {'Completed Successfully' if pipeline_success else 'Completed with Errors'} ---\")\n",
    "\n",
    "        # Close log handler\n",
    "        if file_handler: logger.removeHandler(file_handler); file_handler.close()\n",
    "        return pipeline_success\n",
    "\n",
    "    # --- Main Exception Handling ---\n",
    "    except Exception as e:\n",
    "        logger.error(f\"--- Pipeline Failed Critically --- Error Type: {type(e).__name__} ---\")\n",
    "        logger.error(f\"Error Message: {e}\", exc_info=True)\n",
    "        # Ensure log handler is closed\n",
    "        if file_handler and file_handler in logger.handlers:\n",
    "            logger.removeHandler(file_handler)\n",
    "            file_handler.close()\n",
    "        return False # Indicate critical failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9d824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 18:18:48,315 - INFO - Creating directory structure...\n",
      "2025-04-25 18:18:48,316 - INFO - Created directory: models\n",
      "2025-04-25 18:18:48,318 - INFO - Created directory: features\n",
      "2025-04-25 18:18:48,319 - INFO - Created directory: results\n",
      "2025-04-25 18:18:48,319 - INFO - Created directory: submissions\n",
      "2025-04-25 18:18:48,319 - INFO - Created directory: logs\n",
      "2025-04-25 18:18:48,319 - INFO - Created directory: plots\n",
      "2025-04-25 18:18:48,319 - INFO - Created directory: optuna_trials\n",
      "2025-04-25 18:18:48,319 - INFO - Created directory: scalers\n",
      "2025-04-25 18:18:48,319 - INFO - Created directory: calibrated_models\n",
      "2025-04-25 18:18:48,325 - INFO - Directory structure verified/created.\n",
      "2025-04-25 18:18:48,325 - INFO - --- Starting Complete Pipeline Run --- Timestamp: 20250425_181848 ---\n",
      "2025-04-25 18:18:48,325 - INFO - Pipeline Config: Combined FE, Scaling=True, FeatSelect=True (Thresh=mean), CV Thresh=0.72, n_jobs=16 used, Class Weights Enabled, Const Cols Kept\n",
      "2025-04-25 18:18:48,327 - INFO - Logging detailed output to: logs/pipeline_run_20250425_181848.log\n",
      "2025-04-25 18:18:48,327 - INFO - Loading data...\n",
      "2025-04-25 18:18:48,402 - INFO - Train shape: (1280, 317), Test shape: (854, 316)\n",
      "2025-04-25 18:18:48,409 - INFO - Found 39 unique states and 5 unique feature_1 values.\n",
      "2025-04-25 18:18:48,409 - INFO - Preprocessing training data (using combined logic)...\n",
      "2025-04-25 18:18:48,409 - INFO - Starting preprocessing (Combined Logic). Is training: True\n",
      "2025-04-25 18:18:48,413 - INFO - Target 'salary_category' label encoded.\n",
      "2025-04-25 18:18:48,415 - INFO - Saved label encoder and mapping.\n",
      "2025-04-25 18:18:48,415 - INFO - Initial cleaning: Numerical and Boolean Features...\n",
      "2025-04-25 18:18:48,492 - WARNING - Col 'feature_10' has non-0/1 vals (834). Treat as numeric, impute median.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Complete Pipeline Run (Class Weights, LGBM added) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 18:18:48,494 - INFO - Starting Feature Engineering...\n",
      "2025-04-25 18:18:48,501 - INFO - Fit/saved TE for job_title_grouped\n",
      "2025-04-25 18:18:48,518 - INFO - Processed 'job_title'.\n",
      "2025-04-25 18:18:48,519 - INFO - Processed 'job_posted_date'.\n",
      "2025-04-25 18:18:48,530 - INFO - Added bin f9.\n",
      "2025-04-25 18:18:48,531 - INFO - Added: feature_2_9_interaction\n",
      "2025-04-25 18:18:48,534 - INFO - Added transforms f2 (sq, sqrt, bin).\n",
      "2025-04-25 18:18:48,536 - INFO - Added bool aggs.\n",
      "2025-04-25 18:18:48,537 - INFO - Added: feature_10_8_interaction\n",
      "2025-04-25 18:18:48,539 - INFO - Added new interactions: ['feat2_job_title_encoded', 'feat2_boolsum', 'feat2_recency', 'job_title_encoded_recency']\n",
      "2025-04-25 18:18:48,607 - INFO - Applying PCA (n=15) to job desc...\n",
      "2025-04-25 18:18:48,643 - INFO - Fit/saved PCA.\n",
      "2025-04-25 18:18:48,648 - INFO - Finished job desc features.\n",
      "2025-04-25 18:18:48,649 - INFO - Applying manual OHE for 'job_state' (39 unique).\n",
      "2025-04-25 18:18:48,651 - INFO - Applying manual OHE for 'feature_1' (5 unique).\n",
      "2025-04-25 18:18:48,662 - INFO - Final cleanup and column alignment...\n",
      "2025-04-25 18:18:48,671 - WARNING - Col 'is_senior' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-25 18:18:48,672 - WARNING - Col 'is_junior' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-25 18:18:48,673 - WARNING - Col 'is_developer' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-25 18:18:48,673 - WARNING - Col 'is_specialist' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-25 18:18:48,674 - INFO - Eng col 'feature_9_bin' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,676 - INFO - Eng col 'state_NM' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,676 - INFO - Eng col 'state_DC' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,677 - INFO - Eng col 'state_IA' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,677 - INFO - Eng col 'state_AK' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,677 - INFO - Eng col 'state_OR' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,678 - INFO - Eng col 'state_MD' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,678 - INFO - Eng col 'state_TX' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,679 - WARNING - Col 'state_KS' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-25 18:18:48,679 - INFO - Eng col 'state_NJ' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,681 - WARNING - Col 'state_WY' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-25 18:18:48,681 - INFO - Eng col 'state_VA' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,682 - INFO - Eng col 'state_AR' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,682 - INFO - Eng col 'state_SC' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,683 - INFO - Eng col 'state_UT' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,683 - INFO - Eng col 'state_CT' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,684 - INFO - Eng col 'state_GA' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,684 - INFO - Eng col 'state_NC' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_MN' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_IL' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_MO' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_LA' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_PA' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_OH' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_SD' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - WARNING - Col 'state_RI' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_NV' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_AL' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_OK' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_IN' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_AZ' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_KY' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_FL' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_MI' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_CA' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_NY' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_TN' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,685 - INFO - Eng col 'state_MA' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,696 - INFO - Eng col 'state_WA' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,697 - INFO - Eng col 'state_CO' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,697 - INFO - Eng col 'feat1_A' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,698 - INFO - Eng col 'feat1_B' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,698 - INFO - Eng col 'feat1_D' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,699 - INFO - Eng col 'feat1_C' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,699 - INFO - Eng col 'feat1_E' low card (nunique=2) in train.\n",
      "2025-04-25 18:18:48,700 - INFO - Saved 100 feature names (const cols NOT dropped).\n",
      "2025-04-25 18:18:48,702 - INFO - Preprocessing train done. Shape: (1280, 100). Time: 0.29s\n",
      "2025-04-25 18:18:48,706 - INFO - Train preprocess done. Initial Feats: 100\n",
      "2025-04-25 18:18:48,706 - INFO - Splitting data (80/20)...\n",
      "2025-04-25 18:18:48,710 - INFO - Train (Pre-scale): (1024, 100), Val (Pre-scale): (256, 100)\n",
      "2025-04-25 18:18:48,711 - INFO - Applying StandardScaler...\n",
      "2025-04-25 18:18:48,718 - INFO - Scaler saved: scalers/scaler_20250425_181848.joblib\n",
      "2025-04-25 18:18:48,718 - INFO - Preprocessing test data (using combined logic)...\n",
      "2025-04-25 18:18:48,718 - INFO - Starting preprocessing (Combined Logic). Is training: False\n",
      "2025-04-25 18:18:48,718 - INFO - Loaded LE: label_encoder_20250425_181848.joblib\n",
      "2025-04-25 18:18:48,718 - INFO - Initial cleaning: Numerical and Boolean Features...\n",
      "2025-04-25 18:18:48,784 - WARNING - Col 'feature_10' has non-0/1 vals (540). Treat as numeric, impute median.\n",
      "2025-04-25 18:18:48,786 - INFO - Starting Feature Engineering...\n",
      "2025-04-25 18:18:48,793 - INFO - Loaded TE: features/job_title_encoder_20250425_181848.joblib\n",
      "2025-04-25 18:18:48,797 - INFO - Processed 'job_title'.\n",
      "2025-04-25 18:18:48,797 - INFO - Processed 'job_posted_date'.\n",
      "2025-04-25 18:18:48,797 - INFO - Added bin f9.\n",
      "2025-04-25 18:18:48,797 - INFO - Added: feature_2_9_interaction\n",
      "2025-04-25 18:18:48,797 - INFO - Added transforms f2 (sq, sqrt, bin).\n",
      "2025-04-25 18:18:48,811 - INFO - Added bool aggs.\n",
      "2025-04-25 18:18:48,811 - INFO - Added: feature_10_8_interaction\n",
      "2025-04-25 18:18:48,814 - INFO - Added new interactions: ['feat2_job_title_encoded', 'feat2_boolsum', 'feat2_recency', 'job_title_encoded_recency']\n",
      "2025-04-25 18:18:48,847 - INFO - Applying PCA (n=15) to job desc...\n",
      "2025-04-25 18:18:48,864 - INFO - Loaded PCA: features/job_desc_pca_20250425_181848.joblib\n",
      "2025-04-25 18:18:48,864 - INFO - Finished job desc features.\n",
      "2025-04-25 18:18:48,880 - INFO - Applying manual OHE for 'job_state' (39 unique).\n",
      "2025-04-25 18:18:48,880 - INFO - Applying manual OHE for 'feature_1' (5 unique).\n",
      "2025-04-25 18:18:48,880 - INFO - Final cleanup and column alignment...\n",
      "2025-04-25 18:18:48,899 - INFO - Preprocessing test done. Shape: (854, 100). Time: 0.18s\n",
      "2025-04-25 18:18:48,899 - INFO - Test columns aligned.\n",
      "2025-04-25 18:18:48,899 - INFO - Scaling test data...\n",
      "2025-04-25 18:18:48,914 - INFO - Test preprocess & scale done. Shape: (854, 100)\n",
      "2025-04-25 18:18:48,915 - INFO - Performing feature selection (Threshold: mean)...\n",
      "2025-04-25 18:18:48,917 - INFO - Fitting RF for feature selection...\n",
      "2025-04-25 18:18:49,117 - INFO - Feat selection removed 61 features. Selected 39.\n",
      "2025-04-25 18:18:49,117 - INFO - Selection applied to train/val/test partitions.\n",
      "2025-04-25 18:18:49,117 - INFO - Data shapes post-scaling/selection: Train=(1024, 39), Val=(256, 39), Test=(854, 39)\n",
      "2025-04-25 18:18:49,117 - INFO - Number of features used in modeling: 39\n",
      "2025-04-25 18:18:49,117 - INFO - --- Optimizing models (Class Weights Enabled Where Applicable) ---\n",
      "2025-04-25 18:18:49,117 - INFO - Minimum CV score threshold for qualification: 0.72\n",
      "2025-04-25 18:18:49,117 - INFO - Optimization Order: ['logistic', 'knn', 'adaboost', 'randomforest', 'extratrees', 'gradientboosting', 'mlp', 'keras_mlp', 'catboost', 'xgboost', 'lightgbm']\n",
      "2025-04-25 18:18:49,117 - INFO - --- Optimizing logistic (30 trials) ---\n",
      "2025-04-25 18:18:49,117 - INFO - Starting logistic optimization (30 trials)...\n",
      "2025-04-25 18:18:49,117 - INFO - Optuna timeout for logistic: 3600s.\n",
      "[I 2025-04-25 18:18:49,596] A new study created in RDB with name: logistic_opt_20250425_181848\n",
      "2025-04-25 18:18:49,596 - INFO - Setting Optuna timeout 3600s.\n",
      "[I 2025-04-25 18:18:49,680] Trial 0 finished with value: 0.643558106169297 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l2', 'C': 0.0017594931359200387, 'max_iter': 896}. Best is trial 0 with value: 0.643558106169297.\n",
      "[I 2025-04-25 18:18:49,794] Trial 1 finished with value: 0.6542993782879004 and parameters: {'class_weight': {'0': 2, '1': 1.0, '2': 1.7}, 'penalty': 'l2', 'C': 2.9944181284754956, 'max_iter': 412}. Best is trial 1 with value: 0.6542993782879004.\n",
      "[I 2025-04-25 18:18:50,043] Trial 2 finished with value: 0.6572262075561931 and parameters: {'class_weight': {'0': 1.2, '1': 1.0, '2': 1.3}, 'penalty': 'l2', 'C': 19.574891905252308, 'max_iter': 976}. Best is trial 2 with value: 0.6572262075561931.\n",
      "[I 2025-04-25 18:18:50,379] Trial 3 finished with value: 0.6562458153993304 and parameters: {'class_weight': {'0': 2, '1': 1.0, '2': 1.7}, 'penalty': 'l2', 'C': 809.728879745486, 'max_iter': 867}. Best is trial 2 with value: 0.6572262075561931.\n",
      "[I 2025-04-25 18:19:24,608] Trial 4 finished with value: 0.6630846484935436 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 67.16911249051194, 'max_iter': 778}. Best is trial 4 with value: 0.6630846484935436.\n",
      "[I 2025-04-25 18:19:26,612] Trial 5 finished with value: 0.6543041606886656 and parameters: {'class_weight': {'0': 2, '1': 1.0, '2': 1.7}, 'penalty': 'l1', 'C': 10.171815108368515, 'max_iter': 348}. Best is trial 4 with value: 0.6630846484935436.\n",
      "[I 2025-04-25 18:19:26,690] Trial 6 finished with value: 0.658206599713056 and parameters: {'class_weight': {'0': 1.2, '1': 1.0, '2': 1.3}, 'penalty': 'l2', 'C': 0.005426184749409387, 'max_iter': 413}. Best is trial 4 with value: 0.6630846484935436.\n",
      "[I 2025-04-25 18:19:26,958] Trial 7 finished with value: 0.660157819225251 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l2', 'C': 219.9601571839628, 'max_iter': 447}. Best is trial 4 with value: 0.6630846484935436.\n",
      "[I 2025-04-25 18:19:27,057] Trial 8 finished with value: 0.653328550932568 and parameters: {'class_weight': {'0': 2, '1': 1.0, '2': 1.7}, 'penalty': 'l2', 'C': 0.4618892138385496, 'max_iter': 441}. Best is trial 4 with value: 0.6630846484935436.\n",
      "[I 2025-04-25 18:19:27,133] Trial 9 finished with value: 0.6015686274509804 and parameters: {'class_weight': {'0': 2, '1': 1.0, '2': 1.7}, 'penalty': 'l1', 'C': 0.00842472604322131, 'max_iter': 963}. Best is trial 4 with value: 0.6630846484935436.\n",
      "[I 2025-04-25 18:19:27,292] Trial 10 finished with value: 0.6552941176470588 and parameters: {'class_weight': 'balanced', 'penalty': 'l1', 'C': 0.2190410169805095, 'max_iter': 682}. Best is trial 4 with value: 0.6630846484935436.\n",
      "[I 2025-04-25 18:20:01,987] Trial 11 finished with value: 0.6640650406504064 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 851.5550680767052, 'max_iter': 166}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:20:23,634] Trial 12 finished with value: 0.6640602582496412 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 95.29687664964358, 'max_iter': 118}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:20:47,795] Trial 13 finished with value: 0.6611334289813485 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 573.9152254575578, 'max_iter': 108}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:21:06,877] Trial 14 finished with value: 0.6630846484935436 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 70.16946884234923, 'max_iter': 118}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:21:06,995] Trial 15 finished with value: 0.6543089430894309 and parameters: {'class_weight': 'balanced', 'penalty': 'l1', 'C': 0.05919606379478235, 'max_iter': 242}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:21:45,689] Trial 16 finished with value: 0.6601530368244858 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 89.31374794166919, 'max_iter': 247}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:21:47,055] Trial 17 finished with value: 0.6621090387374461 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 4.7778548892456, 'max_iter': 595}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:21:54,237] Trial 18 finished with value: 0.6621090387374461 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 25.680769545159908, 'max_iter': 234}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:22:30,671] Trial 19 finished with value: 0.6601673840267814 and parameters: {'class_weight': 'balanced', 'penalty': 'l1', 'C': 998.2400020077512, 'max_iter': 179}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:23:29,382] Trial 20 finished with value: 0.6611238641798182 and parameters: {'class_weight': {'0': 1.2, '1': 1.0, '2': 1.3}, 'penalty': 'l1', 'C': 228.774065962104, 'max_iter': 320}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:24:05,402] Trial 21 finished with value: 0.6630846484935436 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 65.3100974094665, 'max_iter': 745}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:24:57,688] Trial 22 finished with value: 0.6640506934481109 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 164.10106969394536, 'max_iter': 563}. Best is trial 11 with value: 0.6640650406504064.\n",
      "[I 2025-04-25 18:25:54,480] Trial 23 finished with value: 0.6669775227164035 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 280.64049788313935, 'max_iter': 555}. Best is trial 23 with value: 0.6669775227164035.\n",
      "[I 2025-04-25 18:26:53,162] Trial 24 finished with value: 0.6650263032042084 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 418.46571665255556, 'max_iter': 665}. Best is trial 23 with value: 0.6669775227164035.\n",
      "[I 2025-04-25 18:27:54,956] Trial 25 finished with value: 0.6650263032042084 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 411.62672655362553, 'max_iter': 641}. Best is trial 23 with value: 0.6669775227164035.\n",
      "[I 2025-04-25 18:27:56,103] Trial 26 finished with value: 0.6621090387374461 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 2.1733670008805452, 'max_iter': 646}. Best is trial 23 with value: 0.6669775227164035.\n",
      "[I 2025-04-25 18:28:53,881] Trial 27 finished with value: 0.6659971305595408 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 362.03805067090866, 'max_iter': 511}. Best is trial 23 with value: 0.6669775227164035.\n",
      "[I 2025-04-25 18:28:59,375] Trial 28 finished with value: 0.6552702056432328 and parameters: {'class_weight': {'0': 1.2, '1': 1.0, '2': 1.3}, 'penalty': 'l1', 'C': 22.04591618742186, 'max_iter': 495}. Best is trial 23 with value: 0.6669775227164035.\n",
      "[I 2025-04-25 18:29:58,273] Trial 29 finished with value: 0.666001912960306 and parameters: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 288.6359243546928, 'max_iter': 739}. Best is trial 23 with value: 0.6669775227164035.\n",
      "2025-04-25 18:29:58,274 - INFO - Opt complete logistic. Best CV score: 0.66698. Best params: {'class_weight': {'0': 1.4, '1': 1.0, '2': 1.4}, 'penalty': 'l1', 'C': 280.64049788313935, 'max_iter': 555}\n",
      "2025-04-25 18:29:58,274 - INFO - Saved Optuna summary: optuna_trials/logistic_study_summary_20250425_181848.txt\n",
      "2025-04-25 18:29:58,274 - INFO - Instantiating final logistic model...\n",
      "2025-04-25 18:29:58,274 - INFO - Converted LogReg class_weight keys to int: {0: 1.4, 1: 1.0, 2: 1.4}\n",
      "2025-04-25 18:29:58,282 - INFO - LogReg final model using class_weight={'0': 1.4, '1': 1.0, '2': 1.4}\n",
      "2025-04-25 18:29:58,282 - INFO - Fitting final logistic model...\n",
      "2025-04-25 18:29:58,285 - ERROR - Error during final fit for logistic: The classes, [0, 1, 2], are not in class_weight\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_3184\\2112995697.py\", line 696, in optimize_model\n",
      "    final_model.fit(X, y)  # Fit standard models\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1276, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "                                                ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1193, in _fit_liblinear\n",
      "    class_weight_ = compute_class_weight(class_weight, classes=classes_, y=y)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 189, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\utils\\class_weight.py\", line 87, in compute_class_weight\n",
      "    raise ValueError(\n",
      "ValueError: The classes, [0, 1, 2], are not in class_weight\n",
      "2025-04-25 18:29:58,285 - INFO - Qualification Check for logistic: final_model exists? False, best_cv_score=0.6669775, threshold=0.72, comparison result: False\n",
      "2025-04-25 18:29:58,285 - INFO - --- NOT QUALIFIED: logistic (CV Score: 0.66698  - Final model fit/save failed) ---\n",
      "2025-04-25 18:29:58,285 - INFO - --- Optimizing knn (25 trials) ---\n",
      "2025-04-25 18:29:58,285 - INFO - Starting knn optimization (25 trials)...\n",
      "2025-04-25 18:29:58,285 - INFO - Optuna timeout for knn: 3600s.\n",
      "[I 2025-04-25 18:29:58,487] A new study created in RDB with name: knn_opt_20250425_181848\n",
      "2025-04-25 18:29:58,495 - INFO - Setting Optuna timeout 3600s.\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[I 2025-04-25 18:29:58,860] Trial 0 finished with value: 0.6835915829746533 and parameters: {'metric': 'minkowski', 'n_neighbors': 11, 'weights': 'distance', 'leaf_size': 32, 'algorithm': 'auto', 'p': 2}. Best is trial 0 with value: 0.6835915829746533.\n",
      "[I 2025-04-25 18:29:59,037] Trial 1 finished with value: 0.6835915829746533 and parameters: {'metric': 'minkowski', 'n_neighbors': 11, 'weights': 'distance', 'leaf_size': 27, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 0 with value: 0.6835915829746533.\n",
      "[I 2025-04-25 18:29:59,285] Trial 2 finished with value: 0.6826398852223816 and parameters: {'metric': 'minkowski', 'n_neighbors': 5, 'weights': 'distance', 'leaf_size': 39, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 0 with value: 0.6835915829746533.\n",
      "[I 2025-04-25 18:29:59,529] Trial 3 finished with value: 0.689454806312769 and parameters: {'metric': 'manhattan', 'n_neighbors': 17, 'weights': 'distance', 'leaf_size': 37, 'algorithm': 'auto'}. Best is trial 3 with value: 0.689454806312769.\n",
      "[I 2025-04-25 18:29:59,696] Trial 4 finished with value: 0.6767670970827355 and parameters: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance', 'leaf_size': 33, 'algorithm': 'auto'}. Best is trial 3 with value: 0.689454806312769.\n",
      "[I 2025-04-25 18:29:59,875] Trial 5 finished with value: 0.6933763749402201 and parameters: {'metric': 'manhattan', 'n_neighbors': 27, 'weights': 'distance', 'leaf_size': 27, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:00,069] Trial 6 finished with value: 0.680674318507891 and parameters: {'metric': 'manhattan', 'n_neighbors': 33, 'weights': 'distance', 'leaf_size': 33, 'algorithm': 'ball_tree'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:00,377] Trial 7 finished with value: 0.6845624103299857 and parameters: {'metric': 'minkowski', 'n_neighbors': 15, 'weights': 'distance', 'leaf_size': 30, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:00,661] Trial 8 finished with value: 0.6767670970827355 and parameters: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance', 'leaf_size': 31, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:00,842] Trial 9 finished with value: 0.6845624103299857 and parameters: {'metric': 'manhattan', 'n_neighbors': 15, 'weights': 'distance', 'leaf_size': 21, 'algorithm': 'kd_tree'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:01,066] Trial 10 finished with value: 0.6933763749402201 and parameters: {'metric': 'manhattan', 'n_neighbors': 27, 'weights': 'distance', 'leaf_size': 23, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:01,257] Trial 11 finished with value: 0.6933763749402201 and parameters: {'metric': 'manhattan', 'n_neighbors': 27, 'weights': 'distance', 'leaf_size': 24, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:01,434] Trial 12 finished with value: 0.6816594930655189 and parameters: {'metric': 'manhattan', 'n_neighbors': 25, 'weights': 'distance', 'leaf_size': 20, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:01,616] Trial 13 finished with value: 0.6816594930655189 and parameters: {'metric': 'manhattan', 'n_neighbors': 25, 'weights': 'distance', 'leaf_size': 25, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:01,794] Trial 14 finished with value: 0.680674318507891 and parameters: {'metric': 'manhattan', 'n_neighbors': 33, 'weights': 'distance', 'leaf_size': 27, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:02,013] Trial 15 finished with value: 0.6894643711142994 and parameters: {'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'distance', 'leaf_size': 24, 'algorithm': 'ball_tree'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:02,328] Trial 16 finished with value: 0.6914155906264945 and parameters: {'metric': 'manhattan', 'n_neighbors': 21, 'weights': 'distance', 'leaf_size': 28, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:02,505] Trial 17 finished with value: 0.6728646580583453 and parameters: {'metric': 'manhattan', 'n_neighbors': 35, 'weights': 'distance', 'leaf_size': 22, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:02,751] Trial 18 finished with value: 0.6914060258249641 and parameters: {'metric': 'minkowski', 'n_neighbors': 21, 'weights': 'distance', 'leaf_size': 23, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:03,135] Trial 19 finished with value: 0.6894643711142994 and parameters: {'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'distance', 'leaf_size': 26, 'algorithm': 'ball_tree'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:03,359] Trial 20 finished with value: 0.6884983261597323 and parameters: {'metric': 'manhattan', 'n_neighbors': 23, 'weights': 'distance', 'leaf_size': 29, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:03,539] Trial 21 finished with value: 0.6894643711142994 and parameters: {'metric': 'manhattan', 'n_neighbors': 29, 'weights': 'distance', 'leaf_size': 24, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:03,719] Trial 22 finished with value: 0.6933763749402201 and parameters: {'metric': 'manhattan', 'n_neighbors': 27, 'weights': 'distance', 'leaf_size': 25, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:03,895] Trial 23 finished with value: 0.6884839789574366 and parameters: {'metric': 'manhattan', 'n_neighbors': 31, 'weights': 'distance', 'leaf_size': 22, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "[I 2025-04-25 18:30:04,072] Trial 24 finished with value: 0.6816594930655189 and parameters: {'metric': 'manhattan', 'n_neighbors': 25, 'weights': 'distance', 'leaf_size': 23, 'algorithm': 'auto'}. Best is trial 5 with value: 0.6933763749402201.\n",
      "2025-04-25 18:30:04,080 - INFO - Opt complete knn. Best CV score: 0.69338. Best params: {'metric': 'manhattan', 'n_neighbors': 27, 'weights': 'distance', 'leaf_size': 27, 'algorithm': 'auto'}\n",
      "2025-04-25 18:30:04,082 - INFO - Saved Optuna summary: optuna_trials/knn_study_summary_20250425_181848.txt\n",
      "2025-04-25 18:30:04,084 - INFO - Instantiating final knn model...\n",
      "2025-04-25 18:30:04,084 - INFO - KNN final model - balancing requires SMOTE or different approach (not applied in final fit here).\n",
      "2025-04-25 18:30:04,084 - INFO - Fitting final knn model...\n",
      "2025-04-25 18:30:04,086 - INFO - Final knn fitted in 0.00s.\n",
      "2025-04-25 18:30:04,086 - INFO - Saving final knn model...\n",
      "2025-04-25 18:30:04,089 - INFO - Saved final knn via joblib: models/knn_20250425_181848.joblib\n",
      "2025-04-25 18:30:04,090 - INFO - Saving importance knn...\n",
      "2025-04-25 18:30:04,090 - INFO - Model knn (KNeighborsClassifier) lacks standard importance attributes (feature_importances_, coef_, relevant estimator_).\n",
      "2025-04-25 18:30:04,091 - INFO - Qualification Check for knn: final_model exists? True, best_cv_score=0.6933764, threshold=0.72, comparison result: False\n",
      "2025-04-25 18:30:04,091 - INFO - --- NOT QUALIFIED: knn (CV Score: 0.69338 ) ---\n",
      "2025-04-25 18:30:04,092 - INFO - --- Optimizing adaboost (30 trials) ---\n",
      "2025-04-25 18:30:04,092 - INFO - Starting adaboost optimization (30 trials)...\n",
      "2025-04-25 18:30:04,093 - INFO - Optuna timeout for adaboost: 3600s.\n",
      "[I 2025-04-25 18:30:04,273] A new study created in RDB with name: adaboost_opt_20250425_181848\n",
      "2025-04-25 18:30:04,284 - INFO - Setting Optuna timeout 3600s.\n",
      "[I 2025-04-25 18:30:16,196] Trial 0 finished with value: 0.6943185078909613 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': 'balanced', 'n_estimators': 300, 'learning_rate': 1.1188616583817885}. Best is trial 0 with value: 0.6943185078909613.\n",
      "[I 2025-04-25 18:30:17,224] Trial 1 finished with value: 0.6328072692491631 and parameters: {'base_estimator_max_depth': 2, 'class_weight_choice': 'balanced', 'n_estimators': 850, 'learning_rate': 0.8980116575240192}. Best is trial 0 with value: 0.6943185078909613.\n",
      "[I 2025-04-25 18:30:21,206] Trial 2 finished with value: 0.6826303204208513 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 100, 'learning_rate': 0.07474481326837314}. Best is trial 0 with value: 0.6943185078909613.\n",
      "[I 2025-04-25 18:30:21,665] Trial 3 finished with value: 0.6279387852702056 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': 'balanced', 'n_estimators': 50, 'learning_rate': 1.131965122047013}. Best is trial 0 with value: 0.6943185078909613.\n",
      "[I 2025-04-25 18:30:36,552] Trial 4 finished with value: 0.7226398852223817 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': 'balanced', 'n_estimators': 300, 'learning_rate': 0.8758299467250618}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:30:37,080] Trial 5 finished with value: 0.6435676709708273 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 100, 'learning_rate': 0.9764570426054472}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:30:37,911] Trial 6 finished with value: 0.6640985174557628 and parameters: {'base_estimator_max_depth': 2, 'class_weight_choice': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 950, 'learning_rate': 0.8870757254024125}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:30:40,228] Trial 7 finished with value: 0.6562458153993305 and parameters: {'base_estimator_max_depth': 2, 'class_weight_choice': 'balanced', 'n_estimators': 150, 'learning_rate': 0.5294216401080507}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:31:09,313] Trial 8 finished with value: 0.7148445719751315 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 600, 'learning_rate': 0.580658998017509}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:31:12,886] Trial 9 finished with value: 0.6123098995695839 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 300, 'learning_rate': 0.14151131671313544}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:31:28,658] Trial 10 finished with value: 0.6943280726924916 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 2, '1': 1.0, '2': 1.9}, 'n_estimators': 550, 'learning_rate': 0.26801986170166037}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:31:38,176] Trial 11 finished with value: 0.7099569583931133 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 600, 'learning_rate': 0.4514919506631192}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:31:41,346] Trial 12 finished with value: 0.6943089430894308 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 2, '1': 1.0, '2': 1.9}, 'n_estimators': 700, 'learning_rate': 0.34456970684369265}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:31:44,612] Trial 13 finished with value: 0.6953084648493544 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 400, 'learning_rate': 0.5742789774327092}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:31:53,236] Trial 14 finished with value: 0.7021425155428025 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': 'balanced', 'n_estimators': 400, 'learning_rate': 0.18333818697983295}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:32:22,722] Trial 15 finished with value: 0.7060401721664276 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 700, 'learning_rate': 1.4782382837053765}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:32:34,843] Trial 16 finished with value: 0.6777522716403634 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 2, '1': 1.0, '2': 1.9}, 'n_estimators': 450, 'learning_rate': 0.6519502158306447}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:32:39,668] Trial 17 finished with value: 0.6924151123864181 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 250, 'learning_rate': 0.32333685463192363}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:32:50,616] Trial 18 finished with value: 0.7001960784313725 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 700, 'learning_rate': 0.2139854257716722}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:33:11,288] Trial 19 finished with value: 0.7060832137733142 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': 'balanced', 'n_estimators': 800, 'learning_rate': 0.060646524461208245}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:33:19,041] Trial 20 finished with value: 0.6933572453371593 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 500, 'learning_rate': 0.730978582877072}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:33:27,089] Trial 21 finished with value: 0.7089909134385461 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 600, 'learning_rate': 0.4381229149103797}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:33:30,306] Trial 22 finished with value: 0.7021472979435677 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 600, 'learning_rate': 0.43656491272223646}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:34:02,978] Trial 23 finished with value: 0.703094213295074 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 550, 'learning_rate': 1.4976593341540754}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:34:07,581] Trial 24 finished with value: 0.6943328550932568 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 800, 'learning_rate': 0.4814910847946126}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:34:09,150] Trial 25 finished with value: 0.6601482544237206 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 650, 'learning_rate': 0.7371708595873716}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:34:15,751] Trial 26 finished with value: 0.7080105212816834 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': 'balanced', 'n_estimators': 400, 'learning_rate': 0.346446780293405}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:34:21,417] Trial 27 finished with value: 0.6914060258249641 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 2, '1': 1.0, '2': 1.9}, 'n_estimators': 250, 'learning_rate': 0.11769528133217552}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:34:40,313] Trial 28 finished with value: 0.7158058345289335 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': 'balanced', 'n_estimators': 500, 'learning_rate': 0.6553801654153697}. Best is trial 4 with value: 0.7226398852223817.\n",
      "[I 2025-04-25 18:34:42,103] Trial 29 finished with value: 0.6454710664753707 and parameters: {'base_estimator_max_depth': 2, 'class_weight_choice': 'balanced', 'n_estimators': 300, 'learning_rate': 1.2672897215325811}. Best is trial 4 with value: 0.7226398852223817.\n",
      "2025-04-25 18:34:42,122 - INFO - Opt complete adaboost. Best CV score: 0.72264. Best params: {'base_estimator_max_depth': 4, 'class_weight_choice': 'balanced', 'n_estimators': 300, 'learning_rate': 0.8758299467250618}\n",
      "2025-04-25 18:34:42,123 - INFO - Saved Optuna summary: optuna_trials/adaboost_study_summary_20250425_181848.txt\n",
      "2025-04-25 18:34:42,124 - INFO - Instantiating final adaboost model...\n",
      "2025-04-25 18:34:42,124 - INFO - Reconstruct AdaBoost DT(max_depth=4, class_weight=balanced) using SAMME\n",
      "2025-04-25 18:34:42,125 - INFO - Fitting final adaboost model...\n",
      "2025-04-25 18:34:45,612 - INFO - Final adaboost fitted in 3.49s.\n",
      "2025-04-25 18:34:45,612 - INFO - Saving final adaboost model...\n",
      "2025-04-25 18:34:45,667 - INFO - Saved final adaboost via joblib: models/adaboost_20250425_181848.joblib\n",
      "2025-04-25 18:34:45,668 - INFO - Attempting calibration for adaboost...\n",
      "2025-04-25 18:34:53,089 - INFO - Saved calibrated model: calibrated_models/adaboost_calibrated_20250425_181848.joblib\n",
      "2025-04-25 18:34:53,090 - INFO - Saving importance adaboost...\n",
      "2025-04-25 18:34:53,466 - INFO - Saved importance plot: plots\\adaboost_feature_importance_20250425_181848.png\n",
      "2025-04-25 18:34:53,469 - INFO - Saved importance csv: results\\adaboost_feature_importance_20250425_181848.csv\n",
      "2025-04-25 18:34:53,470 - INFO - Qualification Check for adaboost: final_model exists? True, best_cv_score=0.7226399, threshold=0.72, comparison result: True\n",
      "2025-04-25 18:34:53,471 - INFO - +++ QUALIFIED: adaboost (CV Score: 0.72264)\n",
      "2025-04-25 18:34:53,472 - INFO - Evaluating model 'adaboost_qualified_holdout_eval'...\n",
      "2025-04-25 18:34:53,523 - INFO - Evaluation Results for 'adaboost_qualified_holdout_eval':\n",
      "2025-04-25 18:34:53,524 - INFO -   Accuracy: 0.77344\n",
      "2025-04-25 18:34:53,530 - INFO - Saved evaluation summary: results\\adaboost_qualified_holdout_eval_evaluation_20250425_181848.txt\n",
      "2025-04-25 18:34:53,621 - INFO - Saved confusion matrix plot: plots\\adaboost_qualified_holdout_eval_confusion_matrix_20250425_181848.png\n",
      "2025-04-25 18:34:53,622 - INFO - Hold-out Acc (adaboost): 0.77344\n",
      "2025-04-25 18:34:53,623 - INFO - --- Generating individual predictions for adaboost ---\n",
      "2025-04-25 18:34:53,624 - INFO - Generating test predictions using adaboost_qual_individual_pred...\n",
      "2025-04-25 18:34:53,663 - INFO - Saved submission: submissions\\solution_adaboost_qual_individual_pred_20250425_181848.csv\n",
      "2025-04-25 18:34:53,665 - INFO - Test prediction distribution for 'adaboost_qual_individual_pred': {'Medium': 317, 'High': 273, 'Low': 264}\n",
      "2025-04-25 18:34:53,665 - INFO - Saved test prediction summary: results\\adaboost_qual_individual_pred_test_prediction_summary_20250425_181848.txt\n",
      "2025-04-25 18:34:53,666 - INFO - Individual prediction file saved for adaboost.\n",
      "2025-04-25 18:34:53,666 - INFO - --- Optimizing randomforest (40 trials) ---\n",
      "2025-04-25 18:34:53,666 - INFO - Starting randomforest optimization (40 trials)...\n",
      "2025-04-25 18:34:53,668 - INFO - Optuna timeout for randomforest: 7200s.\n",
      "[I 2025-04-25 18:34:53,848] A new study created in RDB with name: randomforest_opt_20250425_181848\n",
      "2025-04-25 18:34:53,851 - INFO - Setting Optuna timeout 7200s.\n",
      "[I 2025-04-25 18:34:57,391] Trial 0 finished with value: 0.6659923481587757 and parameters: {'class_weight': {'0': 1.6, '1': 1.0, '2': 1.3}, 'n_estimators': 600, 'max_depth': 8, 'min_samples_split': 14, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 0 with value: 0.6659923481587757.\n",
      "[I 2025-04-25 18:35:05,633] Trial 1 finished with value: 0.6757580105212817 and parameters: {'class_weight': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1000, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 15, 'max_features': 0.7, 'criterion': 'entropy'}. Best is trial 1 with value: 0.6757580105212817.\n",
      "[I 2025-04-25 18:35:51,486] Trial 2 finished with value: 0.7158058345289335 and parameters: {'class_weight': {'0': 1.6, '1': 1.0, '2': 1.3}, 'n_estimators': 1300, 'max_depth': 40, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.7, 'criterion': 'entropy'}. Best is trial 2 with value: 0.7158058345289335.\n",
      "[I 2025-04-25 18:36:24,573] Trial 3 finished with value: 0.6601434720229555 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 1000, 'max_depth': 34, 'min_samples_split': 6, 'min_samples_leaf': 14, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 2 with value: 0.7158058345289335.\n",
      "[I 2025-04-25 18:36:53,309] Trial 4 finished with value: 0.6962792922046868 and parameters: {'class_weight': {'0': 1.6, '1': 1.0, '2': 1.3}, 'n_estimators': 1200, 'max_depth': 42, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 2 with value: 0.7158058345289335.\n",
      "[I 2025-04-25 18:37:21,525] Trial 5 finished with value: 0.6581874701099952 and parameters: {'class_weight': 'balanced', 'n_estimators': 1200, 'max_depth': 14, 'min_samples_split': 16, 'min_samples_leaf': 14, 'max_features': 'sqrt', 'criterion': 'entropy'}. Best is trial 2 with value: 0.7158058345289335.\n",
      "[I 2025-04-25 18:37:56,359] Trial 6 finished with value: 0.6943280726924916 and parameters: {'class_weight': {'0': 1.6, '1': 1.0, '2': 1.3}, 'n_estimators': 1100, 'max_depth': 18, 'min_samples_split': 16, 'min_samples_leaf': 10, 'max_features': 0.7, 'criterion': 'entropy'}. Best is trial 2 with value: 0.7158058345289335.\n",
      "[I 2025-04-25 18:38:34,503] Trial 7 finished with value: 0.7060401721664276 and parameters: {'class_weight': {'0': 1.6, '1': 1.0, '2': 1.3}, 'n_estimators': 1100, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 0.7, 'criterion': 'entropy'}. Best is trial 2 with value: 0.7158058345289335.\n",
      "[I 2025-04-25 18:38:39,774] Trial 8 finished with value: 0.6992061214729794 and parameters: {'class_weight': {'0': 1.6, '1': 1.0, '2': 1.3}, 'n_estimators': 200, 'max_depth': 38, 'min_samples_split': 16, 'min_samples_leaf': 2, 'max_features': 'log2', 'criterion': 'entropy'}. Best is trial 2 with value: 0.7158058345289335.\n",
      "[I 2025-04-25 18:38:51,127] Trial 9 finished with value: 0.6894500239120038 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 1000, 'max_depth': 16, 'min_samples_split': 15, 'min_samples_leaf': 13, 'max_features': 0.7, 'criterion': 'entropy'}. Best is trial 2 with value: 0.7158058345289335.\n",
      "[I 2025-04-25 18:38:58,395] Trial 10 finished with value: 0.6962649450023911 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1500, 'max_depth': 50, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 'log2', 'criterion': 'gini'}. Best is trial 2 with value: 0.7158058345289335.\n",
      "[I 2025-04-25 18:39:07,153] Trial 11 finished with value: 0.7138498326159732 and parameters: {'class_weight': {'0': 1.6, '1': 1.0, '2': 1.3}, 'n_estimators': 1400, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 2 with value: 0.7158058345289335.\n",
      "[I 2025-04-25 18:39:16,390] Trial 12 finished with value: 0.7187470109995218 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1500, 'max_depth': 22, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 12 with value: 0.7187470109995218.\n",
      "[I 2025-04-25 18:39:25,402] Trial 13 finished with value: 0.712883787661406 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1500, 'max_depth': 46, 'min_samples_split': 11, 'min_samples_leaf': 4, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 12 with value: 0.7187470109995218.\n",
      "[I 2025-04-25 18:39:29,730] Trial 14 finished with value: 0.7128933524629364 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 700, 'max_depth': 24, 'min_samples_split': 11, 'min_samples_leaf': 6, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 12 with value: 0.7187470109995218.\n",
      "[I 2025-04-25 18:39:37,655] Trial 15 finished with value: 0.7177857484457197 and parameters: {'class_weight': 'balanced', 'n_estimators': 1300, 'max_depth': 40, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 12 with value: 0.7187470109995218.\n",
      "[I 2025-04-25 18:39:41,621] Trial 16 finished with value: 0.7011669057867049 and parameters: {'class_weight': 'balanced', 'n_estimators': 800, 'max_depth': 24, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'gini'}. Best is trial 12 with value: 0.7187470109995218.\n",
      "[I 2025-04-25 18:39:44,147] Trial 17 finished with value: 0.7011716881874701 and parameters: {'class_weight': 'balanced', 'n_estimators': 400, 'max_depth': 34, 'min_samples_split': 19, 'min_samples_leaf': 9, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 12 with value: 0.7187470109995218.\n",
      "[I 2025-04-25 18:39:52,463] Trial 18 finished with value: 0.7070301291248207 and parameters: {'class_weight': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1400, 'max_depth': 20, 'min_samples_split': 13, 'min_samples_leaf': 4, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 12 with value: 0.7187470109995218.\n",
      "[I 2025-04-25 18:39:59,036] Trial 19 finished with value: 0.6953036824485892 and parameters: {'class_weight': 'balanced', 'n_estimators': 1300, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 'log2', 'criterion': 'gini'}. Best is trial 12 with value: 0.7187470109995218.\n",
      "[I 2025-04-25 18:40:09,046] Trial 20 finished with value: 0.7089813486370158 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1500, 'max_depth': 36, 'min_samples_split': 18, 'min_samples_leaf': 3, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 12 with value: 0.7187470109995218.\n",
      "[I 2025-04-25 18:40:18,782] Trial 21 finished with value: 0.7255762792922047 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1300, 'max_depth': 42, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:40:28,366] Trial 22 finished with value: 0.7206982305117169 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1300, 'max_depth': 44, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:40:38,163] Trial 23 finished with value: 0.7158201817312291 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1400, 'max_depth': 44, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:40:46,841] Trial 24 finished with value: 0.7197178383548541 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1200, 'max_depth': 48, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:40:53,013] Trial 25 finished with value: 0.7138498326159732 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 900, 'max_depth': 48, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:41:01,731] Trial 26 finished with value: 0.7167910090865615 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1200, 'max_depth': 50, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:41:09,328] Trial 27 finished with value: 0.7099521759923482 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1100, 'max_depth': 46, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:41:13,877] Trial 28 finished with value: 0.7128885700621712 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 900, 'max_depth': 44, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:41:16,542] Trial 29 finished with value: 0.6708656145384984 and parameters: {'class_weight': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 500, 'max_depth': 46, 'min_samples_split': 9, 'min_samples_leaf': 10, 'max_features': 'log2', 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:41:25,228] Trial 30 finished with value: 0.6953084648493544 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 1300, 'max_depth': 42, 'min_samples_split': 12, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:41:34,788] Trial 31 finished with value: 0.7197082735533238 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1400, 'max_depth': 28, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:41:45,222] Trial 32 finished with value: 0.7197082735533238 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1400, 'max_depth': 28, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:41:53,737] Trial 33 finished with value: 0.7158153993304639 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1200, 'max_depth': 28, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:42:03,094] Trial 34 finished with value: 0.7148445719751315 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1300, 'max_depth': 32, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:42:11,004] Trial 35 finished with value: 0.7197178383548541 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1100, 'max_depth': 38, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:42:17,951] Trial 36 finished with value: 0.7119177427068388 and parameters: {'class_weight': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1000, 'max_depth': 38, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:42:23,620] Trial 37 finished with value: 0.7119033955045433 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1100, 'max_depth': 42, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:42:33,371] Trial 38 finished with value: 0.689454806312769 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 1200, 'max_depth': 38, 'min_samples_split': 4, 'min_samples_leaf': 12, 'max_features': 0.7, 'criterion': 'gini'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "[I 2025-04-25 18:42:42,516] Trial 39 finished with value: 0.709947393591583 and parameters: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1100, 'max_depth': 42, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 0.7, 'criterion': 'entropy'}. Best is trial 21 with value: 0.7255762792922047.\n",
      "2025-04-25 18:42:42,523 - INFO - Opt complete randomforest. Best CV score: 0.72558. Best params: {'class_weight': {'0': 1.5, '1': 1.0, '2': 1.5}, 'n_estimators': 1300, 'max_depth': 42, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 0.7, 'criterion': 'gini'}\n",
      "2025-04-25 18:42:42,523 - INFO - Saved Optuna summary: optuna_trials/randomforest_study_summary_20250425_181848.txt\n",
      "2025-04-25 18:42:42,523 - INFO - Instantiating final randomforest model...\n",
      "2025-04-25 18:42:42,523 - INFO - Converted RF class_weight keys to int: {0: 1.5, 1: 1.0, 2: 1.5}\n",
      "2025-04-25 18:42:42,523 - INFO - RF final model using class_weight={'0': 1.5, '1': 1.0, '2': 1.5}\n",
      "2025-04-25 18:42:42,523 - INFO - Fitting final randomforest model...\n",
      "2025-04-25 18:42:42,523 - ERROR - Error during final fit for randomforest: The classes, [0, 1, 2], are not in class_weight\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\damod\\AppData\\Local\\Temp\\ipykernel_3184\\2112995697.py\", line 696, in optimize_model\n",
      "    final_model.fit(X, y)  # Fit standard models\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 419, in fit\n",
      "    y, expanded_class_weight = self._validate_y_class_weight(y)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 879, in _validate_y_class_weight\n",
      "    expanded_class_weight = compute_sample_weight(class_weight, y_original)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 189, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\utils\\class_weight.py\", line 206, in compute_sample_weight\n",
      "    weight_k = compute_class_weight(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 189, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\competition\\engineer salary\\advance\\superNova\\Lib\\site-packages\\sklearn\\utils\\class_weight.py\", line 87, in compute_class_weight\n",
      "    raise ValueError(\n",
      "ValueError: The classes, [0, 1, 2], are not in class_weight\n",
      "2025-04-25 18:42:42,531 - INFO - Qualification Check for randomforest: final_model exists? False, best_cv_score=0.7255763, threshold=0.72, comparison result: True\n",
      "2025-04-25 18:42:42,532 - INFO - --- NOT QUALIFIED: randomforest (CV Score: 0.72558  - Final model fit/save failed) ---\n",
      "2025-04-25 18:42:42,533 - INFO - --- Optimizing extratrees (40 trials) ---\n",
      "2025-04-25 18:42:42,533 - INFO - Starting extratrees optimization (40 trials)...\n",
      "2025-04-25 18:42:42,534 - INFO - Optuna timeout for extratrees: 7200s.\n",
      "[I 2025-04-25 18:42:42,709] A new study created in RDB with name: extratrees_opt_20250425_181848\n",
      "2025-04-25 18:42:42,723 - INFO - Setting Optuna timeout 7200s.\n",
      "[I 2025-04-25 18:42:46,693] Trial 0 finished with value: 0.6123194643711143 and parameters: {'class_weight': {'0': 1.9, '1': 1.0, '2': 1.7}, 'n_estimators': 900, 'max_depth': 44, 'min_samples_split': 16, 'min_samples_leaf': 13, 'max_features': 'log2', 'bootstrap': True, 'criterion': 'gini'}. Best is trial 0 with value: 0.6123194643711143.\n",
      "[I 2025-04-25 18:42:49,671] Trial 1 finished with value: 0.6211190817790531 and parameters: {'class_weight': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 800, 'max_depth': 40, 'min_samples_split': 20, 'min_samples_leaf': 14, 'max_features': 'log2', 'bootstrap': False, 'criterion': 'gini'}. Best is trial 1 with value: 0.6211190817790531.\n",
      "[I 2025-04-25 18:42:55,966] Trial 2 finished with value: 0.6659923481587757 and parameters: {'class_weight': {'0': 1.9, '1': 1.0, '2': 1.7}, 'n_estimators': 1400, 'max_depth': 24, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 2 with value: 0.6659923481587757.\n",
      "[I 2025-04-25 18:43:01,893] Trial 3 finished with value: 0.6640459110473458 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 900, 'max_depth': 22, 'min_samples_split': 15, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 2 with value: 0.6659923481587757.\n",
      "[I 2025-04-25 18:43:05,423] Trial 4 finished with value: 0.7060353897656624 and parameters: {'class_weight': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 800, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'entropy'}. Best is trial 4 with value: 0.7060353897656624.\n",
      "[I 2025-04-25 18:43:08,134] Trial 5 finished with value: 0.7187517934002869 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 400, 'max_depth': 24, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 5 with value: 0.7187517934002869.\n",
      "[I 2025-04-25 18:43:13,037] Trial 6 finished with value: 0.68454806312769 and parameters: {'class_weight': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 1300, 'max_depth': 24, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'gini'}. Best is trial 5 with value: 0.7187517934002869.\n",
      "[I 2025-04-25 18:43:17,501] Trial 7 finished with value: 0.7011669057867049 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 700, 'max_depth': 14, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 5 with value: 0.7187517934002869.\n",
      "[I 2025-04-25 18:43:19,808] Trial 8 finished with value: 0.610363462458154 and parameters: {'class_weight': {'0': 1.9, '1': 1.0, '2': 1.7}, 'n_estimators': 500, 'max_depth': 58, 'min_samples_split': 15, 'min_samples_leaf': 13, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 5 with value: 0.7187517934002869.\n",
      "[I 2025-04-25 18:43:21,824] Trial 9 finished with value: 0.6699091343854615 and parameters: {'class_weight': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 500, 'max_depth': 44, 'min_samples_split': 2, 'min_samples_leaf': 14, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'gini'}. Best is trial 5 with value: 0.7187517934002869.\n",
      "[I 2025-04-25 18:43:22,986] Trial 10 finished with value: 0.7119129603060736 and parameters: {'class_weight': 'balanced', 'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 5 with value: 0.7187517934002869.\n",
      "[I 2025-04-25 18:43:24,155] Trial 11 finished with value: 0.7070109995217599 and parameters: {'class_weight': 'balanced', 'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 5 with value: 0.7187517934002869.\n",
      "[I 2025-04-25 18:43:25,305] Trial 12 finished with value: 0.7216834050693448 and parameters: {'class_weight': 'balanced', 'n_estimators': 200, 'max_depth': 16, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:27,967] Trial 13 finished with value: 0.7207173601147776 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 400, 'max_depth': 32, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:29,589] Trial 14 finished with value: 0.6894404591104735 and parameters: {'class_weight': 'balanced', 'n_estimators': 300, 'max_depth': 34, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:35,030] Trial 15 finished with value: 0.7158106169296987 and parameters: {'class_weight': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 1100, 'max_depth': 54, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:38,920] Trial 16 finished with value: 0.6894404591104735 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 600, 'max_depth': 34, 'min_samples_split': 4, 'min_samples_leaf': 9, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:40,802] Trial 17 finished with value: 0.6786944045911046 and parameters: {'class_weight': 'balanced', 'n_estimators': 400, 'max_depth': 18, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:41,817] Trial 18 finished with value: 0.7031181252989 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 200, 'max_depth': 32, 'min_samples_split': 12, 'min_samples_leaf': 7, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'gini'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:47,006] Trial 19 finished with value: 0.7187470109995218 and parameters: {'class_weight': 'balanced', 'n_estimators': 1100, 'max_depth': 50, 'min_samples_split': 6, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:49,186] Trial 20 finished with value: 0.6279531324725012 and parameters: {'class_weight': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 400, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 11, 'max_features': 'log2', 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:51,900] Trial 21 finished with value: 0.7080057388809182 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 400, 'max_depth': 28, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:55,968] Trial 22 finished with value: 0.7197082735533238 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 600, 'max_depth': 20, 'min_samples_split': 11, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:43:59,962] Trial 23 finished with value: 0.7197178383548541 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 600, 'max_depth': 18, 'min_samples_split': 11, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:44:03,914] Trial 24 finished with value: 0.7060497369679579 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 600, 'max_depth': 14, 'min_samples_split': 14, 'min_samples_leaf': 6, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:44:05,491] Trial 25 finished with value: 0.7089765662362506 and parameters: {'class_weight': 'balanced', 'n_estimators': 300, 'max_depth': 28, 'min_samples_split': 13, 'min_samples_leaf': 3, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:44:06,817] Trial 26 finished with value: 0.7079961740793879 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 300, 'max_depth': 42, 'min_samples_split': 17, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': False, 'criterion': 'gini'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:44:10,231] Trial 27 finished with value: 0.7168005738880918 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 500, 'max_depth': 18, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 12 with value: 0.7216834050693448.\n",
      "[I 2025-04-25 18:44:13,675] Trial 28 finished with value: 0.722649450023912 and parameters: {'class_weight': 'balanced', 'n_estimators': 700, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 28 with value: 0.722649450023912.\n",
      "[I 2025-04-25 18:44:17,992] Trial 29 finished with value: 0.6728407460545193 and parameters: {'class_weight': 'balanced', 'n_estimators': 900, 'max_depth': 48, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'log2', 'bootstrap': True, 'criterion': 'gini'}. Best is trial 28 with value: 0.722649450023912.\n",
      "[I 2025-04-25 18:44:22,775] Trial 30 finished with value: 0.653323768531803 and parameters: {'class_weight': 'balanced', 'n_estimators': 1000, 'max_depth': 38, 'min_samples_split': 6, 'min_samples_leaf': 10, 'max_features': 'log2', 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 28 with value: 0.722649450023912.\n",
      "[I 2025-04-25 18:44:26,377] Trial 31 finished with value: 0.7148397895743663 and parameters: {'class_weight': {'0': 1.9, '1': 1.0, '2': 1.7}, 'n_estimators': 700, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 28 with value: 0.722649450023912.\n",
      "[I 2025-04-25 18:44:29,995] Trial 32 finished with value: 0.7119129603060736 and parameters: {'class_weight': 'balanced', 'n_estimators': 700, 'max_depth': 16, 'min_samples_split': 18, 'min_samples_leaf': 2, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 28 with value: 0.722649450023912.\n",
      "[I 2025-04-25 18:44:32,794] Trial 33 finished with value: 0.7021472979435677 and parameters: {'class_weight': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 500, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 28 with value: 0.722649450023912.\n",
      "[I 2025-04-25 18:44:34,467] Trial 34 finished with value: 0.6845671927307508 and parameters: {'class_weight': {'0': 1.9, '1': 1.0, '2': 1.7}, 'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 6, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'entropy'}. Best is trial 28 with value: 0.722649450023912.\n",
      "[I 2025-04-25 18:44:37,699] Trial 35 finished with value: 0.7079866092778575 and parameters: {'class_weight': 'balanced', 'n_estimators': 800, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': False, 'criterion': 'entropy'}. Best is trial 28 with value: 0.722649450023912.\n",
      "[I 2025-04-25 18:44:47,828] Trial 36 finished with value: 0.7255810616929699 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 1500, 'max_depth': 30, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'gini'}. Best is trial 36 with value: 0.7255810616929699.\n",
      "[I 2025-04-25 18:44:55,091] Trial 37 finished with value: 0.7089622190339551 and parameters: {'class_weight': {'0': 1.7, '1': 1.0, '2': 1.4}, 'n_estimators': 1500, 'max_depth': 38, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'criterion': 'gini'}. Best is trial 36 with value: 0.7255810616929699.\n",
      "[I 2025-04-25 18:45:00,155] Trial 38 finished with value: 0.6777235772357723 and parameters: {'class_weight': {'0': 1.2, '1': 1.0, '2': 1.3}, 'n_estimators': 1300, 'max_depth': 30, 'min_samples_split': 7, 'min_samples_leaf': 6, 'max_features': 'log2', 'bootstrap': False, 'criterion': 'gini'}. Best is trial 36 with value: 0.7255810616929699.\n",
      "[I 2025-04-25 18:45:08,242] Trial 39 finished with value: 0.7236154949784792 and parameters: {'class_weight': 'balanced_subsample', 'n_estimators': 1200, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'gini'}. Best is trial 36 with value: 0.7255810616929699.\n",
      "2025-04-25 18:45:08,259 - INFO - Opt complete extratrees. Best CV score: 0.72558. Best params: {'class_weight': 'balanced_subsample', 'n_estimators': 1500, 'max_depth': 30, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.7, 'bootstrap': True, 'criterion': 'gini'}\n",
      "2025-04-25 18:45:08,259 - INFO - Saved Optuna summary: optuna_trials/extratrees_study_summary_20250425_181848.txt\n",
      "2025-04-25 18:45:08,259 - INFO - Instantiating final extratrees model...\n",
      "2025-04-25 18:45:08,259 - INFO - ET final model using class_weight=balanced_subsample\n",
      "2025-04-25 18:45:08,272 - INFO - Fitting final extratrees model...\n",
      "2025-04-25 18:45:10,246 - INFO - Final extratrees fitted in 1.97s.\n",
      "2025-04-25 18:45:10,247 - INFO - Saving final extratrees model...\n",
      "2025-04-25 18:45:10,591 - INFO - Saved final extratrees via joblib: models/extratrees_20250425_181848.joblib\n",
      "2025-04-25 18:45:10,591 - INFO - Attempting calibration for extratrees...\n",
      "2025-04-25 18:45:16,883 - INFO - Saved calibrated model: calibrated_models/extratrees_calibrated_20250425_181848.joblib\n",
      "2025-04-25 18:45:16,883 - INFO - Saving importance extratrees...\n",
      "2025-04-25 18:45:17,339 - INFO - Saved importance plot: plots\\extratrees_feature_importance_20250425_181848.png\n",
      "2025-04-25 18:45:17,355 - INFO - Saved importance csv: results\\extratrees_feature_importance_20250425_181848.csv\n",
      "2025-04-25 18:45:17,356 - INFO - Qualification Check for extratrees: final_model exists? True, best_cv_score=0.7255811, threshold=0.72, comparison result: True\n",
      "2025-04-25 18:45:17,356 - INFO - +++ QUALIFIED: extratrees (CV Score: 0.72558)\n",
      "2025-04-25 18:45:17,356 - INFO - Evaluating model 'extratrees_qualified_holdout_eval'...\n",
      "2025-04-25 18:45:17,528 - INFO - Evaluation Results for 'extratrees_qualified_holdout_eval':\n",
      "2025-04-25 18:45:17,528 - INFO -   Accuracy: 0.75391\n",
      "2025-04-25 18:45:17,539 - INFO - Saved evaluation summary: results\\extratrees_qualified_holdout_eval_evaluation_20250425_181848.txt\n",
      "2025-04-25 18:45:17,630 - INFO - Saved confusion matrix plot: plots\\extratrees_qualified_holdout_eval_confusion_matrix_20250425_181848.png\n",
      "2025-04-25 18:45:17,631 - INFO - Hold-out Acc (extratrees): 0.75391\n",
      "2025-04-25 18:45:17,632 - INFO - --- Generating individual predictions for extratrees ---\n",
      "2025-04-25 18:45:17,633 - INFO - Generating test predictions using extratrees_qual_individual_pred...\n",
      "2025-04-25 18:45:17,808 - INFO - Saved submission: submissions\\solution_extratrees_qual_individual_pred_20250425_181848.csv\n",
      "2025-04-25 18:45:17,808 - INFO - Test prediction distribution for 'extratrees_qual_individual_pred': {'High': 348, 'Medium': 278, 'Low': 228}\n",
      "2025-04-25 18:45:17,808 - INFO - Saved test prediction summary: results\\extratrees_qual_individual_pred_test_prediction_summary_20250425_181848.txt\n",
      "2025-04-25 18:45:17,808 - INFO - Individual prediction file saved for extratrees.\n",
      "2025-04-25 18:45:17,808 - INFO - --- Optimizing gradientboosting (40 trials) ---\n",
      "2025-04-25 18:45:17,808 - INFO - Starting gradientboosting optimization (40 trials)...\n",
      "2025-04-25 18:45:17,808 - INFO - Optuna timeout for gradientboosting: 7200s.\n",
      "[I 2025-04-25 18:45:17,956] A new study created in RDB with name: gradientboosting_opt_20250425_181848\n",
      "2025-04-25 18:45:17,956 - INFO - Setting Optuna timeout 7200s.\n",
      "[I 2025-04-25 18:45:43,132] Trial 0 finished with value: 0.6738211382113821 and parameters: {'n_estimators': 1250, 'learning_rate': 0.021930218273189417, 'max_depth': 4, 'min_samples_split': 24, 'min_samples_leaf': 3, 'subsample': 0.9042549038871374, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.09062213000549564}. Best is trial 0 with value: 0.6738211382113821.\n",
      "[I 2025-04-25 18:46:14,199] Trial 1 finished with value: 0.694337637494022 and parameters: {'n_estimators': 1450, 'learning_rate': 0.013665235628681969, 'max_depth': 5, 'min_samples_split': 11, 'min_samples_leaf': 17, 'subsample': 0.6846599388255726, 'max_features': 'sqrt', 'min_weight_fraction_leaf': 0.06811472452601448}. Best is trial 1 with value: 0.694337637494022.\n",
      "[I 2025-04-25 18:46:27,778] Trial 2 finished with value: 0.7002056432329029 and parameters: {'n_estimators': 350, 'learning_rate': 0.015172666524905397, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 21, 'subsample': 0.88971872381073, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.0014427344836423384}. Best is trial 2 with value: 0.7002056432329029.\n",
      "[I 2025-04-25 18:46:36,728] Trial 3 finished with value: 0.7118986131037781 and parameters: {'n_estimators': 250, 'learning_rate': 0.023527081567692665, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 21, 'subsample': 0.8206803741089215, 'max_features': 'sqrt', 'min_weight_fraction_leaf': 0.015868264305860292}. Best is trial 3 with value: 0.7118986131037781.\n",
      "[I 2025-04-25 18:47:02,275] Trial 4 finished with value: 0.6865040650406504 and parameters: {'n_estimators': 1050, 'learning_rate': 0.10849900575634583, 'max_depth': 6, 'min_samples_split': 30, 'min_samples_leaf': 22, 'subsample': 0.6470418185922897, 'max_features': 'sqrt', 'min_weight_fraction_leaf': 0.07281815421208866}. Best is trial 3 with value: 0.7118986131037781.\n",
      "[I 2025-04-25 18:47:42,456] Trial 5 finished with value: 0.6943424198947872 and parameters: {'n_estimators': 1150, 'learning_rate': 0.03483957292804724, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 11, 'subsample': 0.8656086622940973, 'max_features': 'sqrt', 'min_weight_fraction_leaf': 0.0662586415020324}. Best is trial 3 with value: 0.7118986131037781.\n",
      "[I 2025-04-25 18:47:58,696] Trial 6 finished with value: 0.6874940219990435 and parameters: {'n_estimators': 650, 'learning_rate': 0.037964153098773615, 'max_depth': 5, 'min_samples_split': 21, 'min_samples_leaf': 3, 'subsample': 0.9292761613082734, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.045811056228872576}. Best is trial 3 with value: 0.7118986131037781.\n",
      "[I 2025-04-25 18:48:08,479] Trial 7 finished with value: 0.6738307030129125 and parameters: {'n_estimators': 550, 'learning_rate': 0.047532013808386495, 'max_depth': 3, 'min_samples_split': 14, 'min_samples_leaf': 17, 'subsample': 0.8238735313862788, 'max_features': 'sqrt', 'min_weight_fraction_leaf': 0.08199233397989357}. Best is trial 3 with value: 0.7118986131037781.\n",
      "[I 2025-04-25 18:48:35,406] Trial 8 finished with value: 0.6845671927307508 and parameters: {'n_estimators': 1350, 'learning_rate': 0.1059450483185733, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 8, 'subsample': 0.9111964524816756, 'max_features': 'sqrt', 'min_weight_fraction_leaf': 0.03438309458252954}. Best is trial 3 with value: 0.7118986131037781.\n",
      "[I 2025-04-25 18:48:41,837] Trial 9 finished with value: 0.6826111908177905 and parameters: {'n_estimators': 250, 'learning_rate': 0.07010593530137141, 'max_depth': 9, 'min_samples_split': 11, 'min_samples_leaf': 21, 'subsample': 0.8539468103546098, 'max_features': 'sqrt', 'min_weight_fraction_leaf': 0.061028766993459806}. Best is trial 3 with value: 0.7118986131037781.\n",
      "[I 2025-04-25 18:48:45,547] Trial 10 finished with value: 0.6659827833572454 and parameters: {'n_estimators': 150, 'learning_rate': 0.0104507125163409, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 25, 'subsample': 0.7456164410279152, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.011470267603249212}. Best is trial 3 with value: 0.7118986131037781.\n",
      "[I 2025-04-25 18:48:58,251] Trial 11 finished with value: 0.7177714012434242 and parameters: {'n_estimators': 450, 'learning_rate': 0.019103721715459036, 'max_depth': 8, 'min_samples_split': 17, 'min_samples_leaf': 18, 'subsample': 0.7659402272113013, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.0003922218702595553}. Best is trial 11 with value: 0.7177714012434242.\n",
      "[I 2025-04-25 18:49:10,369] Trial 12 finished with value: 0.7138737446197991 and parameters: {'n_estimators': 450, 'learning_rate': 0.023185355340433225, 'max_depth': 7, 'min_samples_split': 20, 'min_samples_leaf': 16, 'subsample': 0.7721994440071276, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.02058387809071578}. Best is trial 11 with value: 0.7177714012434242.\n",
      "[I 2025-04-25 18:49:28,703] Trial 13 finished with value: 0.7070540411286467 and parameters: {'n_estimators': 850, 'learning_rate': 0.1933442368613322, 'max_depth': 7, 'min_samples_split': 21, 'min_samples_leaf': 15, 'subsample': 0.7493268441158705, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.02638663015328427}. Best is trial 11 with value: 0.7177714012434242.\n",
      "[I 2025-04-25 18:50:11,852] Trial 14 finished with value: 0.7255954088952654 and parameters: {'n_estimators': 450, 'learning_rate': 0.024041112746865725, 'max_depth': 7, 'min_samples_split': 20, 'min_samples_leaf': 12, 'subsample': 0.7118098035302222, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.002921183733040013}. Best is trial 14 with value: 0.7255954088952654.\n",
      "[I 2025-04-25 18:51:57,871] Trial 15 finished with value: 0.7265758010521282 and parameters: {'n_estimators': 750, 'learning_rate': 0.017408433537102537, 'max_depth': 8, 'min_samples_split': 25, 'min_samples_leaf': 11, 'subsample': 0.6969520594471702, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.004955565373651158}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 18:53:29,348] Trial 16 finished with value: 0.694337637494022 and parameters: {'n_estimators': 850, 'learning_rate': 0.0307547559508183, 'max_depth': 8, 'min_samples_split': 26, 'min_samples_leaf': 11, 'subsample': 0.6025574267758228, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.038797201397970174}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 18:54:54,745] Trial 17 finished with value: 0.7216834050693448 and parameters: {'n_estimators': 750, 'learning_rate': 0.010904471253196024, 'max_depth': 6, 'min_samples_split': 27, 'min_samples_leaf': 11, 'subsample': 0.705904688825651, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.006994931326166374}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 18:57:00,822] Trial 18 finished with value: 0.7060640841702535 and parameters: {'n_estimators': 950, 'learning_rate': 0.051359931334631716, 'max_depth': 8, 'min_samples_split': 24, 'min_samples_leaf': 7, 'subsample': 0.7009842084122111, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.027388631374720397}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 18:58:01,110] Trial 19 finished with value: 0.6884744141559063 and parameters: {'n_estimators': 650, 'learning_rate': 0.016316410455285542, 'max_depth': 7, 'min_samples_split': 29, 'min_samples_leaf': 13, 'subsample': 0.652529044490115, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.05430653410751157}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 18:58:53,057] Trial 20 finished with value: 0.7236394069823051 and parameters: {'n_estimators': 550, 'learning_rate': 0.028560511780130072, 'max_depth': 5, 'min_samples_split': 19, 'min_samples_leaf': 7, 'subsample': 0.6010552158210866, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.019071961196247128}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 18:59:46,815] Trial 21 finished with value: 0.7207077953132472 and parameters: {'n_estimators': 550, 'learning_rate': 0.02823771602527227, 'max_depth': 5, 'min_samples_split': 19, 'min_samples_leaf': 7, 'subsample': 0.6015978807707272, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.011840716096818582}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:00:57,057] Trial 22 finished with value: 0.7168005738880918 and parameters: {'n_estimators': 650, 'learning_rate': 0.060385639848583104, 'max_depth': 6, 'min_samples_split': 24, 'min_samples_leaf': 9, 'subsample': 0.6535176069081345, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.024585346747750713}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:01:36,569] Trial 23 finished with value: 0.7167862266857963 and parameters: {'n_estimators': 450, 'learning_rate': 0.02843166446555208, 'max_depth': 4, 'min_samples_split': 17, 'min_samples_leaf': 5, 'subsample': 0.7248674646156601, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.005128652144908303}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:03:16,219] Trial 24 finished with value: 0.7265518890483023 and parameters: {'n_estimators': 750, 'learning_rate': 0.018550662473122346, 'max_depth': 8, 'min_samples_split': 22, 'min_samples_leaf': 13, 'subsample': 0.678553196063175, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.012759358523361769}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:04:00,513] Trial 25 finished with value: 0.7021425155428025 and parameters: {'n_estimators': 950, 'learning_rate': 0.0130077596179282, 'max_depth': 8, 'min_samples_split': 23, 'min_samples_leaf': 13, 'subsample': 0.6721372261336737, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.037293507668094215}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:05:00,765] Trial 26 finished with value: 0.7265662362505978 and parameters: {'n_estimators': 950, 'learning_rate': 0.01819204967622993, 'max_depth': 7, 'min_samples_split': 27, 'min_samples_leaf': 14, 'subsample': 0.8005583602961586, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.010576227924871144}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:07:40,308] Trial 27 finished with value: 0.7197369679579149 and parameters: {'n_estimators': 950, 'learning_rate': 0.01838082053648262, 'max_depth': 9, 'min_samples_split': 27, 'min_samples_leaf': 14, 'subsample': 0.7970100094924548, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.013754374722549291}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:09:20,505] Trial 28 finished with value: 0.6913964610234338 and parameters: {'n_estimators': 750, 'learning_rate': 0.012683426973501432, 'max_depth': 8, 'min_samples_split': 28, 'min_samples_leaf': 9, 'subsample': 0.7990596728170877, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.04619451515603682}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:10:57,810] Trial 29 finished with value: 0.6806551889048301 and parameters: {'n_estimators': 1150, 'learning_rate': 0.017971635432236413, 'max_depth': 7, 'min_samples_split': 25, 'min_samples_leaf': 19, 'subsample': 0.7332974847568883, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.09447660587004884}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:13:23,726] Trial 30 finished with value: 0.6982257293161167 and parameters: {'n_estimators': 1050, 'learning_rate': 0.020985022127822574, 'max_depth': 9, 'min_samples_split': 23, 'min_samples_leaf': 15, 'subsample': 0.6292316023292962, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.03161690404819624}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:15:15,873] Trial 31 finished with value: 0.7246197991391679 and parameters: {'n_estimators': 750, 'learning_rate': 0.022093393659941393, 'max_depth': 7, 'min_samples_split': 22, 'min_samples_leaf': 12, 'subsample': 0.6987738206072653, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.0074923884662262494}. Best is trial 15 with value: 0.7265758010521282.\n",
      "[I 2025-04-25 19:16:58,552] Trial 32 finished with value: 0.7285222381635581 and parameters: {'n_estimators': 850, 'learning_rate': 0.014942566334326376, 'max_depth': 6, 'min_samples_split': 26, 'min_samples_leaf': 10, 'subsample': 0.6792287742939209, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.008063212389026322}. Best is trial 32 with value: 0.7285222381635581.\n",
      "[I 2025-04-25 19:18:45,806] Trial 33 finished with value: 0.7255906264945002 and parameters: {'n_estimators': 850, 'learning_rate': 0.01622837547083232, 'max_depth': 6, 'min_samples_split': 26, 'min_samples_leaf': 9, 'subsample': 0.6358170481705551, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.019949683143266186}. Best is trial 32 with value: 0.7285222381635581.\n",
      "[I 2025-04-25 19:21:17,841] Trial 34 finished with value: 0.7255954088952654 and parameters: {'n_estimators': 1050, 'learning_rate': 0.013889994210631272, 'max_depth': 8, 'min_samples_split': 30, 'min_samples_leaf': 10, 'subsample': 0.667261124501695, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.01039502411834978}. Best is trial 32 with value: 0.7285222381635581.\n",
      "[I 2025-04-25 19:22:59,755] Trial 35 finished with value: 0.7285078909612626 and parameters: {'n_estimators': 1250, 'learning_rate': 0.01186480576122216, 'max_depth': 6, 'min_samples_split': 28, 'min_samples_leaf': 14, 'subsample': 0.6786334930946033, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.016809963580580598}. Best is trial 32 with value: 0.7285222381635581.\n",
      "[I 2025-04-25 19:25:39,869] Trial 36 finished with value: 0.726571018651363 and parameters: {'n_estimators': 1350, 'learning_rate': 0.010518424901590974, 'max_depth': 6, 'min_samples_split': 28, 'min_samples_leaf': 15, 'subsample': 0.7881171553842001, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.0003158302724569602}. Best is trial 32 with value: 0.7285222381635581.\n",
      "[I 2025-04-25 19:26:06,676] Trial 37 finished with value: 0.6992252510760402 and parameters: {'n_estimators': 1450, 'learning_rate': 0.010127867311432276, 'max_depth': 4, 'min_samples_split': 29, 'min_samples_leaf': 19, 'subsample': 0.6231004864426298, 'max_features': 'sqrt', 'min_weight_fraction_leaf': 6.933035901945781e-05}. Best is trial 32 with value: 0.7285222381635581.\n",
      "[I 2025-04-25 19:26:47,045] Trial 38 finished with value: 0.7197274031563845 and parameters: {'n_estimators': 1350, 'learning_rate': 0.012064177513084806, 'max_depth': 6, 'min_samples_split': 29, 'min_samples_leaf': 16, 'subsample': 0.6844020770110143, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.016143745977845965}. Best is trial 32 with value: 0.7285222381635581.\n",
      "[I 2025-04-25 19:27:12,083] Trial 39 finished with value: 0.6757675753228121 and parameters: {'n_estimators': 1250, 'learning_rate': 0.014466494162575635, 'max_depth': 5, 'min_samples_split': 25, 'min_samples_leaf': 10, 'subsample': 0.725703373337317, 'max_features': 'sqrt', 'min_weight_fraction_leaf': 0.08375018260390232}. Best is trial 32 with value: 0.7285222381635581.\n",
      "2025-04-25 19:27:12,102 - INFO - Opt complete gradientboosting. Best CV score: 0.72852. Best params: {'n_estimators': 850, 'learning_rate': 0.014942566334326376, 'max_depth': 6, 'min_samples_split': 26, 'min_samples_leaf': 10, 'subsample': 0.6792287742939209, 'max_features': 'log2', 'min_weight_fraction_leaf': 0.008063212389026322}\n",
      "2025-04-25 19:27:12,102 - INFO - Saved Optuna summary: optuna_trials/gradientboosting_study_summary_20250425_181848.txt\n",
      "2025-04-25 19:27:12,102 - INFO - Instantiating final gradientboosting model...\n",
      "2025-04-25 19:27:12,102 - INFO - GradientBoosting final model - applying sample_weight in fit\n",
      "2025-04-25 19:27:12,102 - INFO - Fitting final gradientboosting model...\n",
      "2025-04-25 19:27:12,102 - INFO - Fitting gradientboosting with additional fit parameters: ['sample_weight']\n",
      "2025-04-25 19:27:17,567 - INFO - Final gradientboosting fitted in 5.47s.\n",
      "2025-04-25 19:27:17,567 - INFO - Saving final gradientboosting model...\n",
      "2025-04-25 19:27:17,634 - INFO - Saved final gradientboosting via joblib: models/gradientboosting_20250425_181848.joblib\n",
      "2025-04-25 19:27:17,634 - INFO - Attempting calibration for gradientboosting...\n",
      "2025-04-25 19:27:28,130 - INFO - Saved calibrated model: calibrated_models/gradientboosting_calibrated_20250425_181848.joblib\n",
      "2025-04-25 19:27:28,130 - INFO - Saving importance gradientboosting...\n",
      "2025-04-25 19:27:28,530 - INFO - Saved importance plot: plots\\gradientboosting_feature_importance_20250425_181848.png\n",
      "2025-04-25 19:27:28,533 - INFO - Saved importance csv: results\\gradientboosting_feature_importance_20250425_181848.csv\n",
      "2025-04-25 19:27:28,535 - INFO - Qualification Check for gradientboosting: final_model exists? True, best_cv_score=0.7285222, threshold=0.72, comparison result: True\n",
      "2025-04-25 19:27:28,535 - INFO - +++ QUALIFIED: gradientboosting (CV Score: 0.72852)\n",
      "2025-04-25 19:27:28,536 - INFO - Evaluating model 'gradientboosting_qualified_holdout_eval'...\n",
      "2025-04-25 19:27:28,562 - INFO - Evaluation Results for 'gradientboosting_qualified_holdout_eval':\n",
      "2025-04-25 19:27:28,563 - INFO -   Accuracy: 0.75000\n",
      "2025-04-25 19:27:28,566 - INFO - Saved evaluation summary: results\\gradientboosting_qualified_holdout_eval_evaluation_20250425_181848.txt\n",
      "2025-04-25 19:27:28,665 - INFO - Saved confusion matrix plot: plots\\gradientboosting_qualified_holdout_eval_confusion_matrix_20250425_181848.png\n",
      "2025-04-25 19:27:28,665 - INFO - Hold-out Acc (gradientboosting): 0.75000\n",
      "2025-04-25 19:27:28,665 - INFO - --- Generating individual predictions for gradientboosting ---\n",
      "2025-04-25 19:27:28,665 - INFO - Generating test predictions using gradientboosting_qual_individual_pred...\n",
      "2025-04-25 19:27:28,764 - INFO - Saved submission: submissions\\solution_gradientboosting_qual_individual_pred_20250425_181848.csv\n",
      "2025-04-25 19:27:28,768 - INFO - Test prediction distribution for 'gradientboosting_qual_individual_pred': {'Medium': 347, 'High': 276, 'Low': 231}\n",
      "2025-04-25 19:27:28,772 - INFO - Saved test prediction summary: results\\gradientboosting_qual_individual_pred_test_prediction_summary_20250425_181848.txt\n",
      "2025-04-25 19:27:28,774 - INFO - Individual prediction file saved for gradientboosting.\n",
      "2025-04-25 19:27:28,775 - INFO - --- Optimizing mlp (25 trials) ---\n",
      "2025-04-25 19:27:28,775 - INFO - Starting mlp optimization (25 trials)...\n",
      "2025-04-25 19:27:28,782 - INFO - Optuna timeout for mlp: 7200s.\n",
      "[I 2025-04-25 19:27:29,182] A new study created in RDB with name: mlp_opt_20250425_181848\n",
      "2025-04-25 19:27:29,201 - INFO - Setting Optuna timeout 7200s.\n",
      "[I 2025-04-25 19:27:34,649] Trial 0 finished with value: 0.6005882352941176 and parameters: {'hidden_layer_sizes': [100], 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0008997137725854864, 'learning_rate_init': 0.00014313750944807479, 'max_iter': 616, 'batch_size': 64}. Best is trial 0 with value: 0.6005882352941176.\n",
      "[I 2025-04-25 19:27:48,124] Trial 1 finished with value: 0.660157819225251 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'relu', 'solver': 'adam', 'alpha': 1.2023396700058243e-05, 'learning_rate_init': 0.003915531097712559, 'max_iter': 1435, 'batch_size': 128}. Best is trial 1 with value: 0.660157819225251.\n",
      "[I 2025-04-25 19:27:57,291] Trial 2 finished with value: 0.6493830703012913 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0006468933311294528, 'learning_rate_init': 0.008645284347470656, 'max_iter': 1015, 'batch_size': 256}. Best is trial 1 with value: 0.660157819225251.\n",
      "[I 2025-04-25 19:28:02,040] Trial 3 finished with value: 0.6240124342419895 and parameters: {'hidden_layer_sizes': [100], 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0011302458384895653, 'learning_rate_init': 0.0002246543575688169, 'max_iter': 567, 'batch_size': 256}. Best is trial 1 with value: 0.660157819225251.\n",
      "[I 2025-04-25 19:28:16,042] Trial 4 finished with value: 0.6542802486848397 and parameters: {'hidden_layer_sizes': [128, 64, 32], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0025984202600190416, 'learning_rate_init': 0.0004584430113176043, 'max_iter': 933, 'batch_size': 128}. Best is trial 1 with value: 0.660157819225251.\n",
      "[I 2025-04-25 19:28:20,418] Trial 5 finished with value: 0.664055475848876 and parameters: {'hidden_layer_sizes': [100], 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0012672778314093802, 'learning_rate_init': 0.0020315451279644746, 'max_iter': 1228, 'batch_size': 64}. Best is trial 5 with value: 0.664055475848876.\n",
      "[I 2025-04-25 19:28:25,703] Trial 6 finished with value: 0.6533381157340984 and parameters: {'hidden_layer_sizes': [100], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00021698129210519514, 'learning_rate_init': 0.0011315828543018154, 'max_iter': 980, 'batch_size': 256}. Best is trial 5 with value: 0.664055475848876.\n",
      "[I 2025-04-25 19:28:31,933] Trial 7 finished with value: 0.6650502152080344 and parameters: {'hidden_layer_sizes': [100], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.00010671933983298263, 'learning_rate_init': 0.00130160581795103, 'max_iter': 1219, 'batch_size': 64}. Best is trial 7 with value: 0.6650502152080344.\n",
      "[I 2025-04-25 19:28:57,795] Trial 8 finished with value: 0.6650310856049736 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'relu', 'solver': 'adam', 'alpha': 7.24416729833764e-05, 'learning_rate_init': 0.00010876797781564754, 'max_iter': 1286, 'batch_size': 128}. Best is trial 7 with value: 0.6650502152080344.\n",
      "[I 2025-04-25 19:29:09,285] Trial 9 finished with value: 0.6523577235772356 and parameters: {'hidden_layer_sizes': [100, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 3.158908399015296e-06, 'learning_rate_init': 0.0003103785543173318, 'max_iter': 830, 'batch_size': 64}. Best is trial 7 with value: 0.6650502152080344.\n",
      "[I 2025-04-25 19:29:26,837] Trial 10 finished with value: 0.6562601626016259 and parameters: {'hidden_layer_sizes': [128, 64], 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.009880667144984694, 'learning_rate_init': 0.0007137224716295144, 'max_iter': 1495, 'batch_size': 64}. Best is trial 7 with value: 0.6650502152080344.\n",
      "[I 2025-04-25 19:29:46,836] Trial 11 finished with value: 0.6679674796747966 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 5.133814074120905e-05, 'learning_rate_init': 0.001767342398867135, 'max_iter': 1284, 'batch_size': 128}. Best is trial 11 with value: 0.6679674796747966.\n",
      "[I 2025-04-25 19:29:57,023] Trial 12 finished with value: 0.6406456241032998 and parameters: {'hidden_layer_sizes': [100, 50], 'activation': 'tanh', 'solver': 'adam', 'alpha': 6.73849607224586e-05, 'learning_rate_init': 0.0017639639669299277, 'max_iter': 1185, 'batch_size': 128}. Best is trial 11 with value: 0.6679674796747966.\n",
      "[I 2025-04-25 19:30:06,323] Trial 13 finished with value: 0.6581970349115255 and parameters: {'hidden_layer_sizes': [128, 64], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.8053313776284613e-05, 'learning_rate_init': 0.003870549448130641, 'max_iter': 1329, 'batch_size': 128}. Best is trial 11 with value: 0.6679674796747966.\n",
      "[I 2025-04-25 19:30:20,525] Trial 14 finished with value: 0.6542706838833094 and parameters: {'hidden_layer_sizes': [128, 64, 32], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.4478416239839214e-05, 'learning_rate_init': 0.0009502561079923459, 'max_iter': 1076, 'batch_size': 64}. Best is trial 11 with value: 0.6679674796747966.\n",
      "[I 2025-04-25 19:30:43,471] Trial 15 finished with value: 0.6699282639885222 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.0255058437212586e-06, 'learning_rate_init': 0.002758276632284926, 'max_iter': 1133, 'batch_size': 64}. Best is trial 15 with value: 0.6699282639885222.\n",
      "[I 2025-04-25 19:31:02,362] Trial 16 finished with value: 0.6504112864658058 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.216404424189274e-06, 'learning_rate_init': 0.0031147555932475282, 'max_iter': 780, 'batch_size': 128}. Best is trial 15 with value: 0.6699282639885222.\n",
      "[I 2025-04-25 19:31:31,210] Trial 17 finished with value: 0.665040650406504 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 4.0696729626391416e-06, 'learning_rate_init': 0.008961197620592177, 'max_iter': 1392, 'batch_size': 64}. Best is trial 15 with value: 0.6699282639885222.\n",
      "[I 2025-04-25 19:31:52,588] Trial 18 finished with value: 0.653299856527977 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.2049550590849272e-06, 'learning_rate_init': 0.005941653771407128, 'max_iter': 1124, 'batch_size': 128}. Best is trial 15 with value: 0.6699282639885222.\n",
      "[I 2025-04-25 19:32:07,760] Trial 19 finished with value: 0.6836202773792444 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 4.957722120389769e-06, 'learning_rate_init': 0.0023417738119657686, 'max_iter': 827, 'batch_size': 256}. Best is trial 19 with value: 0.6836202773792444.\n",
      "[I 2025-04-25 19:32:25,595] Trial 20 finished with value: 0.6719081779053084 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 4.0621826860561955e-06, 'learning_rate_init': 0.0025461258282815894, 'max_iter': 709, 'batch_size': 256}. Best is trial 19 with value: 0.6836202773792444.\n",
      "[I 2025-04-25 19:32:36,235] Trial 21 finished with value: 0.6679961740793878 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 4.0769581567960775e-06, 'learning_rate_init': 0.0026496037767007917, 'max_iter': 711, 'batch_size': 256}. Best is trial 19 with value: 0.6836202773792444.\n",
      "[I 2025-04-25 19:32:39,307] Trial 22 finished with value: 0.6572740315638451 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 2.458455611815117e-06, 'learning_rate_init': 0.00515731758566999, 'max_iter': 853, 'batch_size': 256}. Best is trial 19 with value: 0.6836202773792444.\n",
      "[I 2025-04-25 19:32:42,767] Trial 23 finished with value: 0.668962219033955 and parameters: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 7.984447049529162e-06, 'learning_rate_init': 0.00215235482661504, 'max_iter': 693, 'batch_size': 256}. Best is trial 19 with value: 0.6836202773792444.\n",
      "[I 2025-04-25 19:32:44,005] Trial 24 finished with value: 0.660181731229077 and parameters: {'hidden_layer_sizes': [128, 64], 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.0100088942019214e-06, 'learning_rate_init': 0.00551300519043689, 'max_iter': 512, 'batch_size': 256}. Best is trial 19 with value: 0.6836202773792444.\n",
      "2025-04-25 19:32:44,015 - INFO - Opt complete mlp. Best CV score: 0.68362. Best params: {'hidden_layer_sizes': [256, 128], 'activation': 'tanh', 'solver': 'adam', 'alpha': 4.957722120389769e-06, 'learning_rate_init': 0.0023417738119657686, 'max_iter': 827, 'batch_size': 256}\n",
      "2025-04-25 19:32:44,016 - INFO - Saved Optuna summary: optuna_trials/mlp_study_summary_20250425_181848.txt\n",
      "2025-04-25 19:32:44,017 - INFO - Instantiating final mlp model...\n",
      "2025-04-25 19:32:44,018 - INFO - MLP final model - using sample_weight in fit step\n",
      "2025-04-25 19:32:44,020 - INFO - Fitting final mlp model...\n",
      "2025-04-25 19:32:46,961 - INFO - Final mlp fitted in 2.94s.\n",
      "2025-04-25 19:32:46,962 - INFO - Saving final mlp model...\n",
      "2025-04-25 19:32:46,971 - INFO - Saved final mlp via joblib: models/mlp_20250425_181848.joblib\n",
      "2025-04-25 19:32:46,971 - INFO - Saving importance mlp...\n",
      "2025-04-25 19:32:46,971 - INFO - Standard feature importance not directly available for MLPClassifier mlp.\n",
      "2025-04-25 19:32:46,972 - INFO - Qualification Check for mlp: final_model exists? True, best_cv_score=0.6836203, threshold=0.72, comparison result: False\n",
      "2025-04-25 19:32:46,973 - INFO - --- NOT QUALIFIED: mlp (CV Score: 0.68362 ) ---\n",
      "2025-04-25 19:32:46,973 - INFO - --- Optimizing keras_mlp (20 trials) ---\n",
      "2025-04-25 19:32:46,974 - INFO - Starting keras_mlp optimization (20 trials)...\n",
      "2025-04-25 19:32:46,974 - INFO - Optuna timeout for keras_mlp: 7200s.\n",
      "[I 2025-04-25 19:32:47,156] A new study created in RDB with name: keras_mlp_opt_20250425_181848\n",
      "2025-04-25 19:32:47,162 - INFO - Setting Optuna timeout 7200s.\n",
      "2025-04-25 19:32:47,498 - INFO - Keras model built: Input(39), Hidden(5 layers, units=[22, 17, 18, 22, 26]), Output(3)\n",
      "2025-04-25 19:32:47,499 - INFO -  Activation: relu, Dropout: 0.5493127012500447, L2 Reg: 6.269994874625446e-06, Optimizer: adam, LR: 0.0023600748738228463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "# Assume all necessary functions (run_complete_pipeline, etc.) are defined above\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # Feature Selection Settings\n",
    "    PERFORM_FEATURE_SELECTION = True # Set to True or False\n",
    "    FS_THRESHOLD = 'mean' # Threshold ('mean', 'median', or float like 1e-5)\n",
    "\n",
    "    # Model Qualification Threshold\n",
    "    MIN_CV_SCORE_THRESHOLD = 0.72 # Minimum average CV score to qualify a model\n",
    "\n",
    "    # Parallelism Setting for Sklearn Models (used in Optuna objective and Ensemble)\n",
    "    # Set to 1 if experiencing issues, otherwise set to desired core count (e.g., os.cpu_count() - 2)\n",
    "    # N_CORES_TO_USE = 1 # Start with 1 for stability, increase carefully\n",
    "    # import os\n",
    "    N_CORES_TO_USE = max(1, os.cpu_count() - 4) # Example: Use all but 2 cores\n",
    "\n",
    "    # --- Run the Pipeline ---\n",
    "    pipeline_start_time = time.time()\n",
    "\n",
    "    success = run_complete_pipeline(\n",
    "        perform_feature_selection=PERFORM_FEATURE_SELECTION,\n",
    "        fs_threshold=FS_THRESHOLD,\n",
    "        min_cv_score_threshold=MIN_CV_SCORE_THRESHOLD,\n",
    "        n_jobs_sklearn=N_CORES_TO_USE # Pass the core count\n",
    "        )\n",
    "\n",
    "    pipeline_end_time = time.time()\n",
    "    pipeline_duration = pipeline_end_time - pipeline_start_time\n",
    "\n",
    "    # --- Final Status Output ---\n",
    "    status_msg = f\"Pipeline execution {'succeeded' if success else 'failed'}.\"\n",
    "    duration_msg = f\"Total time: {pipeline_duration:.2f} seconds ({pipeline_duration / 60:.2f} minutes).\"\n",
    "\n",
    "    print(f\"\\n{'='*30}\\n{status_msg}\")\n",
    "    print(duration_msg)\n",
    "    print(f\"{'='*30}\")\n",
    "\n",
    "    # Log final status if possible\n",
    "    try:\n",
    "        logger.info(status_msg)\n",
    "        logger.info(duration_msg)\n",
    "    except Exception as log_final_e:\n",
    "        # This might happen if the logger itself failed earlier\n",
    "        print(f\"Note: Final status logging failed: {log_final_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4a758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845a5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superNova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
