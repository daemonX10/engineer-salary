{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab21e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:37:17,051 - INFO - Set LOKY_MAX_CPU_COUNT to 20\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,\n",
    "    AdaBoostClassifier, VotingClassifier, StackingClassifier\n",
    ")\n",
    "\n",
    "# SVC import is removed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA # Keep PCA if used in FE\n",
    "# TruncatedSVD import removed (can be added back if needed)\n",
    "from category_encoders import TargetEncoder # Keep if used (e.g., for job title)\n",
    "# CatBoostEncoder import removed (can be added back if needed)\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb # Added LightGBM\n",
    "import optuna\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import logging\n",
    "import subprocess\n",
    "import math\n",
    "from sklearn.calibration import CalibratedClassifierCV # <-- Added for Step 2/3 plan\n",
    "\n",
    "# Configure logging (Ensure this runs before logger is used)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Define the global logger instance\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "# Optional: Set LOKY env var if needed for Windows parallelism issues with joblib\n",
    "try:\n",
    "    cpu_count = os.cpu_count()\n",
    "    if cpu_count: os.environ[\"LOKY_MAX_CPU_COUNT\"] = str(cpu_count)\n",
    "    logger.info(f\"Set LOKY_MAX_CPU_COUNT to {os.environ.get('LOKY_MAX_CPU_COUNT')}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not set LOKY_MAX_CPU_COUNT: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3850ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Assume logger is defined globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Creates the necessary directory structure for the project.\"\"\"\n",
    "    directories = ['models', 'features', 'results', 'submissions', 'logs', 'plots', 'optuna_trials', 'scalers', 'calibrated_models'] # Added calibrated_models\n",
    "    logger.info(\"Creating directory structure...\")\n",
    "    for directory in directories:\n",
    "        try:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "                logger.info(f\"Created directory: {directory}\")\n",
    "            # else: # Optional: log if directory already exists\n",
    "                logger.debug(f\"Directory already exists: {directory}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating directory {directory}: {e}\")\n",
    "            # Re-raise the exception to halt execution if directory creation fails,\n",
    "            # as it's likely critical for the pipeline.\n",
    "            raise\n",
    "    logger.info(\"Directory structure verified/created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bab0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Returns the current timestamp in YYYYMMDD_HHMMSS format.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6100ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume logger is defined globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def save_feature_importance(model, feature_names, timestamp, model_name):\n",
    "    \"\"\"\n",
    "    Saves feature importances for compatible models (Tree-based, Linear).\n",
    "    Logs a message for models where standard importance isn't directly available.\n",
    "    \"\"\"\n",
    "    if not feature_names:\n",
    "        logger.warning(f\"No feature names provided for {model_name}. Skipping feature importance.\")\n",
    "        return\n",
    "\n",
    "    importances = None\n",
    "    importance_type = None\n",
    "    is_fitted = True # Assume fitted unless checked otherwise\n",
    "\n",
    "    # --- Model Type Specific Handling ---\n",
    "    if isinstance(model, KerasClassifier):\n",
    "        try:\n",
    "            _ = model.model_ # Check if internal model exists\n",
    "            logger.info(f\"Standard feature importance plot not generated for Keras model {model_name}.\")\n",
    "            logger.info(\"Consider using techniques like Permutation Importance or SHAP.\")\n",
    "        except AttributeError:\n",
    "            logger.warning(f\"Keras model {model_name} not fitted. Skip importance.\")\n",
    "        return # Exit for Keras models\n",
    "\n",
    "    elif isinstance(model, (VotingClassifier, StackingClassifier)):\n",
    "        logger.info(f\"Importance plot not generated for ensemble {model_name}.\")\n",
    "        return # Exit for ensembles\n",
    "\n",
    "    elif isinstance(model, MLPClassifier):\n",
    "        logger.info(f\"Standard feature importance not directly available for MLPClassifier {model_name}.\")\n",
    "        return # Exit for MLP\n",
    "\n",
    "    # Check for standard attributes AFTER handling special cases\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        importance_type = 'Importance'\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        if model.coef_.ndim > 1:\n",
    "            importances = np.abs(model.coef_).mean(axis=0)\n",
    "        else:\n",
    "            importances = np.abs(model.coef_)\n",
    "        importance_type = 'Coefficient Magnitude'\n",
    "    elif hasattr(model, 'estimator_') and hasattr(model.estimator_, 'feature_importances_'):\n",
    "        # Handle cases like AdaBoost where the base estimator holds importance\n",
    "        # Ensure estimator_ exists and has the attribute\n",
    "        if getattr(model, 'estimator_', None) and hasattr(model.estimator_, 'feature_importances_'):\n",
    "             logger.info(f\"Using importance from base estimator ({model.estimator_.__class__.__name__}) of {model_name}.\")\n",
    "             importances = model.estimator_.feature_importances_\n",
    "             importance_type = 'Base Estimator Importance'\n",
    "        else:\n",
    "             logger.warning(f\"Base estimator not found or lacks importance for {model_name}.\")\n",
    "             return\n",
    "\n",
    "    # Add check for LightGBM specifically if feature_importances_ isn't present on fitted model sometimes\n",
    "    elif isinstance(model, lgb.LGBMClassifier) and hasattr(model, 'booster_'):\n",
    "         try:\n",
    "             importances = model.booster_.feature_importance(importance_type='gain') # Or 'split'\n",
    "             importance_type = 'LGBM Gain'\n",
    "             logger.info(f\"Using booster_.feature_importance() for {model_name}.\")\n",
    "         except Exception as lgbm_imp_err:\n",
    "              logger.warning(f\"Could not get LGBM importance via booster_: {lgbm_imp_err}\")\n",
    "              return\n",
    "    else:\n",
    "        logger.info(f\"Model {model_name} ({model.__class__.__name__}) lacks standard importance attributes (feature_importances_, coef_, relevant estimator_).\")\n",
    "        return\n",
    "\n",
    "    # --- Process and Save Importances (if found) ---\n",
    "    if importances is None:\n",
    "        logger.warning(f\"Could not retrieve importances for {model_name}.\")\n",
    "        return\n",
    "\n",
    "    if isinstance(importances, list): # Ensure numpy array\n",
    "        importances = np.array(importances)\n",
    "\n",
    "    if importances.ndim > 1:\n",
    "        logger.warning(f\"Importances shape {importances.shape} for {model_name}. Taking mean over axis 0.\")\n",
    "        importances = importances.mean(axis=0)\n",
    "\n",
    "    if len(importances) != len(feature_names):\n",
    "        logger.warning(f\"Importance length ({len(importances)}) vs names ({len(feature_names)}) mismatch for {model_name}.\")\n",
    "        return\n",
    "\n",
    "    # --- Create DataFrame and Plot ---\n",
    "    try:\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "        # Handle potential NaN/Inf in importance values before sorting\n",
    "        importance_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        importance_df.dropna(subset=['Importance'], inplace=True)\n",
    "        if importance_df.empty:\n",
    "             logger.warning(f\"Importance DataFrame became empty after dropping NaN/Inf for {model_name}.\")\n",
    "             return\n",
    "\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_n = min(30, len(importance_df))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df.head(top_n), palette='viridis')\n",
    "        plt.title(f'Top {top_n} Feature Importances - {model_name}')\n",
    "        plt.xlabel(f'Relative {importance_type}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Ensure directories exist before saving\n",
    "        plot_dir = 'plots'\n",
    "        results_dir = 'results'\n",
    "        if not os.path.exists(plot_dir): os.makedirs(plot_dir)\n",
    "        if not os.path.exists(results_dir): os.makedirs(results_dir)\n",
    "\n",
    "        plot_filename = os.path.join(plot_dir, f'{model_name}_feature_importance_{timestamp}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved importance plot: {plot_filename}\")\n",
    "\n",
    "        csv_filename = os.path.join(results_dir, f'{model_name}_feature_importance_{timestamp}.csv')\n",
    "        importance_df.to_csv(csv_filename, index=False)\n",
    "        logger.info(f\"Saved importance csv: {csv_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not save importance plot/CSV for {model_name}: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b246dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to imports at the top of your file\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "\n",
    "def apply_smote_tomek(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply SMOTETomek to handle class imbalance.\n",
    "    This combines SMOTE (oversampling) with Tomek links (undersampling) for better balance.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target labels\n",
    "        random_state: Random state for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        X_resampled, y_resampled: Balanced dataset\n",
    "    \"\"\"\n",
    "    logger.info(f\"Original class distribution: {Counter(y)}\")\n",
    "    \n",
    "    # Create the resampler\n",
    "    smt = SMOTETomek(random_state=random_state)\n",
    "    \n",
    "    # Apply resampling\n",
    "    X_resampled, y_resampled = smt.fit_resample(X, y)\n",
    "    \n",
    "    logger.info(f\"Resampled class distribution: {Counter(y_resampled)}\")\n",
    "    logger.info(f\"Original shape: {X.shape}, Resampled shape: {X_resampled.shape}\")\n",
    "    \n",
    "    # Preserve DataFrame if input was DataFrame\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1607bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def optimize_feature_selection(X, y, n_jobs=1, cv=5):\n",
    "    \"\"\"\n",
    "    Performs optimal feature selection using RFECV (Recursive Feature Elimination\n",
    "    with Cross-Validation) to find the optimal number of features.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature DataFrame\n",
    "        y: Target vector\n",
    "        n_jobs: Number of parallel jobs\n",
    "        cv: Number of cross-validation folds\n",
    "    \n",
    "    Returns:\n",
    "        selected_features: List of selected feature names\n",
    "        selector: Fitted RFECV object\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting RFECV feature selection optimization...\")\n",
    "    \n",
    "    # Create a base estimator (Random Forest)\n",
    "    base_estimator = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    \n",
    "    # Create RFECV selector\n",
    "    selector = RFECV(\n",
    "        estimator=base_estimator,\n",
    "        step=1,  # Remove one feature at a time\n",
    "        cv=cv,   # Cross-validation folds\n",
    "        scoring='accuracy',\n",
    "        min_features_to_select=5,  # Don't go below 5 features\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the selector\n",
    "    logger.info(\"Fitting RFECV feature selector (this may take time)...\")\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Get selected features\n",
    "    selected_features = X.columns[selector.support_]\n",
    "    \n",
    "    # Plot number of features vs. performance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross-validation score\")\n",
    "    plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\n",
    "    plt.title('Optimal Feature Selection with RFECV')\n",
    "    \n",
    "    # Save the plot\n",
    "    timestamp = get_timestamp()\n",
    "    plt_path = f'plots/rfecv_feature_selection_{timestamp}.png'\n",
    "    plt.savefig(plt_path)\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Optimal number of features: {selector.n_features_}\")\n",
    "    logger.info(f\"Selected {len(selected_features)} features from {X.shape[1]} original features\")\n",
    "    logger.info(f\"Best cross-validation score: {selector.grid_scores_.max():.5f}\")\n",
    "    logger.info(f\"Feature selection plot saved to: {plt_path}\")\n",
    "    \n",
    "    return selected_features, selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c9c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_keras_model(n_features, n_classes, optimizer='adam', learning_rate=0.001,\n",
    "                      hidden_units=[128, 64], dropout_rate=0.3, activation='relu', l2_reg=1e-4):\n",
    "    \"\"\"Builds a Keras MLP model with specified architecture and hyperparameters.\"\"\"\n",
    "\n",
    "    model = keras.Sequential(name=\"keras_mlp_tabular\")\n",
    "    model.add(layers.Input(shape=(n_features,)))\n",
    "\n",
    "    # Optional: Batch Norm before the first layer\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Hidden Layers\n",
    "    for units in hidden_units:\n",
    "        model.add(layers.Dense(\n",
    "            units,\n",
    "            kernel_regularizer=keras.regularizers.l2(l2_reg) # Add L2 regularization\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization()) # Batch Norm after Dense layer\n",
    "        model.add(layers.Activation(activation)) # Activation after Batch Norm\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    # Select Optimizer\n",
    "    if optimizer.lower() == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer.lower() == 'sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        logger.warning(f\"Unsupported optimizer '{optimizer}'. Defaulting to Adam.\")\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Log model summary details\n",
    "    logger.info(f\"Keras model built: Input({n_features}), Hidden({len(hidden_units)} layers, units={hidden_units}), Output({n_classes})\")\n",
    "    logger.info(f\" Activation: {activation}, Dropout: {dropout_rate}, L2 Reg: {l2_reg}, Optimizer: {optimizer}, LR: {learning_rate}\")\n",
    "    # Optional detailed summary:\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    short_model_summary = \"\\n\".join(stringlist)\n",
    "    logger.debug(f\"Keras Model Summary:\\n{short_model_summary}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e9b3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_data(df, all_states, all_feature1, timestamp, is_training=True, feature_columns_to_use=None):\n",
    "    \"\"\"\n",
    "    Preprocesses data using combined FE logic and robust categorical handling.\n",
    "    Warns about constant columns in training data but does not drop them.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting preprocessing (Combined Logic). Is training: {is_training}\")\n",
    "    start_time = time.time()\n",
    "    data = df.copy()\n",
    "    y = None\n",
    "    le = None\n",
    "    target_column = 'salary_category'\n",
    "\n",
    "    # 1. Handle Target Variable\n",
    "    if target_column in data.columns and is_training:\n",
    "        target = data[target_column]\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(target)\n",
    "        logger.info(f\"Target '{target_column}' label encoded.\")\n",
    "        # Save encoder and mapping\n",
    "        if not os.path.exists('features'): os.makedirs('features')\n",
    "        joblib.dump(le, f'features/label_encoder_{timestamp}.joblib')\n",
    "        mapping = {int(v): k for k, v in zip(le.classes_, le.transform(le.classes_))}\n",
    "        mapping_file = f'features/target_mapping_{timestamp}.json'\n",
    "        with open(mapping_file, 'w') as f: json.dump(mapping, f, indent=4)\n",
    "        logger.info(\"Saved label encoder and mapping.\")\n",
    "    elif not is_training:\n",
    "        # Load encoder\n",
    "        try:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "            if encoder_files: le = joblib.load(f'features/{encoder_files[-1]}'); logger.info(f\"Loaded LE: {encoder_files[-1]}\")\n",
    "            else: logger.warning(\"No LE file found!\"); le = None\n",
    "        except Exception as e: logger.error(f\"Failed load LE: {e}\"); le = None\n",
    "    elif is_training: # Training mode but no target\n",
    "        logger.error(f\"Target column '{target_column}' missing in training data!\")\n",
    "        raise ValueError(f\"Target column '{target_column}' not found.\")\n",
    "\n",
    "    # 2. Define Feature Groups\n",
    "    boolean_features = [f for f in ['feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_10', 'feature_11'] if f in data.columns]\n",
    "    numerical_features = [f for f in ['feature_2', 'feature_9', 'feature_12'] if f in data.columns]\n",
    "    job_desc_cols = [col for col in data.columns if col.startswith('job_desc_')]\n",
    "    all_numerical_features = numerical_features + job_desc_cols\n",
    "\n",
    "    # 3. Initial Cleaning\n",
    "    logger.info(\"Initial cleaning: Numerical and Boolean Features...\")\n",
    "    for col in all_numerical_features:\n",
    "        if col in data.columns:\n",
    "            if data[col].dtype == 'object': data[col] = data[col].replace(['', ' ', 'NA', 'None', 'NULL'], np.nan)\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "            median_val = data[col].median(); fill_value = median_val if not pd.isna(median_val) else 0\n",
    "            data[col] = data[col].fillna(fill_value)\n",
    "    for col in boolean_features:\n",
    "        if col in data.columns:\n",
    "            numeric_view = pd.to_numeric(data[col], errors='coerce'); is_boolean_like = numeric_view.dropna().isin([0, 1]).all()\n",
    "            if is_boolean_like: data[col] = numeric_view.fillna(0).astype(int)\n",
    "            else:\n",
    "                num_non_bool = numeric_view.dropna().loc[~numeric_view.dropna().isin([0, 1])].count()\n",
    "                logger.warning(f\"Col '{col}' has non-0/1 vals ({num_non_bool}). Treat as numeric, impute median.\")\n",
    "                median_val = numeric_view.median(); fill_value = median_val if not pd.isna(median_val) else 0\n",
    "                data[col] = numeric_view.fillna(fill_value)\n",
    "\n",
    "    logger.info(\"Starting Feature Engineering...\")\n",
    "    engineered_feature_names = []\n",
    "    target_encoded_title = 'job_title_encoded' # Define expected name\n",
    "\n",
    "    # --- Feature Engineering Steps (Combined logic from analysis) ---\n",
    "    if 'job_title' in data.columns:\n",
    "        data['job_title'] = data['job_title'].fillna('Unknown')\n",
    "        title_flags = ['is_senior', 'is_junior', 'is_developer', 'is_specialist']\n",
    "        data['is_senior'] = data['job_title'].str.lower().str.contains('senior|sr|lead|principal').fillna(False).astype(int)\n",
    "        data['is_junior'] = data['job_title'].str.lower().str.contains('junior|jr|associate|entry').fillna(False).astype(int)\n",
    "        data['is_developer'] = data['job_title'].str.lower().str.contains('develop|programmer|coder|engineer').fillna(False).astype(int)\n",
    "        data['is_specialist'] = data['job_title'].str.lower().str.contains('special|expert|consult').fillna(False).astype(int)\n",
    "        engineered_feature_names.extend(title_flags)\n",
    "        title_counts = data['job_title'].value_counts(); rare_titles = title_counts[title_counts < 10].index\n",
    "        data['job_title_grouped'] = data['job_title'].apply(lambda x: 'Other_Title' if x in rare_titles else x)\n",
    "        title_encoder_col = 'job_title_grouped'; engineered_feature_names.append(target_encoded_title)\n",
    "        if is_training:\n",
    "            job_encoder = TargetEncoder(cols=[title_encoder_col], handle_missing='value', handle_unknown='value')\n",
    "            data[target_encoded_title] = job_encoder.fit_transform(data[[title_encoder_col]], y) # Use encoded y\n",
    "            joblib.dump(job_encoder, f'features/job_title_encoder_{timestamp}.joblib')\n",
    "            logger.info(f\"Fit/saved TE for {title_encoder_col}\")\n",
    "        else:\n",
    "            encoder_path = f'features/job_title_encoder_{timestamp}.joblib'\n",
    "            fallback_files = sorted([f for f in os.listdir('features') if f.startswith('job_title_encoder_')])\n",
    "            loaded = False\n",
    "            \n",
    "            if os.path.exists(encoder_path):\n",
    "                try:\n",
    "                    job_encoder = joblib.load(encoder_path)\n",
    "                    data[target_encoded_title] = job_encoder.transform(data[[title_encoder_col]])\n",
    "                    loaded = True\n",
    "                    logger.info(f\"Loaded TE: {encoder_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed load TE '{encoder_path}': {e}. Try fallback.\")\n",
    "                    \n",
    "            if not loaded and fallback_files:\n",
    "                latest_encoder = fallback_files[-1]\n",
    "                try:\n",
    "                    job_encoder = joblib.load(f'features/{latest_encoder}')\n",
    "                    data[target_encoded_title] = job_encoder.transform(data[[title_encoder_col]])\n",
    "                    loaded = True\n",
    "                    logger.info(f\"Loaded fallback TE: {latest_encoder}\")\n",
    "                except Exception as e_fb:\n",
    "                    logger.error(f\"Fallback TE failed: {e_fb}. Fill 0.5\")\n",
    "            if not loaded: logger.error(\"No TE found. Fill 0.5\"); data[target_encoded_title] = 0.5\n",
    "        data = data.drop(['job_title', 'job_title_grouped'], axis=1, errors='ignore'); logger.info(\"Processed 'job_title'.\")\n",
    "\n",
    "    if 'job_posted_date' in data.columns:\n",
    "        data['job_posted_date'] = data['job_posted_date'].fillna('2000/01')\n",
    "        def extract_year(d): \n",
    "            try: return int(str(d)[:4]) \n",
    "            except: return 2000\n",
    "        def extract_month(d): \n",
    "            try: return int(str(d).split('/')[1]) \n",
    "            except: return 1\n",
    "        data['job_posted_year'] = data['job_posted_date'].apply(extract_year); data['job_posted_month'] = data['job_posted_date'].apply(extract_month); data['job_posted_month'] = data['job_posted_month'].clip(1, 12)\n",
    "        date_features = ['month_sin', 'month_cos', 'job_recency', 'job_posted_year_norm']\n",
    "        data['month_sin'] = np.sin(2 * np.pi * data['job_posted_month'] / 12); data['month_cos'] = np.cos(2 * np.pi * data['job_posted_month'] / 12); data['job_recency'] = data['job_posted_year'] * 12 + data['job_posted_month']\n",
    "        mean_year = 2022; data['job_posted_year_norm'] = data['job_posted_year'] - mean_year\n",
    "        engineered_feature_names.extend(date_features)\n",
    "        data = data.drop(['job_posted_date', 'job_posted_year', 'job_posted_month'], axis=1, errors='ignore'); logger.info(\"Processed 'job_posted_date'.\")\n",
    "\n",
    "    num_transform_features = []\n",
    "    # Feature 9 processing\n",
    "    if 'feature_9' in data.columns:\n",
    "        try: data['feature_9_bin'] = pd.qcut(data['feature_9'].rank(method='first'), q=5, labels=[0, 1, 2, 3, 4], duplicates='drop').astype(int)\n",
    "        except ValueError: logger.warning(\"qcut fail f9, use cut.\")\n",
    "        try: \n",
    "            data['feature_9_bin'] = pd.cut(data['feature_9'], bins=5, labels=[0, 1, 2, 3, 4], include_lowest=True, duplicates='drop').astype(int) \n",
    "        except Exception as e_cut: logger.error(f\"Cut fail f9: {e_cut}. Bin 0.\"); data['feature_9_bin'] = 0\n",
    "        data['feature_9_bin'] = data['feature_9_bin'].fillna(int(data['feature_9_bin'].median())) # Fill potential NaNs from cut\n",
    "        num_transform_features.append('feature_9_bin'); logger.info(\"Added bin f9.\")\n",
    "        if 'feature_2' in data.columns: interaction_name = 'feature_2_9_interaction'; data[interaction_name] = data['feature_2'] * data['feature_9']; num_transform_features.append(interaction_name); logger.info(f\"Added: {interaction_name}\")\n",
    "\n",
    "    # Feature 2 processing\n",
    "    if 'feature_2' in data.columns:\n",
    "        # Log transform (handle potential zeros/negatives if necessary)\n",
    "        data['feature_2_log'] = np.log1p(data['feature_2']) # Add log transform\n",
    "        num_transform_features.append('feature_2_log')\n",
    "\n",
    "        data['feature_2_squared'] = data['feature_2'] ** 2\n",
    "        data['feature_2_sqrt'] = np.sqrt(np.abs(data['feature_2']))\n",
    "        num_transform_features.extend(['feature_2_squared', 'feature_2_sqrt'])\n",
    "        try: \n",
    "            data['feature_2_bin'] = pd.qcut(data['feature_2'].rank(method='first'), q=5, labels=[0, 1, 2, 3, 4], duplicates='drop').astype(int)\n",
    "        except ValueError: \n",
    "            logger.warning(\"qcut fail f2, use cut.\"); \n",
    "            try: \n",
    "                data['feature_2_bin'] = pd.cut(data['feature_2'], bins=5, labels=[0, 1, 2, 3, 4], include_lowest=True, duplicates='drop').astype(int) \n",
    "            except Exception as e_cut: \n",
    "                logger.error(f\"Cut fail f2: {e_cut}. Bin 0.\"); \n",
    "                data['feature_2_bin'] = 0\n",
    "        data['feature_2_bin'] = data['feature_2_bin'].fillna(int(data['feature_2_bin'].median())) # Fill potential NaNs from cut\n",
    "        num_transform_features.append('feature_2_bin')\n",
    "        logger.info(\"Added transforms f2 (sq, sqrt, bin).\")\n",
    "    engineered_feature_names.extend(num_transform_features)\n",
    "\n",
    "    bool_agg_features = []; actual_boolean_cols = [col for col in boolean_features if col in data.columns]\n",
    "    if actual_boolean_cols: data['boolean_sum'] = data[actual_boolean_cols].sum(axis=1); data['boolean_sum_squared'] = data['boolean_sum'] ** 2; bool_agg_features.extend(['boolean_sum', 'boolean_sum_squared']); logger.info(\"Added bool aggs.\")\n",
    "    else: data['boolean_sum'] = 0; data['boolean_sum_squared'] = 0; logger.info(\"No bool features for agg.\")\n",
    "    engineered_feature_names.extend(bool_agg_features)\n",
    "\n",
    "    if 'feature_10' in data.columns and 'feature_8' in data.columns: interaction_name = 'feature_10_8_interaction'; data[interaction_name] = data['feature_10'] * data['feature_8']; engineered_feature_names.append(interaction_name); logger.info(f\"Added: {interaction_name}\")\n",
    "\n",
    "    # --- NEW Interactions based on Analysis ---\n",
    "    new_interactions = []\n",
    "    if 'feature_2' in data.columns:\n",
    "        if target_encoded_title in data.columns:\n",
    "             int_name = f'feat2_{target_encoded_title}'; data[int_name] = data['feature_2'] * data[target_encoded_title]; new_interactions.append(int_name)\n",
    "        if 'boolean_sum' in data.columns:\n",
    "             int_name = 'feat2_boolsum'; data[int_name] = data['feature_2'] * data['boolean_sum']; new_interactions.append(int_name)\n",
    "        if 'job_recency' in data.columns:\n",
    "             int_name = 'feat2_recency'; data[int_name] = data['feature_2'] * data['job_recency']; new_interactions.append(int_name)\n",
    "        # Interaction with top PCA component (assuming pca_0 exists)\n",
    "        if 'job_desc_pca_0' in data.columns:\n",
    "            int_name = 'feat2_pca0'; data[int_name] = data['feature_2'] * data['job_desc_pca_0']; new_interactions.append(int_name)\n",
    "    if target_encoded_title in data.columns and 'job_recency' in data.columns:\n",
    "        int_name = f'{target_encoded_title}_recency'; data[int_name] = data[target_encoded_title] * data['job_recency']; new_interactions.append(int_name)\n",
    "    if new_interactions: logger.info(f\"Added new interactions: {new_interactions}\"); engineered_feature_names.extend(new_interactions)\n",
    "    # --- End New Interactions ---\n",
    "\n",
    "    job_desc_eng_features = []\n",
    "    if job_desc_cols:\n",
    "        desc_agg = ['job_desc_mean', 'job_desc_std', 'job_desc_min', 'job_desc_max', 'job_desc_sum', 'job_desc_q25', 'job_desc_q75', 'job_desc_iqr']\n",
    "        data['job_desc_mean'] = data[job_desc_cols].mean(axis=1); data['job_desc_std'] = data[job_desc_cols].std(axis=1).fillna(0); data['job_desc_min'] = data[job_desc_cols].min(axis=1); data['job_desc_max'] = data[job_desc_cols].max(axis=1); data['job_desc_sum'] = data[job_desc_cols].sum(axis=1); data['job_desc_q25'] = data[job_desc_cols].quantile(0.25, axis=1); data['job_desc_q75'] = data[job_desc_cols].quantile(0.75, axis=1); data['job_desc_iqr'] = data['job_desc_q75'] - data['job_desc_q25']; job_desc_eng_features.extend(desc_agg)\n",
    "        n_pca_components = 15 # Keep default or adjust based on experiments\n",
    "        if len(job_desc_cols) > n_pca_components:\n",
    "            logger.info(f\"Applying PCA (n={n_pca_components}) to job desc...\")\n",
    "            pca_names = [f'job_desc_pca_{i}' for i in range(n_pca_components)]; job_desc_eng_features.extend(pca_names); job_desc_pca_result = None\n",
    "            if is_training: pca = PCA(n_components=n_pca_components, random_state=42); job_desc_pca_result = pca.fit_transform(data[job_desc_cols]); joblib.dump(pca, f'features/job_desc_pca_{timestamp}.joblib'); logger.info(\"Fit/saved PCA.\")\n",
    "            else:\n",
    "                pca_path = f'features/job_desc_pca_{timestamp}.joblib'; fallback_files = sorted([f for f in os.listdir('features') if f.startswith('job_desc_pca_')]); pca_loaded = False; pca = None\n",
    "                if os.path.exists(pca_path): \n",
    "                    try: \n",
    "                        pca = joblib.load(pca_path)\n",
    "                        pca_loaded=True; logger.info(f\"Loaded PCA: {pca_path}\") \n",
    "                    except Exception as e: \n",
    "                        logger.error(f\"Fail load PCA: {e}. Try fallback.\")\n",
    "                if not pca_loaded and fallback_files:\n",
    "                    latest_pca = fallback_files[-1]\n",
    "                    try: \n",
    "                        pca = joblib.load(f'features/{latest_pca}')\n",
    "                        pca_loaded = True\n",
    "                    except Exception as e_fb:\n",
    "                        logger.error(f\"Fallback PCA load failed: {e_fb}.\")\n",
    "                else:\n",
    "                    latest_pca = None\n",
    "                if pca_loaded and pca is not None: \n",
    "                    try: \n",
    "                        job_desc_pca_result = pca.transform(data[job_desc_cols]) \n",
    "                    except Exception as e_trans: \n",
    "                        logger.error(f\"PCA transform fail: {e_trans}. Fill 0.\")\n",
    "                if job_desc_pca_result is None: logger.error(\"PCA result None. Fill 0.\")\n",
    "                job_desc_pca_result = np.zeros((data.shape[0], n_pca_components))\n",
    "            for i in range(min(n_pca_components, job_desc_pca_result.shape[1])): data[pca_names[i]] = job_desc_pca_result[:, i]\n",
    "        else: logger.warning(f\"Skip PCA: Not enough features.\")\n",
    "        data = data.drop(columns=job_desc_cols, errors='ignore'); logger.info(\"Finished job desc features.\")\n",
    "    else: logger.info(\"No job desc features.\")\n",
    "    engineered_feature_names.extend(job_desc_eng_features)\n",
    "\n",
    "    # --- Robust Categorical Handling ---\n",
    "    if 'job_state' in data.columns: data['job_state'] = data['job_state'].fillna('Unknown')\n",
    "    if 'feature_1' in data.columns: data['feature_1'] = data['feature_1'].fillna('Unknown')\n",
    "\n",
    "    manual_ohe_features = []\n",
    "    logger.info(f\"Applying manual OHE for 'job_state' ({len(all_states)} unique).\")\n",
    "    if 'job_state' in data.columns:\n",
    "        for state in all_states: col_name = f'state_{state}'; data[col_name] = (data['job_state'] == state).astype(int); manual_ohe_features.append(col_name)\n",
    "        data = data.drop('job_state', axis=1, errors='ignore')\n",
    "    else: logger.warning(\"'job_state' not found for OHE.\")\n",
    "\n",
    "    logger.info(f\"Applying manual OHE for 'feature_1' ({len(all_feature1)} unique).\")\n",
    "    if 'feature_1' in data.columns:\n",
    "        for feat in all_feature1: col_name = f'feat1_{feat}'; data[col_name] = (data['feature_1'] == feat).astype(int); manual_ohe_features.append(col_name)\n",
    "        data = data.drop('feature_1', axis=1, errors='ignore')\n",
    "    else: logger.warning(\"'feature_1' not found for OHE.\")\n",
    "    engineered_feature_names.extend(manual_ohe_features)\n",
    "    # --- End FE ---\n",
    "\n",
    "    # 5. Final Cleanup and Column Management\n",
    "    logger.info(\"Final cleanup and column alignment...\")\n",
    "    columns_to_exclude = ['obs']\n",
    "    if is_training and target_column in df.columns: columns_to_exclude.append(target_column)\n",
    "    potential_feature_cols = [col for col in data.columns if col not in columns_to_exclude]\n",
    "\n",
    "    inf_cols_handled = []; nan_cols_handled = []\n",
    "    for col in potential_feature_cols:\n",
    "        if pd.api.types.is_numeric_dtype(data[col]):\n",
    "            if np.isinf(data[col]).any(): inf_cols_handled.append(col); data[col] = data[col].replace([np.inf, -np.inf], np.nan)\n",
    "            if data[col].isnull().any(): nan_cols_handled.append(col); data[col] = data[col].fillna(0)\n",
    "    if inf_cols_handled: logger.warning(f\"Replaced Inf: {inf_cols_handled}\")\n",
    "    final_nan_cols = list(set(nan_cols_handled) - set(inf_cols_handled))\n",
    "    if final_nan_cols: logger.info(f\"Filled NaNs with 0: {final_nan_cols}\")\n",
    "    for col in potential_feature_cols:\n",
    "        if data[col].dtype == 'bool': data[col] = data[col].astype(int)\n",
    "\n",
    "    if is_training:\n",
    "        constant_cols_found = []\n",
    "        for col in potential_feature_cols:\n",
    "            nunique_val = data[col].nunique(dropna=False)\n",
    "            if nunique_val <= 1: logger.warning(f\"Col '{col}' constant (nunique={nunique_val}) in train. Engineered: {col in engineered_feature_names}. Keeping col.\") ; constant_cols_found.append(col)\n",
    "            elif nunique_val <= 3 and col in engineered_feature_names: logger.info(f\"Eng col '{col}' low card (nunique={nunique_val}) in train.\")\n",
    "\n",
    "        final_feature_columns = potential_feature_cols\n",
    "        joblib.dump(final_feature_columns, f'features/feature_columns_{timestamp}.joblib')\n",
    "        logger.info(f\"Saved {len(final_feature_columns)} feature names (const cols NOT dropped).\")\n",
    "        X = data[final_feature_columns]\n",
    "        logger.info(f\"Preprocessing train done. Shape: {X.shape}. Time: {time.time() - start_time:.2f}s\")\n",
    "        try: X.head().to_csv(f'features/processed_features_head_{timestamp}.csv', index=False)\n",
    "        except Exception as e: logger.warning(f\"Could not save head: {e}\")\n",
    "        return X, y, final_feature_columns, le\n",
    "    else: # Test Data\n",
    "        if feature_columns_to_use is None:\n",
    "            try:\n",
    "                col_files = sorted([f for f in os.listdir('features') if f.startswith('feature_columns_')])\n",
    "                if col_files:\n",
    "                    latest_col = col_files[-1]\n",
    "                    feature_columns_to_use = joblib.load(f'features/{latest_col}')\n",
    "                    logger.info(f\"Loaded {len(feature_columns_to_use)} cols from: {latest_col}\")\n",
    "                else:\n",
    "                    logger.error(\"CRITICAL: No feature_columns file.\")\n",
    "                    raise FileNotFoundError(\"feature_columns_*.joblib missing.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load feature columns: {e}.\")\n",
    "                raise\n",
    "\n",
    "        X = pd.DataFrame(columns=feature_columns_to_use); missing_cols = []; processed_cols = list(data.columns); extra_cols = list(set(processed_cols) - set(feature_columns_to_use) - set(columns_to_exclude))\n",
    "        for col in feature_columns_to_use:\n",
    "            if col in data.columns: X[col] = data[col]\n",
    "            else: X[col] = 0; missing_cols.append(col)\n",
    "        if missing_cols: logger.warning(f\"Cols missing in test (filled 0): {missing_cols}\")\n",
    "        if extra_cols: logger.warning(f\"Cols extra in test (dropped align): {extra_cols}\")\n",
    "        X = X[feature_columns_to_use]; logger.info(f\"Preprocessing test done. Shape: {X.shape}. Time: {time.time() - start_time:.2f}s\")\n",
    "        return X, y, feature_columns_to_use, le # y is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight  # Added compute_sample_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "# Imports for models used within the function\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              ExtraTreesClassifier, AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import joblib  # For saving models\n",
    "from sklearn.calibration import CalibratedClassifierCV  # For calibration step\n",
    "\n",
    "# Assume build_keras_model function is defined elsewhere\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "# Assume save_feature_importance function is defined\n",
    "BOOSTING_EARLY_STOPPING_PATIENCE = 50\n",
    "def optimize_model(X, y, timestamp, model_type, n_trials=30, n_jobs_optuna=1):\n",
    "    \"\"\"\n",
    "    Optimizes hyperparameters for a given model type using Optuna,\n",
    "    then trains and saves the final model with best parameters.\n",
    "    Includes class_weight='balanced' or equivalent strategies.\n",
    "    Correctly handles Optuna-specific trial parameters during final instantiation.\n",
    "    Attempts calibration after successful model saving.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting {model_type} optimization ({n_trials} trials)...\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    if not isinstance(y, (np.ndarray, pd.Series)):\n",
    "        y = np.array(y)\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)  # Ensure DataFrame for consistent .iloc\n",
    "\n",
    "    n_classes = len(np.unique(y))\n",
    "    n_features = X.shape[1]\n",
    "    y_keras = to_categorical(y, num_classes=n_classes) if model_type == 'keras_mlp' else y\n",
    "\n",
    "    KERAS_EPOCHS = 150  # Reduced Keras epochs slightly\n",
    "    KERAS_PATIENCE = 25  # Increased Keras patience slightly\n",
    "    OPTUNA_TIMEOUT_PER_MODEL = 3600  # Default 1 hour\n",
    "    # Increase timeout for complex models\n",
    "    if model_type in ['xgboost', 'catboost', 'randomforest', 'gradientboosting', 'extratrees', 'lightgbm']:\n",
    "        OPTUNA_TIMEOUT_PER_MODEL = 7200  # Increase to 2 hours\n",
    "    logger.info(f\"Optuna timeout for {model_type}: {OPTUNA_TIMEOUT_PER_MODEL}s.\")\n",
    "\n",
    "    # --- Optuna Objective Function ---\n",
    "    def objective(trial):\n",
    "        model = None\n",
    "        fit_params = {}\n",
    "        use_gpu = False\n",
    "        is_keras = False\n",
    "\n",
    "        # --- Model Definitions for Optuna Trial (Includes custom weights/params) ---\n",
    "        if model_type == 'xgboost':\n",
    "            tree_method = trial.suggest_categorical('tree_method', ['hist', 'gpu_hist'])\n",
    "            param = {\n",
    "                'objective': 'multi:softprob', # Keep for probabilities\n",
    "                'num_class': n_classes,\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 5000, step=100), # Wider range\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 26, step=1), # Wider range, finer step\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.007, 0.15, log=True), # Slightly lower min\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0), # Allow full subsample\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0), # Wider range\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 20), # Wider range (regularization)\n",
    "                'gamma': trial.suggest_float('gamma', 1e-7, 1.0, log=True), # Wider upper bound (regularization)\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-7, 20.0, log=True), # Wider range (regularization)\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-7, 20.0, log=True), # Wider range (regularization)\n",
    "                'random_state': 42,\n",
    "                'booster': 'gbtree',\n",
    "                'tree_method': tree_method,\n",
    "                # 'n_jobs': 1 # Handled by tree_method check\n",
    "            }\n",
    "            if tree_method == 'gpu_hist':\n",
    "                param['gpu_id'] = 0\n",
    "                # param.pop('n_jobs', None) # n_jobs not used with gpu_hist\n",
    "            else:\n",
    "                param['n_jobs'] = 1 # Use 1 core for CPU hist for stability within Optuna\n",
    "                param.pop('gpu_id', None)\n",
    "\n",
    "            model = XGBClassifier(**param)\n",
    "            # Early stopping will be added in the CV loop fit call\n",
    "            fit_params = {} # Reset, ES handled in loop\n",
    "\n",
    "        elif model_type == 'catboost':\n",
    "            task_type = trial.suggest_categorical('task_type', ['CPU', 'GPU'])\n",
    "            # More nuanced class weight options - focus on boosting High (0) and Medium (2) slightly\n",
    "            class_weight_options = [\n",
    "                None,\n",
    "                'Balanced',\n",
    "                # Note: Optuna often saves dict keys as strings, ensure final fit handles int conversion if needed\n",
    "                {0: 1.0, 1: 1.1, 2: 1.1}, # Boost High slightly more\n",
    "                {0: 1.0, 1: 1.0, 2: 1.2}, # Boost High more, Medium slightly\n",
    "                {0: 1.0, 1: 1.0, 2: 1.3}, # Boost Medium slightly more\n",
    "            ]\n",
    "            chosen_class_weight_config = trial.suggest_categorical(\"class_weight_config\",class_weight_options)\n",
    "\n",
    "            param = {\n",
    "                'iterations': trial.suggest_int('iterations', 300, 5500, step=100), # Wider range\n",
    "                'depth': trial.suggest_int('depth', 4, 16), # Wider range\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.007, 0.15, log=True), # Slightly lower min\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.5, 30.0, log=True), # Wider range (regularization)\n",
    "                'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True), # Exploration range\n",
    "                'border_count': trial.suggest_categorical('border_count', [32, 64, 128, 254]), # Added 32\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 0.9), # Wider range\n",
    "                'loss_function': 'MultiClass',\n",
    "                'eval_metric': 'Accuracy', # Using Accuracy as metric, can change to MultiClass\n",
    "                'random_seed': 42,\n",
    "                'thread_count': -1, # Use all available CPU cores if task_type='CPU'\n",
    "                'verbose': False,\n",
    "                'task_type': task_type,\n",
    "                # 'auto_class_weights': None, # Set based on choice below\n",
    "                # 'class_weights': None # Set based on choice below\n",
    "            }\n",
    "            # Apply class weight strategy\n",
    "            if isinstance(chosen_class_weight_config, dict):\n",
    "                param['class_weights'] = chosen_class_weight_config\n",
    "                trial.set_user_attr(\"class_weight_info\", chosen_class_weight_config) # Log the dict\n",
    "            elif chosen_class_weight_config == 'Balanced':\n",
    "                param['auto_class_weights'] = 'Balanced'\n",
    "                trial.set_user_attr(\"class_weight_info\", 'Balanced')\n",
    "            else: # None case\n",
    "                trial.set_user_attr(\"class_weight_info\", 'None')\n",
    "\n",
    "\n",
    "            if task_type == 'GPU':\n",
    "                param['devices'] = '0'\n",
    "                param.pop('thread_count', None) # Not needed for GPU\n",
    "\n",
    "            model = CatBoostClassifier(**param)\n",
    "            fit_params = {'early_stopping_rounds': BOOSTING_EARLY_STOPPING_PATIENCE, 'verbose': False}\n",
    "\n",
    "        elif model_type == 'randomforest':\n",
    "            # More class weight dictionary options\n",
    "            class_weight_choices = [\n",
    "                'balanced',\n",
    "                'balanced_subsample',\n",
    "                # Note: Optuna often saves dict keys as strings, ensure final fit handles int conversion if needed\n",
    "                {0: 1.0, 1: 1.0, 2: 1.1},\n",
    "                {0: 1.0, 1: 1.0, 2: 1.2},\n",
    "                {0: 1.0, 1: 1.0, 2: 1.3},\n",
    "            ]\n",
    "            class_weight = trial.suggest_categorical('class_weight', class_weight_choices)\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 5500, step=100), # Wider range\n",
    "                'max_depth': trial.suggest_int('max_depth', 10, 100, step=2), # Allow deeper trees, rely on leaf constraints\n",
    "                # 'max_depth': trial.suggest_categorical('max_depth', [10, 20, 30, 40, 50, 60, None]), # Alternative: includes None\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 50), # Wider range (regularization)\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 30), # Wider range (regularization)\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.6, 0.8]), # Added float options\n",
    "                'bootstrap': True, # Usually best for RF\n",
    "                'class_weight': class_weight, # Apply choice\n",
    "                'random_state': 42,\n",
    "                'n_jobs': n_jobs_optuna,\n",
    "                'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.05) # Add slight pruning\n",
    "            }\n",
    "            model = RandomForestClassifier(**param)\n",
    "            fit_params = {} # RF doesn't have special fit params here\n",
    "\n",
    "        elif model_type == 'extratrees':\n",
    "            # More class weight dictionary options\n",
    "            class_weight_choices = [\n",
    "                'balanced',\n",
    "                'balanced_subsample',\n",
    "                {0: 1.0, 1: 1.0, 2: 1.1},\n",
    "                {0: 1.0, 1: 1.0, 2: 1.2},\n",
    "                {0: 1.0, 1: 1.0, 2: 1.4},\n",
    "            ]\n",
    "            class_weight = trial.suggest_categorical('class_weight', class_weight_choices)\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 5500, step=500), # Wider range\n",
    "                'max_depth': trial.suggest_int('max_depth', 10, 80, step=2), # Allow deeper\n",
    "                # 'max_depth': trial.suggest_categorical('max_depth', [10, 20, 30, 40, 50, 60, 70, None]), # Alternative\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 40), # Wider range\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 30), # Wider range\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.6, 0.8]), # Added floats\n",
    "                'bootstrap': trial.suggest_categorical('bootstrap', [False, True]), # Tune bootstrap for ET\n",
    "                'class_weight': class_weight, # Apply choice\n",
    "                'random_state': 42,\n",
    "                'n_jobs': n_jobs_optuna,\n",
    "                'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.05) # Add slight pruning\n",
    "            }\n",
    "            model = ExtraTreesClassifier(**param)\n",
    "            fit_params = {}\n",
    "\n",
    "        elif model_type == 'gradientboosting':\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 200, 3500, step=100), # Wider range\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.007, 0.15, log=True), # Slightly lower min\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 14), # Wider range\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 5, 50), # Wider range (regularization)\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 3, 40), # Wider range (regularization)\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0), # Allow 1.0\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.7, 0.9]), # Added floats\n",
    "                'random_state': 42,\n",
    "                'loss': 'log_loss', # Keep log_loss for predict_proba\n",
    "                'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.2), # Wider range\n",
    "                # 'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1) # Add cost-complexity pruning\n",
    "            }\n",
    "            model = GradientBoostingClassifier(**param)\n",
    "            # Sample weights applied in the CV loop fit call below\n",
    "            fit_params = {}\n",
    "\n",
    "        elif model_type == 'adaboost':\n",
    "            base_depth = trial.suggest_int('base_estimator_max_depth', 1, 6) # Allow slightly deeper base trees\n",
    "            # More class weight options\n",
    "            class_weights_options = [\n",
    "                'balanced',\n",
    "                {0: 1.0, 1: 1.0, 2: 1.1},\n",
    "                {0: 1.0, 1: 1.0, 2: 1.2},\n",
    "                {0: 1.0, 1: 1.0, 2: 1.4},\n",
    "            ]\n",
    "            weight_choice = trial.suggest_categorical('class_weight_choice', class_weights_options)\n",
    "            param_ada = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 5000, step=50), # Wider range\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 2.0, log=True), # Wider range\n",
    "                'algorithm':'SAMME', # Try both\n",
    "                'random_state': 42\n",
    "            }\n",
    "            # Apply class weight choice to the base estimator\n",
    "            base_est = DecisionTreeClassifier(max_depth=base_depth, random_state=42, class_weight=weight_choice)\n",
    "            model = AdaBoostClassifier(estimator=base_est, **param_ada)\n",
    "            # Log info for final model reconstruction\n",
    "            trial.set_user_attr(\"base_estimator_max_depth\", base_depth)\n",
    "            trial.set_user_attr(\"class_weight_info\", weight_choice if isinstance(weight_choice, dict) else 'balanced' if weight_choice=='balanced' else 'None')\n",
    "            trial.set_user_attr(\"algorithm\", param_ada['algorithm']) # Log algorithm choice\n",
    "            fit_params = {}\n",
    "\n",
    "        elif model_type == 'lightgbm':\n",
    "            # More class weight options\n",
    "            class_weight_options = [\n",
    "                None,\n",
    "                'balanced',\n",
    "                {0: 1.0, 1: 1.0, 2: 1.1}, # Boost High slightly more\n",
    "                {0: 1.0, 1: 1.0, 2: 1.2}, # Boost High more, Medium slightly\n",
    "                {0: 1.0, 1: 1.0, 2: 1.4}, # Boost Medium more\n",
    "            ]\n",
    "            class_weight = trial.suggest_categorical('class_weight_option', class_weight_options)\n",
    "            param = {\n",
    "                'objective': 'multiclass',\n",
    "                'num_class': n_classes,\n",
    "                'metric': 'multi_logloss', # Standard metric for multi-class probabilities\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 5500, step=100), # Wider range\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.007, 0.15, log=True), # Slightly lower min\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 500, step=5), # Wider range (key param)\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 50), # Wider range, can be -1 if num_leaves is constrained\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0), # Allow 1.0\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0), # Allow 1.0\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 20.0, log=True), # Wider range (regularization)\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 20.0, log=True), # Wider range (regularization)\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 3, 60), # Wider range (regularization)\n",
    "                'class_weight': class_weight, # Apply choice\n",
    "                'random_state': 42,\n",
    "                'n_jobs': n_jobs_optuna,\n",
    "                'verbose': -1,\n",
    "                'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']) # Keep both\n",
    "                # Consider adding 'min_split_gain'\n",
    "                # 'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.1)\n",
    "            }\n",
    "            model = lgb.LGBMClassifier(**param)\n",
    "            # Use specific early stopping callback for LGBM\n",
    "            fit_params = {'callbacks': [lgb.early_stopping(BOOSTING_EARLY_STOPPING_PATIENCE, verbose=False)]}\n",
    "            \n",
    "\n",
    "        else:\n",
    "            logger.error(f\"Unsupported model type: {model_type}\")\n",
    "            raise ValueError(f\"Unsupported: {model_type}\")\n",
    "\n",
    "        # --- Cross-validation ---\n",
    "        scores = []\n",
    "        is_dataframe = isinstance(X, pd.DataFrame)\n",
    "        try:\n",
    "            for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "                # --- Use .iloc consistently for pandas objects ---\n",
    "                if is_dataframe: # If X is a DataFrame, assume y is a Series\n",
    "                    X_train_fold = X.iloc[train_idx]\n",
    "                    X_valid_fold = X.iloc[valid_idx]\n",
    "                    # Select training labels based on position\n",
    "                    y_train_fold = y_keras[train_idx] if is_keras else y.iloc[train_idx] # ***MODIFIED***\n",
    "                    # Select validation labels based on position\n",
    "                    y_valid_fold_orig = y.iloc[valid_idx] # ***MODIFIED***\n",
    "                else: # If X is a numpy array, assume y is also numpy array\n",
    "                    X_train_fold = X[train_idx]\n",
    "                    X_valid_fold = X[valid_idx]\n",
    "                    y_train_fold = y_keras[train_idx] if is_keras else y[train_idx]\n",
    "                    y_valid_fold_orig = y[valid_idx]\n",
    "                current_fit_params = fit_params.copy()\n",
    "\n",
    "                # --- Handle Sample Weights for Models That Need It in Fit ---\n",
    "                fold_sample_weight = None\n",
    "                if model_type in ['gradientboosting']:  \n",
    "                    # Calculate balanced weights\n",
    "                    sample_weight = compute_sample_weight('balanced', y=y_train_fold)\n",
    "                    # Apply custom emphasis based on strategy (e.g., boost High/Medium)\n",
    "                    emphasis_weights = {0: 1.0, 1: 1.0, 2: 1.2}  # Example emphasis\n",
    "                    for cls_idx, weight_multiplier in emphasis_weights.items():\n",
    "                         # Ensure y_train_fold is numpy for boolean indexing if it was a Series\n",
    "                         y_train_fold_np = y_train_fold.values if isinstance(y_train_fold, pd.Series) else y_train_fold\n",
    "                         sample_weight[y_train_fold_np == cls_idx] *= weight_multiplier\n",
    "                    fold_sample_weight = sample_weight\n",
    "                    current_fit_params['sample_weight'] = fold_sample_weight\n",
    "                    logger.debug(f\"Trial {trial.number} Fold {fold+1}: Applied sample weights for {model_type}\")\n",
    "\n",
    "                try:\n",
    "                    # Pass eval_set for models that use it with callbacks/early stopping\n",
    "                    eval_set = [(X_valid_fold, y_valid_fold_orig)]\n",
    "                    \n",
    "                    # --- XGBoost Specific Fit Call ---\n",
    "                    if model_type == 'xgboost':\n",
    "                         model.fit(X_train_fold, y_train_fold,\n",
    "                                   eval_set=eval_set,\n",
    "                                   # Pass directly\n",
    "                                   verbose=False) # Pass other relevant args directly if needed\n",
    "                    # --- LightGBM Specific Fit Call (already seemed correct) ---\n",
    "                    elif model_type == 'lightgbm':\n",
    "                         # Note: LGBM uses callbacks for early stopping, passed via fit_params\n",
    "                         current_fit_params['eval_set'] = eval_set\n",
    "                         current_fit_params['eval_metric'] = 'multi_logloss' # Or match objective metric\n",
    "                         model.fit(X_train_fold, y_train_fold, **current_fit_params)\n",
    "                    # --- CatBoost Specific Fit Call (already seemed correct) ---\n",
    "                    elif model_type == 'catboost':\n",
    "                         current_fit_params['eval_set'] = eval_set\n",
    "                         # Early stopping rounds already part of CatBoost init/params\n",
    "                         model.fit(X_train_fold, y_train_fold, **current_fit_params)\n",
    "                    # --- Default Fit Call for other models ---\n",
    "                    else:\n",
    "                         # Pass sample_weight if applicable (e.g., for GB)\n",
    "                         model.fit(X_train_fold, y_train_fold, **current_fit_params)\n",
    "                    \n",
    "                    # Predict and score\n",
    "                    y_pred = model.predict(X_valid_fold)\n",
    "                    if is_keras and y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "                        y_pred = np.argmax(y_pred, axis=1)\n",
    "                    score = accuracy_score(y_valid_fold_orig, y_pred)\n",
    "                    scores.append(score)\n",
    "                    logger.debug(f\"Trial {trial.number} Fold {fold+1} Score: {score:.5f}\")\n",
    "\n",
    "                except ValueError as ve:\n",
    "                    logger.warning(f\"CV fold {fold+1} VAL ERROR {model_type} trial {trial.number}: {ve}\")\n",
    "                    return 0.0\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"CV fold {fold+1} EXCEPTION {model_type} trial {trial.number}: {e}\", exc_info=True)\n",
    "                    scores = []\n",
    "                    break  # Log full traceback\n",
    "        except Exception as outer_e:\n",
    "            logger.error(f\"Outer CV error {model_type} trial {trial.number}: {outer_e}\", exc_info=True)\n",
    "            return 0.0\n",
    "        if not scores:\n",
    "            logger.error(f\"Cross-validation failed completely for {model_type} trial {trial.number}\")\n",
    "            return 0.0\n",
    "        mean_score = np.mean(scores)\n",
    "        logger.debug(f\"Trial {trial.number} ({model_type}) completed. Avg CV Score: {mean_score:.5f}\")\n",
    "        return mean_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Run Optuna Study ---\n",
    "    study_name = f\"{model_type}_opt_{timestamp}\"\n",
    "    storage_name = f\"sqlite:///optuna_trials/{study_name}.db\"\n",
    "    study = optuna.create_study(direction='maximize', study_name=study_name, storage=storage_name, \n",
    "                              load_if_exists=True, pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "    completed_trials = len([t for t in study.trials if t.state==optuna.trial.TrialState.COMPLETE])\n",
    "    trials_to_run = n_trials-completed_trials\n",
    "    \n",
    "    if trials_to_run > 0:\n",
    "        logger.info(f\"Setting Optuna timeout {OPTUNA_TIMEOUT_PER_MODEL}s.\")\n",
    "        try:\n",
    "            study.optimize(objective, n_trials=trials_to_run, timeout=OPTUNA_TIMEOUT_PER_MODEL, n_jobs=1)\n",
    "        except Exception as opt_e:\n",
    "            logger.error(f\"Optuna optimize fail {model_type}: {opt_e}\", exc_info=True)\n",
    "            return None, -1, {}\n",
    "    else:\n",
    "        logger.info(f\"Study {study_name} has {completed_trials} trials. Skip optimize.\")\n",
    "\n",
    "    # --- Retrieve Results ---\n",
    "    try:\n",
    "        if not any(t.state == optuna.trial.TrialState.COMPLETE for t in study.trials):\n",
    "            logger.error(f\"Optuna study {model_type} no successful trials.\")\n",
    "            return None, -1, {}\n",
    "        best_trial = study.best_trial\n",
    "        best_params = best_trial.params\n",
    "        best_cv_score = best_trial.value\n",
    "    except ValueError:\n",
    "        logger.error(f\"Optuna study {model_type} no best trial.\")\n",
    "        return None, -1, {}\n",
    "    except Exception as res_e:\n",
    "        logger.error(f\"Error get Optuna results {model_type}: {res_e}\", exc_info=True)\n",
    "        return None, -1, {}\n",
    "    logger.info(f\"Opt complete {model_type}. Best CV score: {best_cv_score:.5f}. Best params: {best_params}\")\n",
    "\n",
    "    # --- Save Study Summary ---\n",
    "    try:\n",
    "        summary_file = f'optuna_trials/{model_type}_study_summary_{timestamp}.txt'\n",
    "        params_json = best_params.copy()\n",
    "        if model_type=='adaboost' and \"base_estimator_max_depth\" in best_trial.user_attrs:\n",
    "            params_json['base_estimator_max_depth'] = best_trial.user_attrs[\"base_estimator_max_depth\"]\n",
    "            params_json['class_weight_info'] = best_trial.user_attrs.get(\"class_weight_info\", \"N/A\")\n",
    "        if model_type=='xgboost' and 'tree_method' in best_params:\n",
    "            params_json['tree_method'] = best_params['tree_method']\n",
    "        if model_type=='catboost' and 'task_type' in best_params:\n",
    "            params_json['task_type'] = best_params['task_type']\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(f\"Optuna Summary: {model_type}\\nTS: {timestamp}\\nBest Trial: {best_trial.number}\\nScore: {best_cv_score:.5f}\\n\\nParams:\\n\")\n",
    "            json.dump(params_json, f, indent=4)\n",
    "        logger.info(f\"Saved Optuna summary: {summary_file}\")\n",
    "    except Exception as file_e:\n",
    "        logger.warning(f\"Could not save Optuna summary {model_type}: {file_e}\")\n",
    "\n",
    "    # --- Train final model ---\n",
    "    final_model = None\n",
    "    final_fit_params = {}  # Reset for final fit\n",
    "    try:\n",
    "        logger.info(f\"Instantiating final {model_type} model...\")\n",
    "        # Clean best_params from Optuna-specific args before final instantiation\n",
    "        params_for_final = best_params.copy()\n",
    "        optuna_internal_params = ['class_weight_option', 'class_weight_choice', 'class_weight_idx', \n",
    "                                 'class_weight_strategy', 'use_smote', 'smote_k', \n",
    "                                 'use_focal_loss', 'focal_gamma']  # Params used only in objective logic\n",
    "        for p in optuna_internal_params:\n",
    "            params_for_final.pop(p, None)\n",
    "\n",
    "        # Inside optimize_model, after Optuna, in elif model_type == 'adaboost':\n",
    "        if model_type == 'adaboost':\n",
    "            # Clean best_params from Optuna-specific args before final instantiation\n",
    "            params_for_final = best_params.copy()\n",
    "            # List internal params used only during Optuna trials\n",
    "            optuna_internal_params = ['class_weight_choice', 'base_estimator_max_depth']\n",
    "            for p in optuna_internal_params:\n",
    "                # --- THESE LINES REMOVE THE BAD PARAMETERS ---\n",
    "                params_for_final.pop(p, None)\n",
    "                # --- END OF REMOVAL ---\n",
    "\n",
    "            # Retrieve correct values from Optuna trial attributes\n",
    "            best_d = best_trial.user_attrs.get('base_estimator_max_depth', 1)\n",
    "            weight_info_raw = best_trial.user_attrs.get(\"class_weight_info\", 'balanced')\n",
    "            \n",
    "            # --- *** ADDED: Convert dictionary keys if needed *** ---\n",
    "            weight_info_processed = weight_info_raw\n",
    "            if isinstance(weight_info_raw, dict):\n",
    "                try:\n",
    "                    # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                    weight_info_processed = {int(k): v for k, v in weight_info_raw.items()}\n",
    "                    logger.info(f\"Converted AdaBoost class_weight keys to int: {weight_info_processed}\")\n",
    "                except ValueError as e:\n",
    "                     logger.error(f\"Error converting AdaBoost class_weight keys: {e}. Using raw: {weight_info_raw}\")\n",
    "                     weight_info_processed = weight_info_raw # Fallback to raw if conversion fails\n",
    "            # --- *** END KEY CONVERSION *** ---\n",
    "            \n",
    "            logger.info(f\"Reconstruct AdaBoost DT(max_depth={best_d}, class_weight={weight_info_processed}) using SAMME\")\n",
    "            # Create the base estimator correctly\n",
    "            base_est_inst = DecisionTreeClassifier(max_depth=best_d, random_state=42, class_weight=weight_info_processed)\n",
    "\n",
    "            final_p_ada = params_for_final # Use the cleaned dictionary for AdaBoost itself\n",
    "            final_p_ada['algorithm'] = 'SAMME'\n",
    "            # --- FINAL MODEL TRAINING WILL STILL HAPPEN USING base_est_inst and final_p_ada ---\n",
    "            final_model = AdaBoostClassifier(estimator=base_est_inst, **final_p_ada)\n",
    "\n",
    "        elif model_type == 'xgboost':\n",
    "            final_params_xgb = params_for_final.copy()\n",
    "            final_params_xgb['objective'] = 'multi:softprob'\n",
    "            final_params_xgb['num_class'] = n_classes\n",
    "            final_params_xgb['n_jobs'] = 1\n",
    "            logger.info(\"XGBoost final model - balancing via sample_weight in fit.\")\n",
    "            final_model = XGBClassifier(**final_params_xgb)\n",
    "            # Prepare sample weights for fit step\n",
    "            sample_weights_xgb = compute_sample_weight('balanced', y=y)  # Start with balanced\n",
    "            emphasis_weights = {0: 1.0, 1: 1.0, 2: 1.2}  # Emphasize High/Medium\n",
    "            for cls_idx, weight_multiplier in emphasis_weights.items():\n",
    "                sample_weights_xgb[y == cls_idx] *= weight_multiplier\n",
    "            final_fit_params['sample_weight'] = sample_weights_xgb\n",
    "\n",
    "        elif model_type == 'catboost':\n",
    "            final_params_cat = params_for_final.copy()\n",
    "            final_params_cat['loss_function'] = 'MultiClass'\n",
    "            final_params_cat['verbose'] = False\n",
    "            \n",
    "            # Remove the problematic parameter that's causing the error\n",
    "            if 'class_weight_config' in final_params_cat:\n",
    "                final_params_cat.pop('class_weight_config')\n",
    "            \n",
    "            # Re-apply class weight strategy based on best trial's choice\n",
    "            chosen_weight = best_params.get('class_weight_config')  # Use 'class_weight_config' to match the Optuna trial parameter\n",
    "            \n",
    "            # Handle dictionary class weights with proper key conversion\n",
    "            if isinstance(chosen_weight, dict):\n",
    "                try:\n",
    "                    # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                    chosen_weight_processed = {int(k): v for k, v in chosen_weight.items()}\n",
    "                    logger.info(f\"Converted CatBoost class_weights keys to int: {chosen_weight_processed}\")\n",
    "                    final_params_cat['class_weights'] = chosen_weight_processed\n",
    "                except ValueError as e:\n",
    "                    logger.error(f\"Error converting CatBoost class_weights keys: {e}. Using raw: {chosen_weight}\")\n",
    "                    final_params_cat['class_weights'] = chosen_weight  # Fallback to raw if conversion fails\n",
    "                logger.info(f\"CatBoost using custom weights: {final_params_cat['class_weights']}\")\n",
    "            elif chosen_weight == 'Balanced':\n",
    "                final_params_cat['auto_class_weights'] = 'Balanced'\n",
    "                logger.info(\"CatBoost using auto_class_weights=Balanced\")\n",
    "            else:\n",
    "                logger.info(\"CatBoost using default balancing or no weights.\")\n",
    "            \n",
    "            final_model = CatBoostClassifier(**final_params_cat)\n",
    "\n",
    "        elif model_type == 'randomforest':\n",
    "            final_params_rf = params_for_final.copy()\n",
    "            final_params_rf['n_jobs'] = n_jobs_optuna\n",
    "            \n",
    "            # --- *** ADDED: Process class_weight dictionary keys *** ---\n",
    "            class_weight_raw = best_params.get('class_weight', 'balanced')\n",
    "            class_weight_processed = class_weight_raw\n",
    "            if isinstance(class_weight_raw, dict):\n",
    "                 try:\n",
    "                     # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                     class_weight_processed = {int(k): v for k, v in class_weight_raw.items()}\n",
    "                     logger.info(f\"Converted RF class_weight keys to int: {class_weight_processed}\")\n",
    "                 except ValueError as e:\n",
    "                      logger.error(f\"Error converting RF class_weight keys: {e}. Using raw: {class_weight_raw}\")\n",
    "                      class_weight_processed = class_weight_raw # Fallback\n",
    "            # --- *** END KEY CONVERSION *** ---\n",
    "            \n",
    "            final_params_rf['class_weight'] = class_weight_processed\n",
    "            logger.info(f\"RF final model using class_weight={final_params_rf['class_weight']}\")\n",
    "            final_model = RandomForestClassifier(**final_params_rf)\n",
    "\n",
    "        elif model_type == 'extratrees':\n",
    "            final_params_et = params_for_final.copy()\n",
    "            final_params_et['n_jobs'] = n_jobs_optuna\n",
    "            \n",
    "            # --- *** ADDED: Process class_weight dictionary keys *** ---\n",
    "            class_weight_raw = best_params.get('class_weight', 'balanced')\n",
    "            class_weight_processed = class_weight_raw\n",
    "            if isinstance(class_weight_raw, dict):\n",
    "                 try:\n",
    "                     # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                     class_weight_processed = {int(k): v for k, v in class_weight_raw.items()}\n",
    "                     logger.info(f\"Converted ET class_weight keys to int: {class_weight_processed}\")\n",
    "                 except ValueError as e:\n",
    "                      logger.error(f\"Error converting ET class_weight keys: {e}. Using raw: {class_weight_raw}\")\n",
    "                      class_weight_processed = class_weight_raw # Fallback\n",
    "            # --- *** END KEY CONVERSION *** ---\n",
    "            \n",
    "            final_params_et['class_weight'] = best_params.get('class_weight', 'balanced')  # Use optimized or default balanced\n",
    "            logger.info(f\"ET final model using class_weight={final_params_et['class_weight']}\")\n",
    "            final_model = ExtraTreesClassifier(**final_params_et)\n",
    "\n",
    "        elif model_type == 'gradientboosting':\n",
    "            final_params_gb = params_for_final.copy()\n",
    "            logger.info(\"GradientBoosting final model - applying sample_weight in fit\")\n",
    "            final_model = GradientBoostingClassifier(**final_params_gb)\n",
    "            sample_weights_gb = compute_sample_weight('balanced', y=y)\n",
    "            emphasis_weights = {0: 1.0, 1: 1.0, 2: 1.2}\n",
    "            for cls_idx, mult in emphasis_weights.items():\n",
    "                sample_weights_gb[y == cls_idx] *= mult\n",
    "            final_fit_params['sample_weight'] = sample_weights_gb\n",
    "\n",
    "        # Inside optimize_model, after Optuna, in elif model_type == 'knn':\n",
    "\n",
    "        elif model_type == 'lightgbm':\n",
    "            final_params_lgbm = params_for_final.copy()\n",
    "            final_params_lgbm['objective'] = 'multiclass'\n",
    "            final_params_lgbm['num_class'] = n_classes\n",
    "            final_params_lgbm['n_jobs'] = n_jobs_optuna\n",
    "            \n",
    "            # --- *** ADDED: Refined Key Conversion for LGBM *** ---\n",
    "            class_weight_value_to_use = 'balanced' # Default\n",
    "            # Use 'class_weight_option' key from Optuna params for LGBM\n",
    "            if 'class_weight_option' in best_params:\n",
    "                class_weight_raw = best_params['class_weight_option'] # Get raw value from Optuna result\n",
    "                class_weight_processed = class_weight_raw\n",
    "\n",
    "                if isinstance(class_weight_raw, dict):\n",
    "                    logger.info(f\"Raw class_weight dict found for LGBM: {class_weight_raw}\")\n",
    "                    try:\n",
    "                        if all(isinstance(k, int) for k in class_weight_raw.keys()):\n",
    "                            logger.info(\"LGBM class_weight keys appear to be integers already.\")\n",
    "                            class_weight_processed = class_weight_raw\n",
    "                        else:\n",
    "                            logger.info(\"Attempting conversion of LGBM class_weight keys to int...\")\n",
    "                            class_weight_processed = {int(k): v for k, v in class_weight_raw.items()}\n",
    "                            logger.info(f\"Successfully converted LGBM class_weight keys to int: {class_weight_processed}\")\n",
    "                    except Exception as e_gen:\n",
    "                         logger.error(f\"Error processing LGBM class_weight dict: {e_gen}. Using 'balanced'.\")\n",
    "                         class_weight_processed = 'balanced'\n",
    "                class_weight_value_to_use = class_weight_processed\n",
    "            else:\n",
    "                 logger.info(\"No 'class_weight_option' found in best_params for LGBM, using default 'balanced'.\")\n",
    "                 class_weight_value_to_use = 'balanced'\n",
    "            \n",
    "            final_params_lgbm['class_weight'] = class_weight_value_to_use\n",
    "            logger.info(f\"LGBM final model using class_weight={final_params_lgbm['class_weight']}\")\n",
    "            final_model = lgb.LGBMClassifier(**final_params_lgbm)\n",
    "\n",
    "        # --- Fit the final model ---\n",
    "        if final_model is not None:\n",
    "            logger.info(f\"Fitting final {model_type} model...\")\n",
    "            start_fit_time = time.time()\n",
    "            model_fitted_successfully = False\n",
    "            try:\n",
    "                # Fit using specific params if they exist (like sample_weight)\n",
    "                if final_fit_params:\n",
    "                    logger.info(f\"Fitting {model_type} with additional fit parameters: {list(final_fit_params.keys())}\")\n",
    "                    final_model.fit(X, y, **final_fit_params)  # Pass original y and weights dict\n",
    "                else:\n",
    "                    final_model.fit(X, y)  # Fit standard models\n",
    "\n",
    "                fit_duration = time.time() - start_fit_time\n",
    "                logger.info(f\"Final {model_type} fitted in {fit_duration:.2f}s.\")\n",
    "                model_fitted_successfully = True\n",
    "\n",
    "            except Exception as fit_e:\n",
    "                logger.error(f\"Error during final fit for {model_type}: {fit_e}\", exc_info=True)\n",
    "                # Keep going to return score/params, but model will be None\n",
    "\n",
    "            # --- Save model and importance only if fit succeeded ---\n",
    "            if model_fitted_successfully:\n",
    "                model_path = f'models/{model_type}_{timestamp}.joblib'\n",
    "                logger.info(f\"Saving final {model_type} model...\")\n",
    "                try:\n",
    "                    if isinstance(final_model, KerasClassifier):\n",
    "                        tf_model_save_path = f'models/{model_type}_tfmodel_{timestamp}'\n",
    "                        try:\n",
    "                            final_model.model_.save(tf_model_save_path)\n",
    "                            logger.info(f\"Saved Keras TF model: {tf_model_save_path}\")\n",
    "                        except Exception as k_save_err:\n",
    "                            logger.warning(f\"Keras TF save fail ({k_save_err}), try joblib...\")\n",
    "                            joblib.dump(final_model, model_path)\n",
    "                            logger.info(f\"Saved Keras wrapper: {model_path}\")\n",
    "                    else:\n",
    "                        joblib.dump(final_model, model_path)\n",
    "                        logger.info(f\"Saved final {model_type} via joblib: {model_path}\")\n",
    "                except Exception as save_err:\n",
    "                    logger.error(f\"Failed save model {model_type}: {save_err}\", exc_info=True)\n",
    "\n",
    "                # --- Attempt Calibration AFTER saving base model ---\n",
    "                if model_type not in ['knn', 'mlp']:  # Models less suitable or needing sample_weight for calibration fit\n",
    "                    try:\n",
    "                        logger.info(f\"Attempting calibration for {model_type}...\")\n",
    "                        # Use 'estimator' argument, not 'base_estimator'\n",
    "                        calibrated_model = CalibratedClassifierCV(\n",
    "                            estimator=final_model,\n",
    "                            cv=3,\n",
    "                            method='isotonic',\n",
    "                            n_jobs=n_jobs_optuna,\n",
    "                            ensemble=False\n",
    "                        )\n",
    "                        calibrated_model.fit(X, y)  # Calibrate on the full training data\n",
    "                        calibrated_path = f'calibrated_models/{model_type}_calibrated_{timestamp}.joblib'\n",
    "                        if not os.path.exists('calibrated_models'):\n",
    "                            os.makedirs('calibrated_models')\n",
    "                        joblib.dump(calibrated_model, calibrated_path)\n",
    "                        logger.info(f\"Saved calibrated model: {calibrated_path}\")\n",
    "                    except Exception as cal_err:\n",
    "                        logger.warning(f\"Calibration failed for {model_type}: {cal_err}\", exc_info=False)\n",
    "\n",
    "                # --- Save Importance ---\n",
    "                feat_names = list(X.columns) if isinstance(X, pd.DataFrame) else None\n",
    "                if feat_names:\n",
    "                    logger.info(f\"Saving importance {model_type}...\")\n",
    "                    save_feature_importance(final_model, feat_names, timestamp, model_type)\n",
    "                else:\n",
    "                    logger.warning(f\"No feat names for importance {model_type}.\")\n",
    "\n",
    "            else:  # Fit failed\n",
    "                final_model = None  # Ensure model is None if fit failed\n",
    "\n",
    "        else:  # Instantiation failed\n",
    "            logger.error(f\"Could not instantiate final model {model_type}.\")\n",
    "            return None, best_cv_score, best_params\n",
    "\n",
    "    except Exception as final_e:\n",
    "        logger.error(f\"Failed final instantiate/fit/save process {model_type}: {final_e}\", exc_info=True)\n",
    "        # Return score/params from Optuna, but model is None\n",
    "        return None, best_cv_score, best_params\n",
    "\n",
    "    # Return potentially None model if fit/save failed, but score/params if Optuna succeeded\n",
    "    return final_model, best_cv_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4cf83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this before the create_ensemble function\n",
    "class DynamicEnsemble:\n",
    "    \"\"\"\n",
    "    Custom ensemble that uses specialized models for each class.\n",
    "    Each class gets its own best-performing model for prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_models):\n",
    "        self.class_models = class_models\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.class_models:\n",
    "            return np.zeros(len(X))\n",
    "        \n",
    "        # For each instance, get predictions from all specialized models\n",
    "        final_probs = np.zeros((X.shape[0], 3))\n",
    "        \n",
    "        # Get probabilities from each class-specific model\n",
    "        for class_idx, (_, model) in self.class_models.items():\n",
    "            probs = model.predict_proba(X)\n",
    "            final_probs[:, class_idx] = probs[:, class_idx]\n",
    "        \n",
    "        # Return the class with highest probability\n",
    "        return np.argmax(final_probs, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if not self.class_models:\n",
    "            return np.zeros((len(X), 3))\n",
    "        \n",
    "        # Initialize with equal probabilities\n",
    "        final_probs = np.zeros((X.shape[0], 3))\n",
    "        \n",
    "        # Get probabilities from each class-specific model\n",
    "        for class_idx, (_, model) in self.class_models.items():\n",
    "            probs = model.predict_proba(X)\n",
    "            final_probs[:, class_idx] = probs[:, class_idx]\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        row_sums = final_probs.sum(axis=1)\n",
    "        final_probs = final_probs / row_sums[:, np.newaxis]\n",
    "        \n",
    "        return final_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8951a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import logging\n",
    "import os\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "# Meta-learner imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb # For new default meta-learner\n",
    "# Calibration import (needed if implementing Step 3)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scikeras.wrappers import KerasClassifier # For type checking\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_ensemble(qualified_models_with_scores, X_train_ensemble, y_train_ensemble, timestamp, n_jobs_ensemble=1):\n",
    "    \"\"\"\n",
    "    Creates Voting and Stacking ensembles from a list of qualified models.\n",
    "    Default Stacking meta-learner is now LightGBM with class weights.\n",
    "    Includes comments for adding model calibration.\n",
    "\n",
    "    Args:\n",
    "        qualified_models_with_scores (list): List of tuples: (name, fitted_model, cv_score).\n",
    "                                             Assumes models were potentially trained with class weights.\n",
    "        X_train_ensemble (pd.DataFrame or np.ndarray): Training features for fitting ensembles.\n",
    "        y_train_ensemble (pd.Series or np.ndarray): Training target for fitting ensembles.\n",
    "        timestamp (str): Timestamp string for saving files.\n",
    "        n_jobs_ensemble (int): Number of parallel jobs for ensemble fitting.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fitted_voting_classifier, fitted_stacking_classifier, best_individual_model_object)\n",
    "               Models can be None if creation failed or skipped.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting ensemble creation...\")\n",
    "\n",
    "    if not qualified_models_with_scores:\n",
    "        logger.error(\"No qualified models provided for ensemble creation.\")\n",
    "        return None, None, None\n",
    "\n",
    "    sorted_models = sorted(qualified_models_with_scores, key=lambda x: x[2], reverse=True)\n",
    "    logger.info(f\"Qualified models for ensembling (Name, CV Score): {[(m[0], f'{m[2]:.5f}') for m in sorted_models]}\")\n",
    "    N_ens = len(sorted_models)\n",
    "\n",
    "    if N_ens < 2:\n",
    "        logger.warning(f\"Less than 2 qualified models ({N_ens}). Skipping ensembles.\")\n",
    "        if N_ens == 1: n, m, s = sorted_models[0]; logger.info(f\"Returning single best model: {n} (CV: {s:.5f})\"); return None, None, m\n",
    "        else: return None, None, None\n",
    "\n",
    "    # Filter usable models (as before)\n",
    "    estimators_valid_for_ensemble = []\n",
    "    keras_models_excluded = []\n",
    "    for name, model, score in sorted_models:\n",
    "        is_keras_wrapper = isinstance(model, KerasClassifier); model_saved_path = f'models/{name}_{timestamp}.joblib'; tf_model_path = f'models/{name}_tfmodel_{timestamp}'\n",
    "        if is_keras_wrapper and not os.path.exists(tf_model_path) and not os.path.exists(model_saved_path): logger.warning(f\"Keras model {name} save files missing. Exclude.\"); keras_models_excluded.append(name)\n",
    "        else: estimators_valid_for_ensemble.append((name, model))\n",
    "\n",
    "    if len(estimators_valid_for_ensemble) < 2:\n",
    "        logger.warning(f\"<2 models usable for ensemble. Skipping.\"); non_keras_models = [(n, m, s) for n, m, s in sorted_models if n not in keras_models_excluded]\n",
    "        if non_keras_models: best_n, best_m, best_s = non_keras_models[0]; logger.info(f\"Return best non-excluded: {best_n} ({best_s:.5f})\"); return None, None, best_m\n",
    "        elif sorted_models: best_n, best_m, best_s = sorted_models[0]; logger.info(f\"Return original best: {best_n} ({best_s:.5f})\"); return None, None, best_m\n",
    "        else: return None, None, None\n",
    "\n",
    "    logger.info(f\"Using {len(estimators_valid_for_ensemble)} models for ensemble: {[n for n,m in estimators_valid_for_ensemble]}\")\n",
    "    est_ens = estimators_valid_for_ensemble\n",
    "\n",
    "    # --- Optional Calibration Step (Implement if needed based on Step 3 plan) ---\n",
    "    # Example: Calibrate base models before putting them in est_ens_calibrated\n",
    "    est_ens_calibrated = []\n",
    "    logger.info(\"Calibrating base models for ensemble...\")\n",
    "    for name, model in est_ens:\n",
    "        try:\n",
    "            # Use isotonic calibration, fit on the same training data used for ensemble fitting\n",
    "            # CV within CalibratedClassifierCV helps prevent overfitting during calibration itself\n",
    "            calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=3, n_jobs=n_jobs_ensemble, ensemble=False) # Fit base estimator on each fold\n",
    "            calibrated_model.fit(X_train_ensemble, y_train_ensemble)\n",
    "            calibrated_model_path = f'calibrated_models/{name}_calibrated_{timestamp}.joblib'\n",
    "            joblib.dump(calibrated_model, calibrated_model_path)\n",
    "            est_ens_calibrated.append((name + \"_calibrated\", calibrated_model)) # Use new name and calibrated model\n",
    "            logger.info(f\"Calibrated model {name} and saved to {calibrated_model_path}\")\n",
    "        except Exception as cal_err:\n",
    "            logger.error(f\"Failed to calibrate model {name}: {cal_err}. Skipping calibration for this model.\", exc_info=True)\n",
    "            est_ens_calibrated.append((name, model)) # Use original model if calibration fails\n",
    "    est_ens_to_use = est_ens_calibrated # Use the calibrated list for subsequent steps\n",
    "    logger.info(f\"Using {len(est_ens_to_use)} models (calibrated where possible) for final ensemble.\")\n",
    "    # --- End Optional Calibration Step ---\n",
    "\n",
    "    # Use original (potentially uncalibrated) estimators for now\n",
    "    est_ens_to_use = est_ens\n",
    "\n",
    "    # --- Voting Classifier ---\n",
    "    vote_clf = None\n",
    "    can_soft = all(hasattr(m, 'predict_proba') for _, m in est_ens_to_use)\n",
    "    weights_used = None\n",
    "\n",
    "    if can_soft:\n",
    "        logger.info(\"Attempting Soft Voting...\")\n",
    "        # Weighting based on original CV scores (even if models are calibrated)\n",
    "        scores_map = {name: score for name, model, score in qualified_models_with_scores}\n",
    "        # Use original names to get scores, handle potentially calibrated names\n",
    "        scores = [scores_map.get(name.replace('_calibrated','')) for name, model in est_ens_to_use if name.replace('_calibrated','') in scores_map]\n",
    "\n",
    "        if scores and len(scores) == len(est_ens_to_use): # Ensure weights align\n",
    "            min_s = min(scores); shift_s = [s - min_s + 1e-6 for s in scores]; tot_s = sum(shift_s)\n",
    "            weights_used = [s / tot_s for s in shift_s] if tot_s > 0 else None\n",
    "            logger.info(f\"Weights:{list(np.round(weights_used,3)) if weights_used else 'Uniform'}\")\n",
    "        else:\n",
    "            logger.warning(\"Could not align scores for weighting. Using uniform weights.\")\n",
    "            weights_used = None\n",
    "\n",
    "        try:\n",
    "            vote_clf = VotingClassifier(estimators=est_ens_to_use, voting='soft', weights=weights_used, n_jobs=n_jobs_ensemble)\n",
    "            vote_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            vote_path = f'models/voting_ensemble_soft_{timestamp}.joblib'\n",
    "            joblib.dump(vote_clf, vote_path)\n",
    "            logger.info(f\"Saved Soft Voting Ensemble: {vote_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Soft Voting: {e}\", exc_info=True); vote_clf = None; can_soft = False\n",
    "    else:\n",
    "        logger.warning(\"Not all base models support predict_proba for Soft Voting.\")\n",
    "\n",
    "    if not can_soft: # Fallback or initial choice\n",
    "        logger.warning(\"Attempting Hard Voting...\")\n",
    "        weights_used = None # No weights for hard voting\n",
    "        try:\n",
    "            vote_clf = VotingClassifier(estimators=est_ens_to_use, voting='hard', n_jobs=n_jobs_ensemble)\n",
    "            vote_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            vote_path = f'models/voting_ensemble_hard_{timestamp}.joblib'\n",
    "            joblib.dump(vote_clf, vote_path)\n",
    "            logger.info(f\"Saved Hard Voting Ensemble: {vote_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Hard Voting: {e}\", exc_info=True); vote_clf = None\n",
    "    \n",
    "    dynamic_ensemble = None\n",
    "    try:\n",
    "        if len(est_ens_to_use) >= 2:\n",
    "            logger.info(\"Creating dynamic ensemble...\")\n",
    "            \n",
    "            # Dynamic ensemble combines predictions from best models for each class\n",
    "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            class_performance = {0: [], 1: [], 2: []}  # Track performance for each class\n",
    "            \n",
    "            for name, model in est_ens_to_use:\n",
    "                # Skip if model doesn't have predict_proba\n",
    "                if not hasattr(model, 'predict_proba'):\n",
    "                    continue\n",
    "                    \n",
    "                class_scores = {0: [], 1: [], 2: []}\n",
    "                \n",
    "                # Evaluate model on each fold\n",
    "                for train_idx, val_idx in skf.split(X_train_ensemble, y_train_ensemble):\n",
    "                    X_fold, y_fold = X_train_ensemble.iloc[val_idx], y_train_ensemble.iloc[val_idx]\n",
    "                    \n",
    "                    # Get probabilities for each class\n",
    "                    try:\n",
    "                        y_probs = model.predict_proba(X_fold)\n",
    "                        \n",
    "                        # Calculate performance for each class\n",
    "                        for class_idx in [0, 1, 2]:\n",
    "                            # For each class, measure how well the model predicts that class\n",
    "                            binary_y_true = (y_fold == class_idx).astype(int)\n",
    "                            binary_y_score = y_probs[:, class_idx]\n",
    "                            \n",
    "                            # Use AUC as a class-specific performance measure\n",
    "                            from sklearn.metrics import roc_auc_score\n",
    "                            try:\n",
    "                                auc = roc_auc_score(binary_y_true, binary_y_score)\n",
    "                                class_scores[class_idx].append(auc)\n",
    "                            except Exception as auc_err:\n",
    "                                logger.warning(f\"AUC calculation failed for {name}, class {class_idx}: {auc_err}\")\n",
    "                                class_scores[class_idx].append(0.5)  # Default if calculation fails\n",
    "                    except Exception as pred_err:\n",
    "                        logger.warning(f\"Prediction failed for {name}: {pred_err}\")\n",
    "                        continue\n",
    "                \n",
    "                # Average scores across folds\n",
    "                for class_idx in [0, 1, 2]:\n",
    "                    if class_scores[class_idx]:\n",
    "                        avg_score = sum(class_scores[class_idx]) / len(class_scores[class_idx])\n",
    "                        # Keep track of model and its performance for each class\n",
    "                        class_performance[class_idx].append((name, model, avg_score))\n",
    "                    \n",
    "            # Select best model for each class\n",
    "            best_models = {}\n",
    "            for class_idx in [0, 1, 2]:\n",
    "                sorted_models = sorted(class_performance[class_idx], key=lambda x: x[2], reverse=True)\n",
    "                if sorted_models:\n",
    "                    best_name, best_model, best_score = sorted_models[0]\n",
    "                    best_models[class_idx] = (best_name, best_model)\n",
    "                    logger.info(f\"Best model for class {class_idx}: {best_name} (AUC: {best_score:.5f})\")\n",
    "\n",
    "            # Create the dynamic ensemble with best models for each class\n",
    "            if best_models:\n",
    "                dynamic_ensemble = DynamicEnsemble(best_models)\n",
    "                dynamic_path = f'models/dynamic_ensemble_{timestamp}.joblib'\n",
    "                joblib.dump(dynamic_ensemble, dynamic_path)\n",
    "                logger.info(f\"Saved Dynamic Ensemble: {dynamic_path}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Dynamic Ensemble: {e}\", exc_info=True)\n",
    "        dynamic_ensemble = None\n",
    "    \n",
    "    # --- Stacking Classifier ---\n",
    "    stack_clf = None\n",
    "    # Check predict_proba on the models actually used in the ensemble list\n",
    "    can_stack = all(hasattr(m, 'predict_proba') for _, m in est_ens_to_use)\n",
    "    meta_learner = None\n",
    "\n",
    "    if can_stack:\n",
    "        logger.info(\"Attempting Stacking...\")\n",
    "        try:\n",
    "            # *** Use LightGBM as meta-learner with class weights ***\n",
    "            meta_learner = lgb.LGBMClassifier(\n",
    "                n_estimators=150, # Reasonably more estimators for meta\n",
    "                learning_rate=0.05, # Slightly lower LR\n",
    "                num_leaves=20,      # Limit complexity\n",
    "                # max_depth=5,       # Optional: limit depth further\n",
    "                class_weight='balanced', # Crucial for imbalanced meta-predictions\n",
    "                random_state=42,\n",
    "                n_jobs=1 # Meta learner should run on single core\n",
    "            )\n",
    "            # Alternative: Simpler Logistic Regression\n",
    "            # meta_learner = LogisticRegression(random_state=42, class_weight='balanced', solver='liblinear', C=1.0, n_jobs=1)\n",
    "\n",
    "            logger.info(f\"Using {meta_learner.__class__.__name__} as stacking meta-learner.\")\n",
    "            stack_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "            stack_clf = StackingClassifier(\n",
    "                estimators=est_ens_to_use, # Use potentially calibrated models\n",
    "                final_estimator=meta_learner,\n",
    "                cv=stack_cv,\n",
    "                stack_method='predict_proba',\n",
    "                n_jobs=1,\n",
    "                passthrough=False\n",
    "            )\n",
    "            stack_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            stack_path = f'models/stacking_ensemble_{timestamp}.joblib'\n",
    "            joblib.dump(stack_clf, stack_path)\n",
    "            logger.info(f\"Saved Stacking Ensemble (Meta: {meta_learner.__class__.__name__}): {stack_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Stacking: {e}\", exc_info=True)\n",
    "            stack_clf = None\n",
    "    else:\n",
    "        logger.warning(\"Cannot create Stacking Ensemble (requires predict_proba).\")\n",
    "\n",
    "    # Determine best individual model from the original qualified list\n",
    "    best_ind_q_model = None; best_n = \"N/A\"; best_s = -1.0\n",
    "    if sorted_models:\n",
    "        best_n, best_m, best_s = sorted_models[0]\n",
    "        best_ind_q_model = best_m\n",
    "        logger.info(f\"Best individual qualified model identified: {best_n} (CV Score: {best_s:.5f})\")\n",
    "\n",
    "    # Save summary (remains the same logic)\n",
    "    try:\n",
    "        summary_path = f'results/ensemble_creation_summary_{timestamp}.txt'\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(\"Ensemble Summary\\n=============\\nQualified Models:\\n\")\n",
    "            for n, _, s in sorted_models: f.write(f\"- {n}: CV {s:.5f} {'(Excl.)' if n in keras_models_excluded else '(Incl.)'}\\n\")\n",
    "            f.write(f\"\\nEnsembles ({len(est_ens_to_use)} models):\\n\") # Reflect models used\n",
    "            vote_t = 'Soft' if vote_clf and vote_clf.voting == 'soft' else ('Hard' if vote_clf and vote_clf.voting == 'hard' else 'N/A')\n",
    "            f.write(f\"- Voting ({vote_t}): {'Saved' if vote_clf else 'Failed/Skipped'}.\\n\")\n",
    "            meta_name = meta_learner.__class__.__name__ if meta_learner and stack_clf else 'N/A'\n",
    "            f.write(f\"- Stacking (Meta:{meta_name}): {'Saved' if stack_clf else 'Failed/Skipped'}.\\n\")\n",
    "            if keras_models_excluded: f.write(f\"\\nKeras Excluded: {', '.join(keras_models_excluded)}\\n\")\n",
    "            if best_ind_q_model: f.write(f\"\\nBest individual model overall (from qualified list): {best_n}\\n\")\n",
    "        logger.info(f\"Saved ensemble summary: {summary_path}\")\n",
    "    except Exception as file_e: logger.warning(f\"Could not save ensemble summary: {file_e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    return vote_clf, stack_clf, best_ind_q_model , dynamic_ensemble\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c12018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scikeras.wrappers import KerasClassifier # Keep for isinstance check\n",
    "import joblib # Keep if fallback LE loading is needed\n",
    "import os # For checking plot/results dirs\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def evaluate_model(model, X_eval, y_eval, model_name, timestamp, le):\n",
    "    \"\"\"Evaluates a trained model on a given dataset (e.g., validation or hold-out).\"\"\"\n",
    "    if model is None:\n",
    "        logger.warning(f\"Skip eval {model_name}: model None.\")\n",
    "        return None, None # Return None for accuracy and report dictionary\n",
    "\n",
    "    # Handle LabelEncoder loading if missing\n",
    "    if le is None:\n",
    "        logger.warning(f\"Skip eval {model_name}: LabelEncoder (le) is None. Attempting fallback.\")\n",
    "        try:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "            if encoder_files:\n",
    "                le = joblib.load(f'features/{encoder_files[-1]}')\n",
    "                logger.info(f\"Loaded fallback LE for evaluation: {encoder_files[-1]}\")\n",
    "            else:\n",
    "                logger.error(\"Evaluation cannot proceed without LabelEncoder.\")\n",
    "                return None, None\n",
    "        except Exception as load_err:\n",
    "             logger.error(f\"Evaluation failed: Could not load fallback LE: {load_err}\")\n",
    "             return None, None\n",
    "\n",
    "    logger.info(f\"Evaluating model '{model_name}'...\")\n",
    "    # Ensure y_eval is a numpy array for consistency\n",
    "    if isinstance(y_eval, pd.Series):\n",
    "        y_eval = y_eval.values\n",
    "\n",
    "    is_keras_wrapper = isinstance(model, KerasClassifier)\n",
    "\n",
    "    try:\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_eval)\n",
    "        \n",
    "        # --- ADDED: Handle CatBoost prediction type ---\n",
    "        # Check if the model is CatBoost AND predictions are not integers\n",
    "        is_catboost = 'CatBoostClassifier' in str(type(model)) # Simple check\n",
    "        if is_catboost and not np.issubdtype(y_pred.dtype, np.integer):\n",
    "            logger.warning(f\"CatBoost predictions seem non-integer ({y_pred.dtype}): {y_pred[:5]}. Attempting mapping to encoded labels.\")\n",
    "            # Create a mapping from original string labels -> encoded integers\n",
    "            label_to_encoded = {label: i for i, label in enumerate(le.classes_)}\n",
    "            try:\n",
    "                # Attempt direct mapping (if CatBoost predicts original labels like 'low')\n",
    "                y_pred_mapped = np.array([label_to_encoded.get(str(p), -1) for p in y_pred.flatten()])\n",
    "                if np.any(y_pred_mapped == -1): # Check if direct mapping failed\n",
    "                    logger.warning(\"Direct label mapping failed, trying string-to-int mapping (assuming '0', '1', '2').\")\n",
    "                    # Attempt string-to-int mapping (if CatBoost predicts '0', '1', '2')\n",
    "                    y_pred_mapped = np.array([int(p) for p in y_pred.flatten()])\n",
    "                y_pred = y_pred_mapped # Use the mapped integer predictions\n",
    "                logger.info(f\"Successfully mapped CatBoost predictions to integers: {y_pred[:5]}\")\n",
    "            except Exception as map_err:\n",
    "                logger.error(f\"Failed to map CatBoost predictions to integers: {map_err}. Evaluation/Prediction may fail.\")\n",
    "                # Let the original y_pred pass through, error will likely occur later\n",
    "        # --- END CatBoost Handling ---\n",
    "\n",
    "        # Ensure predictions are class indices (for Keras primarily, now redundant for CatBoost if mapped)\n",
    "        if isinstance(model, KerasClassifier) and y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "            # ... (argmax logic) ...\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        elif not np.issubdtype(y_pred.dtype, np.integer):\n",
    "             logger.error(f\"Predictions for {model_name} are not integers after processing: {y_pred.dtype}. Evaluation may fail.\")\n",
    "             # Optionally try to force conversion, but it's risky\n",
    "             # y_pred = y_pred.astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_eval, y_pred)\n",
    "\n",
    "        # Generate classification report and confusion matrix\n",
    "        try:\n",
    "            y_eval_labels = le.inverse_transform(y_eval)\n",
    "            y_pred_labels = le.inverse_transform(y_pred)\n",
    "            target_names = le.classes_ # Get class names in correct order\n",
    "        except Exception as le_error:\n",
    "            logger.warning(f\"LabelEncoder inverse_transform failed for '{model_name}': {le_error}. Using numeric labels for report.\")\n",
    "            y_eval_labels = y_eval\n",
    "            y_pred_labels = y_pred\n",
    "            target_names = [str(i) for i in sorted(np.unique(y_eval))]\n",
    "\n",
    "        # Classification Report\n",
    "        report_str = classification_report(y_eval_labels, y_pred_labels, target_names=target_names, zero_division=0)\n",
    "        report_dict = classification_report(y_eval_labels, y_pred_labels, target_names=target_names, output_dict=True, zero_division=0)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_eval_labels, y_pred_labels, labels=target_names)\n",
    "\n",
    "        logger.info(f\"Evaluation Results for '{model_name}':\")\n",
    "        logger.info(f\"  Accuracy: {accuracy:.5f}\")\n",
    "\n",
    "        # Ensure results directory exists\n",
    "        results_dir = 'results'\n",
    "        if not os.path.exists(results_dir): os.makedirs(results_dir)\n",
    "\n",
    "        # Save evaluation results to a file\n",
    "        eval_filename = os.path.join(results_dir, f'{model_name}_evaluation_{timestamp}.txt')\n",
    "        with open(eval_filename, 'w') as f:\n",
    "            f.write(f\"Model Evaluation Summary\\n=========================\\n\")\n",
    "            f.write(f\"Model Name: {model_name}\\nTimestamp: {timestamp}\\nAccuracy: {accuracy:.5f}\\n\\n\")\n",
    "            f.write(\"Classification Report:\\n\"); f.write(report_str)\n",
    "            f.write(\"\\n\\nConfusion Matrix:\\n\"); f.write(np.array2string(cm))\n",
    "        logger.info(f\"Saved evaluation summary: {eval_filename}\")\n",
    "\n",
    "        # Ensure plots directory exists\n",
    "        plot_dir = 'plots'\n",
    "        if not os.path.exists(plot_dir): os.makedirs(plot_dir)\n",
    "\n",
    "        # Save confusion matrix plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "        plt.xlabel('Predicted Label'); plt.ylabel('True Label')\n",
    "        plt.title(f'Confusion Matrix - {model_name} (Accuracy: {accuracy:.3f})')\n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(plot_dir, f'{model_name}_confusion_matrix_{timestamp}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close() # Close the plot\n",
    "        logger.info(f\"Saved confusion matrix plot: {plot_filename}\")\n",
    "\n",
    "        return accuracy, report_dict\n",
    "\n",
    "    except AttributeError as ae:\n",
    "         if 'predict' in str(ae): logger.error(f\"Evaluation error '{model_name}': Model not fitted? AttrErr: {ae}\", exc_info=True)\n",
    "         else: logger.error(f\"AttributeError during eval '{model_name}': {ae}\", exc_info=True)\n",
    "         return None, None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during eval '{model_name}': {e}\", exc_info=True)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab564437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import joblib\n",
    "import os\n",
    "from scikeras.wrappers import KerasClassifier # Keep for isinstance check\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def make_test_predictions(model, X_test, test_obs, timestamp, model_name, le):\n",
    "    \"\"\"Generates predictions on the test set and saves results.\"\"\"\n",
    "    logger.info(f\"Generating test predictions using {model_name}...\")\n",
    "\n",
    "    if model is None:\n",
    "        logger.error(f\"Predict fail {model_name}: model None.\")\n",
    "        return None\n",
    "\n",
    "    # Handle missing LabelEncoder (with fallback)\n",
    "    if le is None:\n",
    "        logger.warning(f\"LE None for {model_name}. Try fallback...\")\n",
    "        try:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "            if encoder_files:\n",
    "                le=joblib.load(f'features/{encoder_files[-1]}')\n",
    "                logger.info(f\"Loaded fallback LE: {encoder_files[-1]}.\")\n",
    "            else:\n",
    "                logger.error(f\"Predict fail: LE None, no fallback.\")\n",
    "                return None\n",
    "        except Exception as load_e:\n",
    "            logger.error(f\"LE None, fallback load fail:{load_e}. No predict.\")\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Make predictions (potentially encoded integers or strings from CatBoost)\n",
    "        y_pred_raw = model.predict(X_test)\n",
    "\n",
    "        # --- ADDED: Handle CatBoost prediction type ---\n",
    "        y_pred_enc = y_pred_raw # Default assumption\n",
    "        is_catboost = 'CatBoostClassifier' in str(type(model))\n",
    "        if is_catboost and not np.issubdtype(y_pred_raw.dtype, np.integer):\n",
    "            logger.warning(f\"CatBoost test predictions non-integer ({y_pred_raw.dtype}): {y_pred_raw[:5]}. Attempting mapping.\")\n",
    "            label_to_encoded = {label: i for i, label in enumerate(le.classes_)}\n",
    "            try:\n",
    "                y_pred_mapped = np.array([label_to_encoded.get(str(p), -1) for p in y_pred_raw.flatten()])\n",
    "                if np.any(y_pred_mapped == -1):\n",
    "                    logger.warning(\"Direct label mapping failed, trying string-to-int mapping.\")\n",
    "                    y_pred_mapped = np.array([int(p) for p in y_pred_raw.flatten()])\n",
    "                y_pred_enc = y_pred_mapped # Use mapped integer predictions\n",
    "                logger.info(f\"Mapped CatBoost test predictions to integers: {y_pred_enc[:5]}\")\n",
    "            except Exception as map_err:\n",
    "                logger.error(f\"Failed map CatBoost test predictions: {map_err}. Prediction may fail.\")\n",
    "                # Use original predictions; inverse_transform will likely fail\n",
    "                y_pred_enc = y_pred_raw\n",
    "        # --- END CatBoost Handling ---\n",
    "\n",
    "        # Ensure predictions are class indices (for Keras)\n",
    "        if isinstance(model, KerasClassifier) and y_pred_enc.ndim > 1 and y_pred_enc.shape[1] > 1:\n",
    "            y_pred_enc = np.argmax(y_pred_enc, axis=1)\n",
    "        elif not np.issubdtype(y_pred_enc.dtype, np.integer):\n",
    "             logger.error(f\"Test predictions for {model_name} not integers after processing: {y_pred_enc.dtype}. Inverse transform likely to fail.\")\n",
    "             # Cannot proceed reliably if predictions aren't integer encoded labels here\n",
    "\n",
    "        # Inverse transform to get original salary category labels\n",
    "        predicted_labels = le.inverse_transform(y_pred_enc) # This now expects integer y_pred_enc\n",
    "\n",
    "\n",
    "        # Create submission DataFrame\n",
    "        submission_df = pd.DataFrame({'obs': test_obs, 'salary_category': predicted_labels})\n",
    "\n",
    "        # Sanitize model name for filename\n",
    "        safe_model_name = model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "        # Ensure directories exist\n",
    "        submission_dir = 'submissions'\n",
    "        results_dir = 'results'\n",
    "        if not os.path.exists(submission_dir): os.makedirs(submission_dir)\n",
    "        if not os.path.exists(results_dir): os.makedirs(results_dir)\n",
    "\n",
    "        submission_path = os.path.join(submission_dir, f'solution_{safe_model_name}_{timestamp}.csv')\n",
    "\n",
    "        # Save submission file\n",
    "        submission_df.to_csv(submission_path, index=False)\n",
    "        logger.info(f\"Saved submission: {submission_path}\")\n",
    "\n",
    "        # Log value counts of predictions for analysis\n",
    "        pred_value_counts = submission_df['salary_category'].value_counts().to_dict()\n",
    "        logger.info(f\"Test prediction distribution for '{model_name}': {pred_value_counts}\")\n",
    "\n",
    "        # Save prediction summary\n",
    "        summary_filename = os.path.join(results_dir, f'{safe_model_name}_test_prediction_summary_{timestamp}.txt')\n",
    "        with open(summary_filename, 'w') as f:\n",
    "            f.write(f\"Test Prediction Summary\\n======================\\n\")\n",
    "            f.write(f\"Model Name: {model_name}\\n\")\n",
    "            try: f.write(f\"Model Class: {model.__class__.__name__}\\n\")\n",
    "            except: f.write(f\"Model Class: N/A\\n\")\n",
    "            f.write(f\"Timestamp: {timestamp}\\nTotal Predictions: {len(predicted_labels)}\\n\\nDistribution:\\n\")\n",
    "            total_preds = len(predicted_labels)\n",
    "            if total_preds > 0:\n",
    "                for label, count in sorted(pred_value_counts.items()):\n",
    "                    percentage = (count / total_preds) * 100\n",
    "                    f.write(f\"- {label}: {count} ({percentage:.2f}%)\\n\")\n",
    "            else:\n",
    "                f.write(\"- No predictions were generated.\\n\")\n",
    "        logger.info(f\"Saved test prediction summary: {summary_filename}\")\n",
    "\n",
    "        return submission_df\n",
    "\n",
    "    except AttributeError as ae:\n",
    "        logger.error(f\"AttributeError during predict/inverse_transform '{model_name}': {ae}.\", exc_info=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during test prediction '{model_name}': {e}\", exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc317202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier # Keep for FS\n",
    "# Imports for models used within the pipeline are assumed to be at the top of the file\n",
    "# Imports for utility functions are assumed to be at the top of the file\n",
    "import os\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "# Assume utility functions (create_directory_structure, get_timestamp) are defined\n",
    "# Assume preprocessing function (preprocess_data) is defined\n",
    "# Assume model optimization function (optimize_model) is defined\n",
    "# Assume ensemble function (create_ensemble) is defined\n",
    "# Assume evaluation function (evaluate_model) is defined\n",
    "# Assume prediction function (make_test_predictions) is defined\n",
    "\n",
    "def run_complete_pipeline(perform_feature_selection=False, min_cv_score_threshold=0.72, fs_threshold='mean', n_jobs_sklearn=1):\n",
    "    \"\"\"\n",
    "    Run the complete model training pipeline with combined FE logic,\n",
    "    updated model list, and class weight balancing strategy integrated\n",
    "    into optimize_model.\n",
    "    \"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    main_log_file = None\n",
    "    file_handler = None\n",
    "    pipeline_success = False # Track overall success\n",
    "\n",
    "    try:\n",
    "        # 1. Setup\n",
    "        print(\"--- Starting Complete Pipeline Run (Class Weights, LGBM added) ---\")\n",
    "        create_directory_structure()\n",
    "        main_log_file = f'logs/pipeline_run_{timestamp}.log'\n",
    "        file_handler = logging.FileHandler(main_log_file)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        if file_handler.baseFilename not in [h.baseFilename for h in logger.handlers if isinstance(h, logging.FileHandler)]:\n",
    "             logger.addHandler(file_handler)\n",
    "\n",
    "        logger.info(f\"--- Starting Complete Pipeline Run --- Timestamp: {timestamp} ---\")\n",
    "        logger.info(f\"Pipeline Config: Combined FE, Scaling=True, FeatSelect={perform_feature_selection} (Thresh={fs_threshold}), CV Thresh={min_cv_score_threshold}, n_jobs={n_jobs_sklearn} used, Class Weights Enabled, Const Cols Kept\")\n",
    "        logger.info(f\"Logging detailed output to: {main_log_file}\")\n",
    "\n",
    "        # 2. Load Data\n",
    "        logger.info(\"Loading data...\")\n",
    "        try: train_df = pd.read_csv('train.csv'); test_df = pd.read_csv('test.csv'); logger.info(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "        except FileNotFoundError as e: logger.error(f\"Data load error: {e}.\"); raise\n",
    "        if 'salary_category' not in train_df.columns: logger.error(\"Target missing.\"); raise ValueError(\"Missing target\")\n",
    "        if train_df.empty or test_df.empty: logger.error(\"Data empty.\"); raise ValueError(\"Empty data\")\n",
    "        if 'obs' not in test_df.columns: logger.error(\"Column 'obs' missing in test.csv.\"); raise ValueError(\"Missing obs\")\n",
    "        all_states = set(train_df['job_state'].dropna().unique()).union(set(test_df['job_state'].dropna().unique()))\n",
    "        all_feature1 = set(train_df['feature_1'].dropna().unique()).union(set(test_df['feature_1'].dropna().unique()))\n",
    "        logger.info(f\"Found {len(all_states)} unique states and {len(all_feature1)} unique feature_1 values.\")\n",
    "\n",
    "        # 3. Preprocess Training Data\n",
    "        logger.info(\"Preprocessing training data (using combined logic)...\")\n",
    "        X_train_orig, y_train_orig, feature_cols_initial, label_encoder = preprocess_data(train_df, all_states, all_feature1, timestamp, is_training=True)\n",
    "        if X_train_orig is None or y_train_orig is None or label_encoder is None or feature_cols_initial is None: logger.error(\"Train preprocess failed.\"); raise RuntimeError(\"Preprocessing train failed\")\n",
    "        logger.info(f\"Train preprocess done. Initial Feats: {X_train_orig.shape[1]}\"); y_train_orig = pd.Series(y_train_orig)\n",
    "\n",
    "        # 4. Train/Validation Split\n",
    "        logger.info(\"Splitting data (80/20)...\")\n",
    "        X_train_full, X_val, y_train_full, y_val = train_test_split(X_train_orig, y_train_orig, test_size=0.20, random_state=42, stratify=y_train_orig)\n",
    "        logger.info(f\"Train (Pre-scale): {X_train_full.shape}, Val (Pre-scale): {X_val.shape}\"); y_train_full = pd.Series(y_train_full, index=X_train_full.index); y_val = pd.Series(y_val, index=X_val.index)\n",
    "\n",
    "        # 5. SCALING STEP\n",
    "        logger.info(\"Applying StandardScaler...\")\n",
    "        scaler = StandardScaler(); X_train_full_scaled = scaler.fit_transform(X_train_full[feature_cols_initial]); X_val_scaled = scaler.transform(X_val[feature_cols_initial])\n",
    "        X_train_full_scaled = pd.DataFrame(X_train_full_scaled, index=X_train_full.index, columns=feature_cols_initial)\n",
    "        X_val_scaled = pd.DataFrame(X_val_scaled, index=X_val.index, columns=feature_cols_initial)\n",
    "        scaler_path = f'scalers/scaler_{timestamp}.joblib'; joblib.dump(scaler, scaler_path); logger.info(f\"Scaler saved: {scaler_path}\")\n",
    "\n",
    "        # 6. Preprocess & Scale Test Data\n",
    "        logger.info(\"Preprocessing test data (using combined logic)...\")\n",
    "        X_test_orig, _, _, _ = preprocess_data(test_df, all_states, all_feature1, timestamp, is_training=False, feature_columns_to_use=feature_cols_initial)\n",
    "        if X_test_orig is None: logger.error(\"Test preprocess failed.\"); raise RuntimeError(\"Preprocessing test failed\")\n",
    "        try: X_test_aligned = X_test_orig[feature_cols_initial]; logger.info(\"Test columns aligned.\")\n",
    "        except KeyError as ke: logger.error(f\"Test col mismatch after preprocess: {ke}.\"); raise RuntimeError(f\"Test column mismatch: {ke}\")\n",
    "        logger.info(\"Scaling test data...\"); X_test_scaled = scaler.transform(X_test_aligned)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test_aligned.index, columns=feature_cols_initial)\n",
    "        logger.info(f\"Test preprocess & scale done. Shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        # Define Data Partitions\n",
    "        X_opt_train = X_train_full_scaled.copy(); y_opt_train = y_train_full.copy(); X_holdout_val = X_val_scaled.copy(); y_holdout_val = y_val.copy(); X_final_test = X_test_scaled.copy()\n",
    "        current_feature_cols = list(feature_cols_initial)\n",
    "\n",
    "        # 7. Optional Feature Selection\n",
    "        if perform_feature_selection:\n",
    "            logger.info(f\"Performing feature selection (Threshold: {fs_threshold})...\")\n",
    "            try:\n",
    "                selector_model = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=n_jobs_sklearn, class_weight='balanced', max_depth=20)\n",
    "                logger.info(\"Fitting RF for feature selection...\"); selector_model.fit(X_opt_train, y_opt_train)\n",
    "                selector = SelectFromModel(selector_model, threshold=fs_threshold, prefit=True)\n",
    "                selected_mask = selector.get_support(); selected_features = X_opt_train.columns[selected_mask]\n",
    "                num_orig = X_opt_train.shape[1]; num_sel = len(selected_features)\n",
    "                if num_sel == 0: logger.error(\"FS removed ALL features!\"); raise RuntimeError(\"FS removed all features.\")\n",
    "                elif num_sel < num_orig:\n",
    "                    num_removed = num_orig - num_sel; logger.info(f\"Feat selection removed {num_removed} features. Selected {num_sel}.\")\n",
    "                    current_feature_cols = list(selected_features)\n",
    "                    X_opt_train = X_opt_train[current_feature_cols]; X_holdout_val = X_holdout_val[current_feature_cols]; X_final_test = X_final_test[current_feature_cols]\n",
    "                    logger.info(f\"Selection applied to train/val/test partitions.\")\n",
    "                    joblib.dump(current_feature_cols, f'features/selected_feature_columns_{timestamp}.joblib')\n",
    "                else: logger.info(f\"Feature selection removed no features with threshold '{fs_threshold}'.\"); perform_feature_selection = False\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error feature selection: {e}. Use all scaled.\", exc_info=True); perform_feature_selection = False\n",
    "                current_feature_cols = list(feature_cols_initial); X_opt_train=X_train_full_scaled[current_feature_cols]; X_holdout_val=X_val_scaled[current_feature_cols]; X_final_test=X_test_scaled[current_feature_cols]\n",
    "        else:\n",
    "            logger.info(\"Skipping feature selection.\")\n",
    "            X_opt_train = X_opt_train[current_feature_cols]; X_holdout_val = X_holdout_val[current_feature_cols]; X_final_test = X_final_test[current_feature_cols]\n",
    "\n",
    "        logger.info(f\"Data shapes post-scaling/selection: Train={X_opt_train.shape}, Val={X_holdout_val.shape}, Test={X_final_test.shape}\")\n",
    "        logger.info(f\"Number of features used in modeling: {len(current_feature_cols)}\")\n",
    "\n",
    "        # 8. Optimize, Train Base Models & Make Individual Predictions\n",
    "        # --- UPDATED LIST (No SVC, Added LightGBM) ---\n",
    "        models_to_optimize = [\n",
    "             ('adaboost', 100),\n",
    "            ('catboost', 100), ('xgboost', 100), \n",
    "            ('lightgbm', 100),\n",
    "            ('randomforest', 100), ('extratrees', 100),\n",
    "            ('gradientboosting', 100)\n",
    "        ]\n",
    "        qualified_models_with_scores = [] # Stores (name, fitted_model, cv_score)\n",
    "        optimized_params_all = {} # Stores best params found by Optuna\n",
    "\n",
    "        logger.info(f\"--- Optimizing models (Class Weights Enabled Where Applicable) ---\")\n",
    "        logger.info(f\"Minimum CV score threshold for qualification: {min_cv_score_threshold}\")\n",
    "        logger.info(f\"Optimization Order: {[m[0] for m in models_to_optimize]}\")\n",
    "\n",
    "        for model_name, n_trials in models_to_optimize:\n",
    "            indiv_sub_df = None\n",
    "            try:\n",
    "                logger.info(f\"--- Optimizing {model_name} ({n_trials} trials) ---\")\n",
    "                # Optimize_model now handles final training with class_weight where applicable\n",
    "                final_model, best_cv_score, best_params = optimize_model(\n",
    "                    X_opt_train, y_opt_train, timestamp, model_name,\n",
    "                    n_trials=n_trials, n_jobs_optuna=n_jobs_sklearn\n",
    "                )\n",
    "\n",
    "                # Log details for qualification debugging\n",
    "                log_score = best_cv_score if best_cv_score is not None else -1.0\n",
    "                comparison_result = best_cv_score >= min_cv_score_threshold if best_cv_score is not None else False\n",
    "                logger.info(f\"Qualification Check for {model_name}: \"\n",
    "                            f\"final_model exists? {final_model is not None}, \"\n",
    "                            f\"best_cv_score={log_score:.7f}, \"\n",
    "                            f\"threshold={min_cv_score_threshold}, \"\n",
    "                            f\"comparison result: {comparison_result}\")\n",
    "\n",
    "                # Qualification Check\n",
    "                if final_model is not None and best_cv_score is not None and best_cv_score >= min_cv_score_threshold:\n",
    "                    logger.info(f\"+++ QUALIFIED: {model_name} (CV Score: {best_cv_score:.5f})\")\n",
    "                    # Evaluate on Holdout\n",
    "                    holdout_acc, _ = evaluate_model(final_model, X_holdout_val, y_holdout_val, f\"{model_name}_qualified_holdout_eval\", timestamp, label_encoder)\n",
    "                    if holdout_acc is not None:\n",
    "                        logger.info(f\"Hold-out Acc ({model_name}): {holdout_acc:.5f}\")\n",
    "                        # Store model with HOLD-OUT score for potential ensemble weighting later\n",
    "                        qualified_models_with_scores.append((model_name, final_model, holdout_acc)) # Store HOLD-OUT acc\n",
    "                    else:\n",
    "                        logger.warning(f\"Hold-out Eval failed for {model_name}. Cannot use its score for weighting.\")\n",
    "                        # Still add model? Maybe add with CV score as fallback weight? Or exclude?\n",
    "                        # Let's add with CV score for now, but this case needs consideration.\n",
    "                        qualified_models_with_scores.append((model_name, final_model, best_cv_score))\n",
    "\n",
    "                    if best_params: optimized_params_all[model_name] = best_params\n",
    "\n",
    "                    # Generate Individual Predictions\n",
    "                    logger.info(f\"--- Generating individual predictions for {model_name} ---\")\n",
    "                    indiv_sub_df = make_test_predictions(final_model, X_final_test, test_df['obs'], timestamp, f\"{model_name}_qual_individual_pred\", label_encoder)\n",
    "                    if indiv_sub_df is None: logger.error(f\"Failed individual predictions for {model_name}.\")\n",
    "                    else: logger.info(f\"Individual prediction file saved for {model_name}.\")\n",
    "\n",
    "                elif best_cv_score is not None: # Didn't meet threshold or final fit failed\n",
    "                     logger.info(f\"--- NOT QUALIFIED: {model_name} (CV Score: {best_cv_score:.5f} {' - Final model fit/save failed' if final_model is None else ''}) ---\")\n",
    "                     if best_params: optimized_params_all[model_name] = best_params # Still save params\n",
    "                else: # Optuna itself failed or returned None score\n",
    "                    logger.warning(f\"Optimization failed or returned invalid score for {model_name}. Skip.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in main optimization loop for {model_name}: {e}\", exc_info=True)\n",
    "\n",
    "        # --- Post-Optimization Check ---\n",
    "        logger.info(\"--- Model Optimization Phase Complete ---\")\n",
    "        if not qualified_models_with_scores:\n",
    "            logger.error(f\"CRITICAL: NO models met CV threshold {min_cv_score_threshold}. Aborting.\")\n",
    "            if file_handler: logger.removeHandler(file_handler); file_handler.close()\n",
    "            return False\n",
    "        logger.info(f\"--- {len(qualified_models_with_scores)} models qualified. ---\")\n",
    "        # Log based on HOLD-OUT score now stored\n",
    "        logger.info(f\"Qualified Models (Name, Holdout Acc/CV Score): {[(m[0], f'{m[2]:.5f}') for m in qualified_models_with_scores]}\")\n",
    "\n",
    "        # 9. Create Ensembles & Select FINAL Best Model\n",
    "        final_model = None; final_model_name = \"N/A\"; vote_ens = None; stack_ens = None; best_ind_q_model = None\n",
    "\n",
    "        if len(qualified_models_with_scores) == 1:\n",
    "            # Use holdout score here too\n",
    "            final_model_name, final_model, final_holdout_score = qualified_models_with_scores[0]\n",
    "            logger.warning(f\"Only 1 qualified: {final_model_name} (Holdout Acc: {final_holdout_score:.5f}). Select it.\")\n",
    "            best_ind_q_model = final_model\n",
    "        elif len(qualified_models_with_scores) > 1:\n",
    "            logger.info(f\"--- Creating and Evaluating Ensembles (using balanced models if applicable) ---\")\n",
    "            # Pass potentially balanced models to create_ensemble\n",
    "            # create_ensemble now uses LGBM meta-learner by default\n",
    "            vote_ens, stack_ens, best_ind_q_model_obj, dynamic_ens = create_ensemble(\n",
    "                        qualified_models_with_scores, X_opt_train, y_opt_train, timestamp, n_jobs_ensemble=n_jobs_sklearn)\n",
    "\n",
    "            # Evaluate candidate ensembles and best individual on HOLD-OUT set\n",
    "            logger.info(\"--- Evaluating candidate final models on HOLD-OUT validation set ---\")\n",
    "            candidates = {} # Store {name: (holdout_accuracy, model_object)}\n",
    "            best_ind_name_from_ensemble = None # Track name from ensemble function\n",
    "\n",
    "            if vote_ens:\n",
    "                vote_model_name = f\"voting_ensemble_{vote_ens.voting}_qualified\"\n",
    "                logger.info(f\"--- Eval {vote_model_name} ---\")\n",
    "                val_acc, _ = evaluate_model(vote_ens, X_holdout_val, y_holdout_val, f\"{vote_model_name}_holdout_eval\", timestamp, label_encoder)\n",
    "                if val_acc is not None: candidates[vote_model_name] = (val_acc, vote_ens); logger.info(f\"Hold-out Acc ({vote_model_name}): {val_acc:.5f}\")\n",
    "                else: logger.warning(f\"Eval fail: {vote_model_name}\")\n",
    "\n",
    "            if stack_ens:\n",
    "                stack_meta_name = stack_ens.final_estimator_.__class__.__name__\n",
    "                stack_model_name = f\"stacking_ensemble_{stack_meta_name}_qualified\"\n",
    "                logger.info(f\"--- Eval {stack_model_name} ---\")\n",
    "                val_acc, _ = evaluate_model(stack_ens, X_holdout_val, y_holdout_val, f\"{stack_model_name}_holdout_eval\", timestamp, label_encoder)\n",
    "                if val_acc is not None: candidates[stack_model_name] = (val_acc, stack_ens); logger.info(f\"Hold-out Acc ({stack_model_name}): {val_acc:.5f}\")\n",
    "                else: logger.warning(f\"Eval fail: {stack_model_name}\")\n",
    "                \n",
    "            # Add evaluation for dynamic ensemble\n",
    "            if dynamic_ens:\n",
    "                dynamic_model_name = \"dynamic_ensemble_qualified\"\n",
    "                logger.info(f\"--- Eval {dynamic_model_name} ---\")\n",
    "                val_acc, _ = evaluate_model(dynamic_ens, X_holdout_val, y_holdout_val, f\"{dynamic_model_name}_holdout_eval\", timestamp, label_encoder)\n",
    "                if val_acc is not None:\n",
    "                    candidates[dynamic_model_name] = (val_acc, dynamic_ens)\n",
    "                    logger.info(f\"Hold-out Acc ({dynamic_model_name}): {val_acc:.5f}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Eval fail: {dynamic_model_name}\")\n",
    "\n",
    "            # Find the best individual model's info again to evaluate it\n",
    "            if qualified_models_with_scores:\n",
    "                 # Sort by stored score (holdout acc or fallback CV score)\n",
    "                 best_ind_info = max(qualified_models_with_scores, key=lambda item: item[2])\n",
    "                 best_ind_name_from_list = best_ind_info[0]\n",
    "                 best_ind_q_model = best_ind_info[1] # The actual best model object\n",
    "                 best_ind_stored_score = best_ind_info[2]\n",
    "                 logger.info(f\"--- Eval Best Individual ({best_ind_name_from_list}, Stored Score: {best_ind_stored_score:.5f}) ---\")\n",
    "                 eval_name = f\"{best_ind_name_from_list}_best_qual_holdout_eval\"\n",
    "                 val_acc, _ = evaluate_model(best_ind_q_model, X_holdout_val, y_holdout_val, eval_name, timestamp, label_encoder)\n",
    "                 if val_acc is not None:\n",
    "                     cand_name = f\"{best_ind_name_from_list}_best_qualified\"\n",
    "                     candidates[cand_name] = (val_acc, best_ind_q_model)\n",
    "                     logger.info(f\"Hold-out Acc ({best_ind_name_from_list}): {val_acc:.5f}\")\n",
    "                 else: logger.warning(f\"Hold-out Eval failed for {best_ind_name_from_list}\")\n",
    "\n",
    "            # Select FINAL model based on highest HOLD-OUT accuracy among candidates\n",
    "            if candidates:\n",
    "                final_model_name = max(candidates, key=lambda k: candidates[k][0])\n",
    "                final_val_score, final_model = candidates[final_model_name]\n",
    "                logger.info(f\"--- FINAL MODEL SELECTION ---\")\n",
    "                logger.info(f\"Selected '{final_model_name}' as FINAL model (Hold-Out Acc: {final_val_score:.5f})\")\n",
    "            else:\n",
    "                logger.error(\"Hold-out evaluation failed for all candidates.\")\n",
    "                # Fallback: Use best individual based on stored score (holdout or CV)\n",
    "                if best_ind_q_model and best_ind_name_from_list:\n",
    "                    final_model = best_ind_q_model\n",
    "                    final_model_name = f\"{best_ind_name_from_list}_best_qualified_fallback\"\n",
    "                    logger.warning(f\"FALLBACK: Using best individual model '{final_model_name}' based on its stored score.\")\n",
    "                else:\n",
    "                    logger.error(\"Could not determine final model even as fallback. Aborting.\")\n",
    "                    raise RuntimeError(\"Final model selection failed.\")\n",
    "\n",
    "        # Check if a final model was successfully selected\n",
    "        if not final_model:\n",
    "            logger.error(\"No final model could be selected. Aborting.\")\n",
    "            raise RuntimeError(\"Final model selection failed.\")\n",
    "\n",
    "        # 10. Make FINAL Test Predictions\n",
    "        logger.info(f\"--- Generating FINAL predictions using: {final_model_name} ---\")\n",
    "        final_sub_df = make_test_predictions(final_model, X_final_test, test_df['obs'], timestamp, f\"{final_model_name}_FINAL\", label_encoder)\n",
    "        if final_sub_df is None:\n",
    "            logger.error(f\"Failed FINAL submission with {final_model_name}.\")\n",
    "            # Consider if pipeline should fail here\n",
    "            pipeline_success = False # Mark as failed if submission fails\n",
    "        else:\n",
    "            logger.info(f\"FINAL submission file generated with {final_model_name}.\")\n",
    "            pipeline_success = True # Mark successful only if prediction works\n",
    "\n",
    "        # 11. Final Summary\n",
    "        logger.info(\"--- Pipeline Run Summary ---\")\n",
    "        logger.info(f\"Timestamp: {timestamp}\")\n",
    "        logger.info(f\"Config: Combined FE, Scaling=True, FeatSelect={perform_feature_selection} (Thresh={fs_threshold}), CV Thresh={min_cv_score_threshold}, n_jobs={n_jobs_sklearn}, Class Weights Enabled, Const Cols Kept\")\n",
    "        logger.info(f\"Final # Features: {len(current_feature_cols)}\")\n",
    "        logger.info(\"Models Optimized: \" + \", \".join([m[0] for m in models_to_optimize]))\n",
    "        qual_details = [(m[0], f\"{m[2]:.5f}\") for m in qualified_models_with_scores] if qualified_models_with_scores else [\"None\"]\n",
    "        logger.info(\"Models Qualified (Name, Holdout Acc/CV Score): \" + \", \".join([f\"{n}({s})\" for n, s in qual_details]))\n",
    "        logger.info(f\"Ensembles Created: Voting={'Yes' if vote_ens else 'No'}, Stacking={'Yes' if stack_ens else 'No'} (Meta: {stack_ens.final_estimator_.__class__.__name__ if stack_ens else 'N/A'}), Dynamic={'Yes' if dynamic_ens else 'No'}\")\n",
    "        logger.info(f\"Final model selected: {final_model_name}\")\n",
    "        logger.info(\"Individual predictions saved for qualified models.\")\n",
    "        if final_sub_df is not None:\n",
    "            safe_final_n = final_model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\").replace(\" \", \"_\")\n",
    "            final_sub_path = f\"submissions/solution_{safe_final_n}_FINAL_{timestamp}.csv\"\n",
    "            logger.info(f\"Final submission file: {final_sub_path}\")\n",
    "        else:\n",
    "            logger.warning(\"No FINAL submission file was generated.\")\n",
    "        logger.info(f\"Logs in: {main_log_file}\")\n",
    "        logger.info(f\"--- Pipeline {'Completed Successfully' if pipeline_success else 'Completed with Errors'} ---\")\n",
    "\n",
    "        # Close log handler\n",
    "        if file_handler: logger.removeHandler(file_handler); file_handler.close()\n",
    "        return pipeline_success\n",
    "\n",
    "    # --- Main Exception Handling ---\n",
    "    except Exception as e:\n",
    "        logger.error(f\"--- Pipeline Failed Critically --- Error Type: {type(e).__name__} ---\")\n",
    "        logger.error(f\"Error Message: {e}\", exc_info=True)\n",
    "        # Ensure log handler is closed\n",
    "        if file_handler and file_handler in logger.handlers:\n",
    "            logger.removeHandler(file_handler)\n",
    "            file_handler.close()\n",
    "        return False # Indicate critical failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9d824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:37:17,369 - INFO - Creating directory structure...\n",
      "2025-04-28 15:37:17,369 - INFO - Created directory: models\n",
      "2025-04-28 15:37:17,369 - INFO - Created directory: features\n",
      "2025-04-28 15:37:17,369 - INFO - Created directory: results\n",
      "2025-04-28 15:37:17,369 - INFO - Created directory: submissions\n",
      "2025-04-28 15:37:17,369 - INFO - Created directory: logs\n",
      "2025-04-28 15:37:17,369 - INFO - Created directory: plots\n",
      "2025-04-28 15:37:17,369 - INFO - Created directory: optuna_trials\n",
      "2025-04-28 15:37:17,369 - INFO - Created directory: scalers\n",
      "2025-04-28 15:37:17,369 - INFO - Created directory: calibrated_models\n",
      "2025-04-28 15:37:17,369 - INFO - Directory structure verified/created.\n",
      "2025-04-28 15:37:17,384 - INFO - --- Starting Complete Pipeline Run --- Timestamp: 20250428_153717 ---\n",
      "2025-04-28 15:37:17,385 - INFO - Pipeline Config: Combined FE, Scaling=True, FeatSelect=True (Thresh=mean), CV Thresh=0.72, n_jobs=16 used, Class Weights Enabled, Const Cols Kept\n",
      "2025-04-28 15:37:17,385 - INFO - Logging detailed output to: logs/pipeline_run_20250428_153717.log\n",
      "2025-04-28 15:37:17,385 - INFO - Loading data...\n",
      "2025-04-28 15:37:17,470 - INFO - Train shape: (1280, 317), Test shape: (854, 316)\n",
      "2025-04-28 15:37:17,472 - INFO - Found 39 unique states and 5 unique feature_1 values.\n",
      "2025-04-28 15:37:17,472 - INFO - Preprocessing training data (using combined logic)...\n",
      "2025-04-28 15:37:17,475 - INFO - Starting preprocessing (Combined Logic). Is training: True\n",
      "2025-04-28 15:37:17,476 - INFO - Target 'salary_category' label encoded.\n",
      "2025-04-28 15:37:17,478 - INFO - Saved label encoder and mapping.\n",
      "2025-04-28 15:37:17,478 - INFO - Initial cleaning: Numerical and Boolean Features...\n",
      "2025-04-28 15:37:17,534 - WARNING - Col 'feature_10' has non-0/1 vals (834). Treat as numeric, impute median.\n",
      "2025-04-28 15:37:17,534 - INFO - Starting Feature Engineering...\n",
      "2025-04-28 15:37:17,552 - INFO - Fit/saved TE for job_title_grouped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Complete Pipeline Run (Class Weights, LGBM added) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:37:17,557 - INFO - Processed 'job_title'.\n",
      "2025-04-28 15:37:17,565 - INFO - Processed 'job_posted_date'.\n",
      "2025-04-28 15:37:17,568 - INFO - Added bin f9.\n",
      "2025-04-28 15:37:17,568 - INFO - Added: feature_2_9_interaction\n",
      "2025-04-28 15:37:17,568 - INFO - Added transforms f2 (sq, sqrt, bin).\n",
      "2025-04-28 15:37:17,568 - INFO - Added bool aggs.\n",
      "2025-04-28 15:37:17,568 - INFO - Added: feature_10_8_interaction\n",
      "2025-04-28 15:37:17,568 - INFO - Added new interactions: ['feat2_job_title_encoded', 'feat2_boolsum', 'feat2_recency', 'job_title_encoded_recency']\n",
      "2025-04-28 15:37:17,618 - INFO - Applying PCA (n=15) to job desc...\n",
      "2025-04-28 15:37:17,668 - INFO - Fit/saved PCA.\n",
      "2025-04-28 15:37:17,678 - INFO - Finished job desc features.\n",
      "2025-04-28 15:37:17,678 - INFO - Applying manual OHE for 'job_state' (39 unique).\n",
      "2025-04-28 15:37:17,687 - INFO - Applying manual OHE for 'feature_1' (5 unique).\n",
      "2025-04-28 15:37:17,696 - INFO - Final cleanup and column alignment...\n",
      "2025-04-28 15:37:17,702 - WARNING - Col 'is_senior' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-28 15:37:17,702 - WARNING - Col 'is_junior' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-28 15:37:17,702 - WARNING - Col 'is_developer' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-28 15:37:17,702 - WARNING - Col 'is_specialist' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-28 15:37:17,702 - INFO - Eng col 'feature_9_bin' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,712 - INFO - Eng col 'state_KY' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,714 - INFO - Eng col 'state_UT' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,714 - INFO - Eng col 'state_IA' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,716 - INFO - Eng col 'state_IN' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,717 - INFO - Eng col 'state_OR' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_WA' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - WARNING - Col 'state_WY' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_SC' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_MI' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_TX' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_TN' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_AL' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_MD' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_NM' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_NV' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - WARNING - Col 'state_KS' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_CO' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_VA' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_CT' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - WARNING - Col 'state_RI' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_IL' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_LA' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_NC' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_FL' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_PA' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_OK' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,718 - INFO - Eng col 'state_MN' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,733 - INFO - Eng col 'state_GA' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,733 - INFO - Eng col 'state_CA' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,734 - INFO - Eng col 'state_AR' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,734 - INFO - Eng col 'state_DC' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'state_AK' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'state_OH' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'state_MA' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'state_NY' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'state_SD' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'state_MO' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'state_AZ' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'state_NJ' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'feat1_D' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'feat1_C' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'feat1_A' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'feat1_B' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Eng col 'feat1_E' low card (nunique=2) in train.\n",
      "2025-04-28 15:37:17,735 - INFO - Saved 100 feature names (const cols NOT dropped).\n",
      "2025-04-28 15:37:17,735 - INFO - Preprocessing train done. Shape: (1280, 100). Time: 0.26s\n",
      "2025-04-28 15:37:17,750 - INFO - Train preprocess done. Initial Feats: 100\n",
      "2025-04-28 15:37:17,750 - INFO - Splitting data (80/20)...\n",
      "2025-04-28 15:37:17,751 - INFO - Train (Pre-scale): (1024, 100), Val (Pre-scale): (256, 100)\n",
      "2025-04-28 15:37:17,751 - INFO - Applying StandardScaler...\n",
      "2025-04-28 15:37:17,763 - INFO - Scaler saved: scalers/scaler_20250428_153717.joblib\n",
      "2025-04-28 15:37:17,763 - INFO - Preprocessing test data (using combined logic)...\n",
      "2025-04-28 15:37:17,763 - INFO - Starting preprocessing (Combined Logic). Is training: False\n",
      "2025-04-28 15:37:17,767 - INFO - Loaded LE: label_encoder_20250428_153717.joblib\n",
      "2025-04-28 15:37:17,768 - INFO - Initial cleaning: Numerical and Boolean Features...\n",
      "2025-04-28 15:37:17,818 - WARNING - Col 'feature_10' has non-0/1 vals (540). Treat as numeric, impute median.\n",
      "2025-04-28 15:37:17,818 - INFO - Starting Feature Engineering...\n",
      "2025-04-28 15:37:17,833 - INFO - Loaded TE: features/job_title_encoder_20250428_153717.joblib\n",
      "2025-04-28 15:37:17,834 - INFO - Processed 'job_title'.\n",
      "2025-04-28 15:37:17,839 - INFO - Processed 'job_posted_date'.\n",
      "2025-04-28 15:37:17,839 - INFO - Added bin f9.\n",
      "2025-04-28 15:37:17,850 - INFO - Added: feature_2_9_interaction\n",
      "2025-04-28 15:37:17,853 - INFO - Added transforms f2 (sq, sqrt, bin).\n",
      "2025-04-28 15:37:17,854 - INFO - Added bool aggs.\n",
      "2025-04-28 15:37:17,854 - INFO - Added: feature_10_8_interaction\n",
      "2025-04-28 15:37:17,854 - INFO - Added new interactions: ['feat2_job_title_encoded', 'feat2_boolsum', 'feat2_recency', 'job_title_encoded_recency']\n",
      "2025-04-28 15:37:17,884 - INFO - Applying PCA (n=15) to job desc...\n",
      "2025-04-28 15:37:17,901 - INFO - Loaded PCA: features/job_desc_pca_20250428_153717.joblib\n",
      "2025-04-28 15:37:17,918 - INFO - Finished job desc features.\n",
      "2025-04-28 15:37:17,918 - INFO - Applying manual OHE for 'job_state' (39 unique).\n",
      "2025-04-28 15:37:17,918 - INFO - Applying manual OHE for 'feature_1' (5 unique).\n",
      "2025-04-28 15:37:17,933 - INFO - Final cleanup and column alignment...\n",
      "2025-04-28 15:37:17,967 - INFO - Preprocessing test done. Shape: (854, 100). Time: 0.20s\n",
      "2025-04-28 15:37:17,968 - INFO - Test columns aligned.\n",
      "2025-04-28 15:37:17,968 - INFO - Scaling test data...\n",
      "2025-04-28 15:37:17,968 - INFO - Test preprocess & scale done. Shape: (854, 100)\n",
      "2025-04-28 15:37:17,968 - INFO - Performing feature selection (Threshold: mean)...\n",
      "2025-04-28 15:37:17,968 - INFO - Fitting RF for feature selection...\n",
      "2025-04-28 15:37:18,177 - INFO - Feat selection removed 60 features. Selected 40.\n",
      "2025-04-28 15:37:18,177 - INFO - Selection applied to train/val/test partitions.\n",
      "2025-04-28 15:37:18,180 - INFO - Data shapes post-scaling/selection: Train=(1024, 40), Val=(256, 40), Test=(854, 40)\n",
      "2025-04-28 15:37:18,180 - INFO - Number of features used in modeling: 40\n",
      "2025-04-28 15:37:18,180 - INFO - --- Optimizing models (Class Weights Enabled Where Applicable) ---\n",
      "2025-04-28 15:37:18,182 - INFO - Minimum CV score threshold for qualification: 0.72\n",
      "2025-04-28 15:37:18,182 - INFO - Optimization Order: ['adaboost', 'catboost', 'xgboost', 'lightgbm', 'randomforest', 'extratrees', 'gradientboosting']\n",
      "2025-04-28 15:37:18,183 - INFO - --- Optimizing adaboost (100 trials) ---\n",
      "2025-04-28 15:37:18,184 - INFO - Starting adaboost optimization (100 trials)...\n",
      "2025-04-28 15:37:18,184 - INFO - Optuna timeout for adaboost: 3600s.\n",
      "[I 2025-04-28 15:37:18,745] A new study created in RDB with name: adaboost_opt_20250428_153717\n",
      "2025-04-28 15:37:18,751 - INFO - Setting Optuna timeout 3600s.\n",
      "[I 2025-04-28 15:37:26,567] Trial 0 finished with value: 0.6787183165949306 and parameters: {'base_estimator_max_depth': 2, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 4850, 'learning_rate': 0.05513546919271016}. Best is trial 0 with value: 0.6787183165949306.\n",
      "[I 2025-04-28 15:37:49,297] Trial 1 finished with value: 0.5654328072692492 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 2300, 'learning_rate': 0.021621931137806753}. Best is trial 0 with value: 0.6787183165949306.\n",
      "[I 2025-04-28 15:40:56,487] Trial 2 finished with value: 0.6797082735533237 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': 'balanced', 'n_estimators': 2700, 'learning_rate': 0.010424781297966774}. Best is trial 2 with value: 0.6797082735533237.\n",
      "[I 2025-04-28 15:43:04,743] Trial 3 finished with value: 0.6728263988522238 and parameters: {'base_estimator_max_depth': 2, 'class_weight_choice': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 3350, 'learning_rate': 1.6234889426743808}. Best is trial 2 with value: 0.6797082735533237.\n",
      "[I 2025-04-28 15:45:00,527] Trial 4 finished with value: 0.568364418938307 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 4550, 'learning_rate': 0.010883095115837089}. Best is trial 2 with value: 0.6797082735533237.\n",
      "[I 2025-04-28 15:45:24,699] Trial 5 finished with value: 0.6543472022955524 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 3150, 'learning_rate': 0.3014106138424482}. Best is trial 2 with value: 0.6797082735533237.\n",
      "[I 2025-04-28 15:48:35,755] Trial 6 finished with value: 0.7206599713055954 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': 'balanced', 'n_estimators': 2200, 'learning_rate': 0.3612224479595599}. Best is trial 6 with value: 0.7206599713055954.\n",
      "[I 2025-04-28 15:52:12,659] Trial 7 finished with value: 0.7079770444763271 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 900, 'learning_rate': 0.14124928659676525}. Best is trial 6 with value: 0.7206599713055954.\n",
      "[I 2025-04-28 15:53:47,534] Trial 8 finished with value: 0.5664084170253467 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': 'balanced', 'n_estimators': 3300, 'learning_rate': 0.05716666685609466}. Best is trial 6 with value: 0.7206599713055954.\n",
      "[I 2025-04-28 15:53:50,461] Trial 9 finished with value: 0.6347584887613582 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 200, 'learning_rate': 0.16710199855727814}. Best is trial 6 with value: 0.7206599713055954.\n",
      "[I 2025-04-28 15:59:46,818] Trial 10 finished with value: 0.7245911047345768 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': 'balanced', 'n_estimators': 1650, 'learning_rate': 1.1128219372493398}. Best is trial 10 with value: 0.7245911047345768.\n",
      "[I 2025-04-28 16:07:18,561] Trial 11 finished with value: 0.7138402678144429 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': 'balanced', 'n_estimators': 1650, 'learning_rate': 1.1230575600810586}. Best is trial 10 with value: 0.7245911047345768.\n",
      "[I 2025-04-28 16:13:20,313] Trial 12 finished with value: 0.712883787661406 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': 'balanced', 'n_estimators': 1550, 'learning_rate': 0.6029509283901239}. Best is trial 10 with value: 0.7245911047345768.\n",
      "[I 2025-04-28 16:18:13,440] Trial 13 finished with value: 0.7216690578670493 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': 'balanced', 'n_estimators': 1900, 'learning_rate': 0.5434280572978644}. Best is trial 10 with value: 0.7245911047345768.\n",
      "[I 2025-04-28 16:21:15,271] Trial 14 finished with value: 0.7304543280726925 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': 'balanced', 'n_estimators': 1200, 'learning_rate': 0.8751524601633347}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:22:13,116] Trial 15 finished with value: 0.7236202773792444 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': 'balanced', 'n_estimators': 1000, 'learning_rate': 1.0543881884332256}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:22:17,988] Trial 16 finished with value: 0.6640746054519369 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': 'balanced', 'n_estimators': 100, 'learning_rate': 1.8792830729218726}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:23:39,069] Trial 17 finished with value: 0.729454806312769 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 1100, 'learning_rate': 0.8006837250185014}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:24:30,153] Trial 18 finished with value: 0.70506456241033 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 950, 'learning_rate': 0.19858045539829627}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:24:58,818] Trial 19 finished with value: 0.7040650406504064 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 600, 'learning_rate': 0.5954475277267232}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:27:02,534] Trial 20 finished with value: 0.7099378287900526 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 1300, 'learning_rate': 0.0875905585831106}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:28:28,358] Trial 21 finished with value: 0.7304495456719273 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 1350, 'learning_rate': 0.9332875327683648}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:29:12,128] Trial 22 finished with value: 0.7245863223338116 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 650, 'learning_rate': 0.8066269641130271}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:30:00,339] Trial 23 finished with value: 0.7021425155428025 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 1250, 'learning_rate': 0.3517386329967266}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:30:31,634] Trial 24 finished with value: 0.7021520803443329 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 500, 'learning_rate': 1.8074825199354267}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:32:23,506] Trial 25 finished with value: 0.724595887135342 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 1950, 'learning_rate': 0.7718769115719708}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:35:24,248] Trial 26 finished with value: 0.7167766618842659 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2700, 'learning_rate': 0.41558882451626655}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:35:26,510] Trial 27 finished with value: 0.6884744141559063 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 1350, 'learning_rate': 0.26559023736395665}. Best is trial 14 with value: 0.7304543280726925.\n",
      "[I 2025-04-28 16:39:09,910] Trial 28 finished with value: 0.7226303204208513 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 3700, 'learning_rate': 1.2579562446992054}. Best is trial 14 with value: 0.7304543280726925.\n",
      "2025-04-28 16:39:09,910 - INFO - Opt complete adaboost. Best CV score: 0.73045. Best params: {'base_estimator_max_depth': 5, 'class_weight_choice': 'balanced', 'n_estimators': 1200, 'learning_rate': 0.8751524601633347}\n",
      "2025-04-28 16:39:09,910 - INFO - Saved Optuna summary: optuna_trials/adaboost_study_summary_20250428_153717.txt\n",
      "2025-04-28 16:39:09,910 - INFO - Instantiating final adaboost model...\n",
      "2025-04-28 16:39:09,910 - INFO - Reconstruct AdaBoost DT(max_depth=5, class_weight=balanced) using SAMME\n",
      "2025-04-28 16:39:09,910 - INFO - Fitting final adaboost model...\n",
      "2025-04-28 16:39:28,060 - INFO - Final adaboost fitted in 18.15s.\n",
      "2025-04-28 16:39:28,061 - INFO - Saving final adaboost model...\n",
      "2025-04-28 16:39:28,244 - INFO - Saved final adaboost via joblib: models/adaboost_20250428_153717.joblib\n",
      "2025-04-28 16:39:28,245 - INFO - Attempting calibration for adaboost...\n",
      "2025-04-28 16:40:12,530 - INFO - Saved calibrated model: calibrated_models/adaboost_calibrated_20250428_153717.joblib\n",
      "2025-04-28 16:40:12,530 - INFO - Saving importance adaboost...\n",
      "2025-04-28 16:40:14,145 - INFO - Saved importance plot: plots\\adaboost_feature_importance_20250428_153717.png\n",
      "2025-04-28 16:40:14,147 - INFO - Saved importance csv: results\\adaboost_feature_importance_20250428_153717.csv\n",
      "2025-04-28 16:40:14,148 - INFO - Qualification Check for adaboost: final_model exists? True, best_cv_score=0.7304543, threshold=0.72, comparison result: True\n",
      "2025-04-28 16:40:14,149 - INFO - +++ QUALIFIED: adaboost (CV Score: 0.73045)\n",
      "2025-04-28 16:40:14,149 - INFO - Evaluating model 'adaboost_qualified_holdout_eval'...\n",
      "2025-04-28 16:40:14,263 - INFO - Evaluation Results for 'adaboost_qualified_holdout_eval':\n",
      "2025-04-28 16:40:14,264 - INFO -   Accuracy: 0.76562\n",
      "2025-04-28 16:40:14,265 - INFO - Saved evaluation summary: results\\adaboost_qualified_holdout_eval_evaluation_20250428_153717.txt\n",
      "2025-04-28 16:40:14,350 - INFO - Saved confusion matrix plot: plots\\adaboost_qualified_holdout_eval_confusion_matrix_20250428_153717.png\n",
      "2025-04-28 16:40:14,351 - INFO - Hold-out Acc (adaboost): 0.76562\n",
      "2025-04-28 16:40:14,351 - INFO - --- Generating individual predictions for adaboost ---\n",
      "2025-04-28 16:40:14,352 - INFO - Generating test predictions using adaboost_qual_individual_pred...\n",
      "2025-04-28 16:40:14,492 - INFO - Saved submission: submissions\\solution_adaboost_qual_individual_pred_20250428_153717.csv\n",
      "2025-04-28 16:40:14,493 - INFO - Test prediction distribution for 'adaboost_qual_individual_pred': {'Medium': 328, 'High': 273, 'Low': 253}\n",
      "2025-04-28 16:40:14,494 - INFO - Saved test prediction summary: results\\adaboost_qual_individual_pred_test_prediction_summary_20250428_153717.txt\n",
      "2025-04-28 16:40:14,494 - INFO - Individual prediction file saved for adaboost.\n",
      "2025-04-28 16:40:14,495 - INFO - --- Optimizing catboost (100 trials) ---\n",
      "2025-04-28 16:40:14,496 - INFO - Starting catboost optimization (100 trials)...\n",
      "2025-04-28 16:40:14,496 - INFO - Optuna timeout for catboost: 7200s.\n",
      "[I 2025-04-28 16:40:14,728] A new study created in RDB with name: catboost_opt_20250428_153717\n",
      "2025-04-28 16:40:14,734 - INFO - Setting Optuna timeout 7200s.\n",
      "[I 2025-04-28 16:40:16,591] Trial 0 finished with value: 0.7119081779053085 and parameters: {'task_type': 'CPU', 'class_weight_config': 'Balanced', 'iterations': 3900, 'depth': 8, 'learning_rate': 0.1184852734015384, 'l2_leaf_reg': 1.7246842853656301, 'random_strength': 0.003586488755255766, 'border_count': 32, 'bagging_temperature': 0.4885276740466827}. Best is trial 0 with value: 0.7119081779053085.\n",
      "[I 2025-04-28 16:41:14,218] Trial 1 finished with value: 0.6923816355810617 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 1600, 'depth': 13, 'learning_rate': 0.10318698856363799, 'l2_leaf_reg': 7.487629245488217, 'random_strength': 1.0009368441759299, 'border_count': 64, 'bagging_temperature': 0.722098737878351}. Best is trial 0 with value: 0.7119081779053085.\n",
      "[I 2025-04-28 16:41:21,034] Trial 2 finished with value: 0.6855475848876136 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.2, '1': 1.1, '2': 1.0}, 'iterations': 2600, 'depth': 9, 'learning_rate': 0.011562226623927082, 'l2_leaf_reg': 12.291673348361893, 'random_strength': 0.24728489973713932, 'border_count': 32, 'bagging_temperature': 0.7798376815617404}. Best is trial 0 with value: 0.7119081779053085.\n",
      "[I 2025-04-28 16:42:09,060] Trial 3 finished with value: 0.7060545193687231 and parameters: {'task_type': 'CPU', 'class_weight_config': 'Balanced', 'iterations': 2600, 'depth': 11, 'learning_rate': 0.04349905099612369, 'l2_leaf_reg': 1.2789225446692054, 'random_strength': 0.058337985652538796, 'border_count': 128, 'bagging_temperature': 0.24640185186548996}. Best is trial 0 with value: 0.7119081779053085.\n",
      "[I 2025-04-28 16:42:22,064] Trial 4 finished with value: 0.6874892395982783 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.2, '1': 1.1, '2': 1.0}, 'iterations': 4400, 'depth': 11, 'learning_rate': 0.018161852465835147, 'l2_leaf_reg': 18.88138592637745, 'random_strength': 0.053499322087812176, 'border_count': 128, 'bagging_temperature': 0.8350430397701127}. Best is trial 0 with value: 0.7119081779053085.\n",
      "[I 2025-04-28 16:42:48,171] Trial 5 finished with value: 0.6855475848876136 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.2, '1': 1.1, '2': 1.0}, 'iterations': 3600, 'depth': 12, 'learning_rate': 0.065345680240425, 'l2_leaf_reg': 2.238027650648277, 'random_strength': 1.3602053202957436, 'border_count': 32, 'bagging_temperature': 0.4923092224069326}. Best is trial 0 with value: 0.7119081779053085.\n",
      "[I 2025-04-28 16:42:52,221] Trial 6 finished with value: 0.7255619320899092 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.2, '1': 1.1, '2': 1.0}, 'iterations': 2000, 'depth': 9, 'learning_rate': 0.08305423055857017, 'l2_leaf_reg': 0.5077313026421935, 'random_strength': 0.07003167571066247, 'border_count': 32, 'bagging_temperature': 0.6509883638890405}. Best is trial 6 with value: 0.7255619320899092.\n",
      "[I 2025-04-28 16:43:15,880] Trial 7 finished with value: 0.7070157819225251 and parameters: {'task_type': 'GPU', 'class_weight_config': None, 'iterations': 900, 'depth': 11, 'learning_rate': 0.03779085693441669, 'l2_leaf_reg': 2.9254908565136484, 'random_strength': 0.05534659463692866, 'border_count': 254, 'bagging_temperature': 0.09359653575038032}. Best is trial 6 with value: 0.7255619320899092.\n",
      "[I 2025-04-28 16:43:19,860] Trial 8 finished with value: 0.6718842659014825 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 3000, 'depth': 7, 'learning_rate': 0.0071923995786730595, 'l2_leaf_reg': 11.90955406599514, 'random_strength': 0.5092072598945477, 'border_count': 128, 'bagging_temperature': 0.8519178969012902}. Best is trial 6 with value: 0.7255619320899092.\n",
      "[I 2025-04-28 16:43:20,977] Trial 9 finished with value: 0.6963271162123386 and parameters: {'task_type': 'CPU', 'class_weight_config': None, 'iterations': 3600, 'depth': 5, 'learning_rate': 0.01838849311814369, 'l2_leaf_reg': 1.0128431986998174, 'random_strength': 0.1099478168091733, 'border_count': 64, 'bagging_temperature': 0.18952429696955256}. Best is trial 6 with value: 0.7255619320899092.\n",
      "[I 2025-04-28 17:17:00,651] Trial 10 finished with value: 0.6952845528455284 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.1, '1': 1.0, '2': 1.0}, 'iterations': 300, 'depth': 15, 'learning_rate': 0.07004471729306574, 'l2_leaf_reg': 0.5637787287724524, 'random_strength': 0.004809160853532641, 'border_count': 254, 'bagging_temperature': 0.6171204508599726}. Best is trial 6 with value: 0.7255619320899092.\n",
      "[I 2025-04-28 17:17:02,615] Trial 11 finished with value: 0.7304830224772836 and parameters: {'task_type': 'CPU', 'class_weight_config': 'Balanced', 'iterations': 5400, 'depth': 8, 'learning_rate': 0.13809253631715837, 'l2_leaf_reg': 0.6047999564940894, 'random_strength': 0.002159431519733345, 'border_count': 32, 'bagging_temperature': 0.4107553989661527}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:17:03,931] Trial 12 finished with value: 0.7246102343376375 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.1, '1': 1.0, '2': 1.0}, 'iterations': 5500, 'depth': 5, 'learning_rate': 0.1451712149308444, 'l2_leaf_reg': 0.5518675040867475, 'random_strength': 7.774747674214943, 'border_count': 32, 'bagging_temperature': 0.344015634228377}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:17:05,781] Trial 13 finished with value: 0.7255714968914395 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 5400, 'depth': 7, 'learning_rate': 0.07426838355585302, 'l2_leaf_reg': 0.8623991328132398, 'random_strength': 0.0010318230766942832, 'border_count': 32, 'bagging_temperature': 0.6194394358741755}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:17:07,274] Trial 14 finished with value: 0.6943232902917265 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 5300, 'depth': 7, 'learning_rate': 0.05947555778157669, 'l2_leaf_reg': 4.362133364530628, 'random_strength': 0.0010191040956119175, 'border_count': 32, 'bagging_temperature': 0.406133318548949}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:17:08,555] Trial 15 finished with value: 0.7167910090865615 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 4800, 'depth': 4, 'learning_rate': 0.14322248256532444, 'l2_leaf_reg': 0.9334694343387343, 'random_strength': 0.0099539181792879, 'border_count': 32, 'bagging_temperature': 0.5641902540514846}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:17:10,014] Trial 16 finished with value: 0.694313725490196 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 4700, 'depth': 6, 'learning_rate': 0.049572383135326394, 'l2_leaf_reg': 4.53724864452951, 'random_strength': 0.0010408895139511968, 'border_count': 32, 'bagging_temperature': 0.3046255162677418}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:17:39,004] Trial 17 finished with value: 0.7031085604973697 and parameters: {'task_type': 'CPU', 'class_weight_config': 'Balanced', 'iterations': 5100, 'depth': 9, 'learning_rate': 0.028793486081751852, 'l2_leaf_reg': 0.8403738864782236, 'random_strength': 0.013876813667901207, 'border_count': 254, 'bagging_temperature': 0.0019154226572551392}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:17:41,147] Trial 18 finished with value: 0.7246150167384027 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 4200, 'depth': 7, 'learning_rate': 0.08773872816511331, 'l2_leaf_reg': 1.5639062589961825, 'random_strength': 0.003530362154760442, 'border_count': 64, 'bagging_temperature': 0.4192254174079668}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:22:37,949] Trial 19 finished with value: 0.6816260162601625 and parameters: {'task_type': 'CPU', 'class_weight_config': 'Balanced', 'iterations': 4700, 'depth': 14, 'learning_rate': 0.09682468633211916, 'l2_leaf_reg': 29.29996241221206, 'random_strength': 0.018425533652013916, 'border_count': 32, 'bagging_temperature': 0.5679608034980255}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:42:35,709] Trial 20 finished with value: 0.6747824007651841 and parameters: {'task_type': 'CPU', 'class_weight_config': None, 'iterations': 5500, 'depth': 16, 'learning_rate': 0.02684546127609442, 'l2_leaf_reg': 0.7665570894636723, 'random_strength': 0.0022983577817937088, 'border_count': 32, 'bagging_temperature': 0.6861389942533878}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:42:48,643] Trial 21 finished with value: 0.7158058345289335 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.2, '1': 1.1, '2': 1.0}, 'iterations': 2100, 'depth': 9, 'learning_rate': 0.07979718016279587, 'l2_leaf_reg': 0.5254279323559791, 'random_strength': 0.006931544498362996, 'border_count': 32, 'bagging_temperature': 0.6471310310446711}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:42:59,251] Trial 22 finished with value: 0.7236346245815399 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.2, '1': 1.1, '2': 1.0}, 'iterations': 1600, 'depth': 8, 'learning_rate': 0.11546763919270484, 'l2_leaf_reg': 0.7194521726695097, 'random_strength': 0.025298024009803438, 'border_count': 32, 'bagging_temperature': 0.524513306590895}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:43:35,656] Trial 23 finished with value: 0.7109277857484457 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.1, '1': 1.0, '2': 1.0}, 'iterations': 3200, 'depth': 10, 'learning_rate': 0.054559244488727315, 'l2_leaf_reg': 1.2055305644693792, 'random_strength': 0.00210711083242244, 'border_count': 32, 'bagging_temperature': 0.7605093314060665}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:43:47,513] Trial 24 finished with value: 0.72168818747011 and parameters: {'task_type': 'CPU', 'class_weight_config': 'Balanced', 'iterations': 1700, 'depth': 8, 'learning_rate': 0.07639181580480434, 'l2_leaf_reg': 0.6713988625474582, 'random_strength': 0.00173359623019523, 'border_count': 32, 'bagging_temperature': 0.6319106628114581}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:43:53,519] Trial 25 finished with value: 0.7187661406025825 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.2, '1': 1.1, '2': 1.0}, 'iterations': 2200, 'depth': 6, 'learning_rate': 0.14944021814114747, 'l2_leaf_reg': 2.3807154205094974, 'random_strength': 6.453296588511746, 'border_count': 32, 'bagging_temperature': 0.36524137360827913}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:44:23,237] Trial 26 finished with value: 0.7275274988043998 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 1100, 'depth': 10, 'learning_rate': 0.1123500956674076, 'l2_leaf_reg': 1.3473080577234768, 'random_strength': 0.18671165950534646, 'border_count': 64, 'bagging_temperature': 0.8967478472077052}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:44:51,991] Trial 27 finished with value: 0.716805356288857 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 1000, 'depth': 10, 'learning_rate': 0.11430960865286023, 'l2_leaf_reg': 1.6861128813205883, 'random_strength': 0.15368407453114194, 'border_count': 64, 'bagging_temperature': 0.8687253551373249}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:45:50,788] Trial 28 finished with value: 0.7109230033476805 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 5100, 'depth': 12, 'learning_rate': 0.1186969007814451, 'l2_leaf_reg': 1.2373306559716448, 'random_strength': 0.02908613440830934, 'border_count': 64, 'bagging_temperature': 0.8989588333086755}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:46:01,001] Trial 29 finished with value: 0.7207125777140124 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 4000, 'depth': 6, 'learning_rate': 0.12435132710143557, 'l2_leaf_reg': 2.145335318485602, 'random_strength': 0.2659127033624236, 'border_count': 64, 'bagging_temperature': 0.46465388470402236}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:46:19,311] Trial 30 finished with value: 0.7275514108082257 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3600, 'depth': 8, 'learning_rate': 0.09118171614307394, 'l2_leaf_reg': 3.0342308649154126, 'random_strength': 0.004579983573944154, 'border_count': 64, 'bagging_temperature': 0.7635832796757257}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:46:37,290] Trial 31 finished with value: 0.7294787183165949 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3700, 'depth': 8, 'learning_rate': 0.09433358908174401, 'l2_leaf_reg': 3.2647990801917937, 'random_strength': 0.004055287515275889, 'border_count': 64, 'bagging_temperature': 0.7944357513281942}. Best is trial 11 with value: 0.7304830224772836.\n",
      "[I 2025-04-28 17:46:54,891] Trial 32 finished with value: 0.7314299378287901 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3500, 'depth': 8, 'learning_rate': 0.10702469285813249, 'l2_leaf_reg': 2.836134814062014, 'random_strength': 0.005385943798917934, 'border_count': 64, 'bagging_temperature': 0.7815068397240508}. Best is trial 32 with value: 0.7314299378287901.\n",
      "[I 2025-04-28 17:47:11,539] Trial 33 finished with value: 0.7343663318986131 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3500, 'depth': 8, 'learning_rate': 0.09574223074945414, 'l2_leaf_reg': 3.162111946464699, 'random_strength': 0.004497963068475151, 'border_count': 64, 'bagging_temperature': 0.7410947548029072}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:47:33,234] Trial 34 finished with value: 0.7246197991391679 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3300, 'depth': 9, 'learning_rate': 0.0952823157458971, 'l2_leaf_reg': 6.451339380043222, 'random_strength': 0.00829422167396779, 'border_count': 64, 'bagging_temperature': 0.7089735577004568}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:47:53,197] Trial 35 finished with value: 0.7148445719751315 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2700, 'depth': 8, 'learning_rate': 0.044728116873076054, 'l2_leaf_reg': 5.819607775773542, 'random_strength': 0.002891132978254454, 'border_count': 64, 'bagging_temperature': 0.8026098118576787}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:48:08,361] Trial 36 finished with value: 0.707039693926351 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3900, 'depth': 7, 'learning_rate': 0.06118583988986408, 'l2_leaf_reg': 8.564650508007372, 'random_strength': 0.005583891832890049, 'border_count': 64, 'bagging_temperature': 0.815844212010414}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:48:20,541] Trial 37 finished with value: 0.7314586322333811 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4200, 'depth': 5, 'learning_rate': 0.09878743187260572, 'l2_leaf_reg': 3.145133382925707, 'random_strength': 0.011986199625913317, 'border_count': 128, 'bagging_temperature': 0.7164063102255492}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:48:31,338] Trial 38 finished with value: 0.7265758010521282 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4400, 'depth': 4, 'learning_rate': 0.12834857011919099, 'l2_leaf_reg': 3.7337194161537854, 'random_strength': 0.030396900559408187, 'border_count': 128, 'bagging_temperature': 0.722749185580228}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:48:49,346] Trial 39 finished with value: 0.7070301291248207 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3300, 'depth': 6, 'learning_rate': 0.03883138128974265, 'l2_leaf_reg': 8.719507677314239, 'random_strength': 0.011894239511317085, 'border_count': 128, 'bagging_temperature': 0.2395224261729436}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:49:54,211] Trial 40 finished with value: 0.6865231946437111 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4400, 'depth': 12, 'learning_rate': 0.014321433510645637, 'l2_leaf_reg': 2.3532965215164965, 'random_strength': 0.0019529553436731515, 'border_count': 128, 'bagging_temperature': 0.5220533121358291}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:50:05,382] Trial 41 finished with value: 0.730468675274988 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3900, 'depth': 5, 'learning_rate': 0.10540506592028297, 'l2_leaf_reg': 3.469954366467008, 'random_strength': 0.003541231745471565, 'border_count': 128, 'bagging_temperature': 0.7401968300886711}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:50:17,469] Trial 42 finished with value: 0.729507412721186 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4000, 'depth': 5, 'learning_rate': 0.10315907596611457, 'l2_leaf_reg': 4.019609578787599, 'random_strength': 0.00697834970738693, 'border_count': 128, 'bagging_temperature': 0.7304649980103627}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:50:30,821] Trial 43 finished with value: 0.7275561932089909 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2800, 'depth': 5, 'learning_rate': 0.13379227979741293, 'l2_leaf_reg': 5.096039802538634, 'random_strength': 0.0015829353428268062, 'border_count': 128, 'bagging_temperature': 0.6744732142015081}. Best is trial 33 with value: 0.7343663318986131.\n",
      "[I 2025-04-28 17:50:43,495] Trial 44 finished with value: 0.7392635102821616 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3500, 'depth': 5, 'learning_rate': 0.08339733854108383, 'l2_leaf_reg': 1.9693732533572836, 'random_strength': 0.0030917154027703367, 'border_count': 128, 'bagging_temperature': 0.7573634716526008}. Best is trial 44 with value: 0.7392635102821616.\n",
      "[I 2025-04-28 17:50:56,449] Trial 45 finished with value: 0.7236537541846007 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3000, 'depth': 4, 'learning_rate': 0.06770849538994177, 'l2_leaf_reg': 1.9698908628038208, 'random_strength': 0.017437893207373843, 'border_count': 128, 'bagging_temperature': 0.8374813225262734}. Best is trial 44 with value: 0.7392635102821616.\n",
      "[I 2025-04-28 17:51:07,326] Trial 46 finished with value: 0.7236394069823051 and parameters: {'task_type': 'GPU', 'class_weight_config': None, 'iterations': 3400, 'depth': 6, 'learning_rate': 0.08199447386779339, 'l2_leaf_reg': 2.759218353185493, 'random_strength': 0.03931357740445901, 'border_count': 254, 'bagging_temperature': 0.6005542090747884}. Best is trial 44 with value: 0.7392635102821616.\n",
      "[I 2025-04-28 17:51:24,929] Trial 47 finished with value: 0.720726924916308 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4200, 'depth': 7, 'learning_rate': 0.0544877852675521, 'l2_leaf_reg': 1.9267759904509187, 'random_strength': 0.0026994299763791994, 'border_count': 128, 'bagging_temperature': 0.6741667692087719}. Best is trial 44 with value: 0.7392635102821616.\n",
      "[I 2025-04-28 17:52:14,900] Trial 48 finished with value: 0.6992109038737446 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.1, '1': 1.0, '2': 1.0}, 'iterations': 2400, 'depth': 11, 'learning_rate': 0.021640263236541217, 'l2_leaf_reg': 2.6852970680961876, 'random_strength': 0.005869239593771918, 'border_count': 128, 'bagging_temperature': 0.7674965236151221}. Best is trial 44 with value: 0.7392635102821616.\n",
      "[I 2025-04-28 17:52:24,865] Trial 49 finished with value: 0.718780487804878 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3000, 'depth': 4, 'learning_rate': 0.06636604137640348, 'l2_leaf_reg': 1.5144147918901436, 'random_strength': 0.0015182997800283848, 'border_count': 254, 'bagging_temperature': 0.8490132333215255}. Best is trial 44 with value: 0.7392635102821616.\n",
      "[I 2025-04-28 17:52:43,854] Trial 50 finished with value: 0.6884791965566714 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4900, 'depth': 9, 'learning_rate': 0.00974186859166303, 'l2_leaf_reg': 1.091461177351629, 'random_strength': 0.009960067669516855, 'border_count': 128, 'bagging_temperature': 0.2813848820615352}. Best is trial 44 with value: 0.7392635102821616.\n",
      "[I 2025-04-28 17:52:55,994] Trial 51 finished with value: 0.7275466284074605 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3700, 'depth': 5, 'learning_rate': 0.106078848928748, 'l2_leaf_reg': 3.5633339375954405, 'random_strength': 0.003481742814773888, 'border_count': 128, 'bagging_temperature': 0.7348681791771735}. Best is trial 44 with value: 0.7392635102821616.\n",
      "[I 2025-04-28 17:53:04,978] Trial 52 finished with value: 0.725609756097561 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3800, 'depth': 5, 'learning_rate': 0.1266304322266603, 'l2_leaf_reg': 5.174957785238631, 'random_strength': 0.004106458487975739, 'border_count': 128, 'bagging_temperature': 0.5807797553551164}. Best is trial 44 with value: 0.7392635102821616.\n",
      "[I 2025-04-28 17:53:16,832] Trial 53 finished with value: 0.7255906264945002 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3500, 'depth': 6, 'learning_rate': 0.10207517369972907, 'l2_leaf_reg': 2.590054568477199, 'random_strength': 0.0013532482442334248, 'border_count': 128, 'bagging_temperature': 0.7020185514580104}. Best is trial 44 with value: 0.7392635102821616.\n",
      "[I 2025-04-28 17:53:31,238] Trial 54 finished with value: 0.7412195121951219 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4100, 'depth': 5, 'learning_rate': 0.08465692030097485, 'l2_leaf_reg': 3.061256001744908, 'random_strength': 0.002785288110771944, 'border_count': 128, 'bagging_temperature': 0.3850038845369453}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:53:40,664] Trial 55 finished with value: 0.7138833094213295 and parameters: {'task_type': 'GPU', 'class_weight_config': None, 'iterations': 4200, 'depth': 4, 'learning_rate': 0.08080175012847036, 'l2_leaf_reg': 4.35840355340175, 'random_strength': 0.014554023352805937, 'border_count': 64, 'bagging_temperature': 0.423124044162862}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:54:01,899] Trial 56 finished with value: 0.7138737446197991 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 5100, 'depth': 7, 'learning_rate': 0.08696796271763006, 'l2_leaf_reg': 13.413209234766397, 'random_strength': 0.002788071915106514, 'border_count': 128, 'bagging_temperature': 0.3788372029709117}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:54:15,525] Trial 57 finished with value: 0.7285222381635581 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4500, 'depth': 6, 'learning_rate': 0.07277888012722367, 'l2_leaf_reg': 1.9512511995238448, 'random_strength': 0.08041927349972328, 'border_count': 128, 'bagging_temperature': 0.48390513818968783}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:54:31,005] Trial 58 finished with value: 0.7167910090865615 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.1, '1': 1.0, '2': 1.0}, 'iterations': 3200, 'depth': 8, 'learning_rate': 0.13764256565311078, 'l2_leaf_reg': 3.055497896793881, 'random_strength': 0.008128385469413916, 'border_count': 254, 'bagging_temperature': 0.3227042318010345}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:54:42,005] Trial 59 finished with value: 0.7285270205643233 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4100, 'depth': 7, 'learning_rate': 0.08683283708604601, 'l2_leaf_reg': 1.44321564445053, 'random_strength': 0.005308086636508255, 'border_count': 64, 'bagging_temperature': 0.3833751464825459}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:55:24,765] Trial 60 finished with value: 0.7167910090865615 and parameters: {'task_type': 'CPU', 'class_weight_config': {'0': 1.2, '1': 1.1, '2': 1.0}, 'iterations': 3500, 'depth': 9, 'learning_rate': 0.14669313675007756, 'l2_leaf_reg': 4.882831377696126, 'random_strength': 0.0023482257443652485, 'border_count': 128, 'bagging_temperature': 0.16594734921478183}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:55:36,426] Trial 61 finished with value: 0.725609756097561 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3800, 'depth': 5, 'learning_rate': 0.10476696636968365, 'l2_leaf_reg': 3.369206158629793, 'random_strength': 0.0033501588692757493, 'border_count': 128, 'bagging_temperature': 0.7566447946817044}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:55:48,486] Trial 62 finished with value: 0.7294978479196557 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4600, 'depth': 5, 'learning_rate': 0.11925953794667708, 'l2_leaf_reg': 3.9808660265495495, 'random_strength': 0.0011614678899194227, 'border_count': 128, 'bagging_temperature': 0.44944321103304874}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:55:58,971] Trial 63 finished with value: 0.7080487804878048 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4300, 'depth': 4, 'learning_rate': 0.07422173992704377, 'l2_leaf_reg': 6.68790686032194, 'random_strength': 3.327861585042745, 'border_count': 128, 'bagging_temperature': 0.6619497888367698}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:56:11,035] Trial 64 finished with value: 0.7314490674318508 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3100, 'depth': 6, 'learning_rate': 0.10911891458093446, 'l2_leaf_reg': 2.564431753761488, 'random_strength': 0.9129510750439892, 'border_count': 128, 'bagging_temperature': 0.7890621808550352}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:56:27,312] Trial 65 finished with value: 0.7275227164036346 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2500, 'depth': 7, 'learning_rate': 0.058861368943907186, 'l2_leaf_reg': 1.717356911328159, 'random_strength': 0.6930019162299578, 'border_count': 32, 'bagging_temperature': 0.8216124070944367}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:56:34,594] Trial 66 finished with value: 0.7001673840267814 and parameters: {'task_type': 'CPU', 'class_weight_config': None, 'iterations': 3100, 'depth': 6, 'learning_rate': 0.09308002456027432, 'l2_leaf_reg': 2.2560881976301, 'random_strength': 2.246702313506694, 'border_count': 64, 'bagging_temperature': 0.5347366188322868}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:56:46,133] Trial 67 finished with value: 0.7373075083692013 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2800, 'depth': 6, 'learning_rate': 0.11619152339660384, 'l2_leaf_reg': 2.8287622947443967, 'random_strength': 0.37220147390889824, 'border_count': 64, 'bagging_temperature': 0.7952172831136236}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:56:57,833] Trial 68 finished with value: 0.734390243902439 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2900, 'depth': 6, 'learning_rate': 0.11099159317435102, 'l2_leaf_reg': 2.4993652886615756, 'random_strength': 1.1321420492310954, 'border_count': 64, 'bagging_temperature': 0.7935383075452486}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:57:10,171] Trial 69 finished with value: 0.7236394069823051 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2900, 'depth': 6, 'learning_rate': 0.1165500046661978, 'l2_leaf_reg': 2.536568021235504, 'random_strength': 1.3512210890055922, 'border_count': 64, 'bagging_temperature': 0.8755779734096987}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:57:19,935] Trial 70 finished with value: 0.7246054519368723 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.2, '1': 1.1, '2': 1.0}, 'iterations': 2400, 'depth': 5, 'learning_rate': 0.08464005684031654, 'l2_leaf_reg': 1.8425111502634555, 'random_strength': 0.31507367016022153, 'border_count': 64, 'bagging_temperature': 0.7866837316314346}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:57:30,751] Trial 71 finished with value: 0.72168818747011 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2700, 'depth': 6, 'learning_rate': 0.09786420961757929, 'l2_leaf_reg': 2.9157555302934055, 'random_strength': 0.3937077731739594, 'border_count': 64, 'bagging_temperature': 0.7870939502851626}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:57:47,107] Trial 72 finished with value: 0.7285270205643233 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3400, 'depth': 7, 'learning_rate': 0.11117991016288731, 'l2_leaf_reg': 2.0840845371919414, 'random_strength': 0.7235917773226846, 'border_count': 64, 'bagging_temperature': 0.820823246771542}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:57:57,490] Trial 73 finished with value: 0.7304878048780488 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3200, 'depth': 5, 'learning_rate': 0.12719618517477335, 'l2_leaf_reg': 3.1451546977026843, 'random_strength': 1.0452829500205594, 'border_count': 64, 'bagging_temperature': 0.6978892014128023}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:58:07,884] Trial 74 finished with value: 0.7168005738880918 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2900, 'depth': 4, 'learning_rate': 0.0757082041186668, 'l2_leaf_reg': 2.425345880318045, 'random_strength': 1.7088449544297137, 'border_count': 64, 'bagging_temperature': 0.8632301845660543}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:58:22,352] Trial 75 finished with value: 0.7343806791009087 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3500, 'depth': 6, 'learning_rate': 0.08925190097188655, 'l2_leaf_reg': 3.8358204003226564, 'random_strength': 0.4442476809220165, 'border_count': 64, 'bagging_temperature': 0.7550706686854621}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:58:37,159] Trial 76 finished with value: 0.7304830224772836 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.1, '1': 1.0, '2': 1.0}, 'iterations': 1900, 'depth': 6, 'learning_rate': 0.09117544444204903, 'l2_leaf_reg': 4.098553067448856, 'random_strength': 0.5297213454036132, 'border_count': 64, 'bagging_temperature': 0.7538319258221934}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:58:51,611] Trial 77 finished with value: 0.7236537541846007 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2700, 'depth': 6, 'learning_rate': 0.06389788130666399, 'l2_leaf_reg': 3.7492388992414476, 'random_strength': 0.15927836479900082, 'border_count': 64, 'bagging_temperature': 0.7195377081775741}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:59:03,176] Trial 78 finished with value: 0.7353610712577714 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3700, 'depth': 5, 'learning_rate': 0.09826948316741904, 'l2_leaf_reg': 2.1493853360035793, 'random_strength': 0.7214607019840761, 'border_count': 128, 'bagging_temperature': 0.6379622740822759}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:59:14,879] Trial 79 finished with value: 0.725609756097561 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3700, 'depth': 5, 'learning_rate': 0.048968477592708934, 'l2_leaf_reg': 2.2023815876908563, 'random_strength': 0.46538795400308886, 'border_count': 64, 'bagging_temperature': 0.635855367853069}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:59:26,412] Trial 80 finished with value: 0.7187374461979914 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4000, 'depth': 4, 'learning_rate': 0.07031016341730614, 'l2_leaf_reg': 1.6233349553584093, 'random_strength': 3.1429802517639103, 'border_count': 128, 'bagging_temperature': 0.6829039948961038}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:59:37,111] Trial 81 finished with value: 0.7343950263032042 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3100, 'depth': 5, 'learning_rate': 0.09889392614440876, 'l2_leaf_reg': 3.220921539151708, 'random_strength': 0.7799430519076636, 'border_count': 128, 'bagging_temperature': 0.8010730714475345}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 17:59:48,699] Trial 82 finished with value: 0.7265614538498326 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3400, 'depth': 5, 'learning_rate': 0.09698904629649753, 'l2_leaf_reg': 3.2307434256214775, 'random_strength': 0.22529713995150968, 'border_count': 128, 'bagging_temperature': 0.8115322957898343}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:00:01,714] Trial 83 finished with value: 0.7275514108082257 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3600, 'depth': 5, 'learning_rate': 0.08655799484480729, 'l2_leaf_reg': 4.491686939971068, 'random_strength': 0.7372312368144659, 'border_count': 128, 'bagging_temperature': 0.7388706393602993}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:00:12,508] Trial 84 finished with value: 0.7265805834528933 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3300, 'depth': 5, 'learning_rate': 0.07841597235830747, 'l2_leaf_reg': 2.837156593553308, 'random_strength': 0.31379865722357303, 'border_count': 254, 'bagging_temperature': 0.8347064081207797}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:00:21,691] Trial 85 finished with value: 0.7178048780487805 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3100, 'depth': 4, 'learning_rate': 0.13556959223109674, 'l2_leaf_reg': 3.5479574121315576, 'random_strength': 0.10310165620797074, 'border_count': 128, 'bagging_temperature': 0.768239149128843}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:00:36,503] Trial 86 finished with value: 0.6933859397417504 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 300, 'depth': 6, 'learning_rate': 0.03292039367242641, 'l2_leaf_reg': 5.546245335042307, 'random_strength': 0.5711979596316636, 'border_count': 128, 'bagging_temperature': 0.6537847749719454}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:00:43,712] Trial 87 finished with value: 0.7148541367766619 and parameters: {'task_type': 'GPU', 'class_weight_config': None, 'iterations': 3700, 'depth': 4, 'learning_rate': 0.11954758522395419, 'l2_leaf_reg': 2.2943016116665933, 'random_strength': 1.0680730956070335, 'border_count': 64, 'bagging_temperature': 0.7065496705498889}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:00:57,996] Trial 88 finished with value: 0.7236346245815399 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4100, 'depth': 7, 'learning_rate': 0.10073527510011952, 'l2_leaf_reg': 1.7485068171657545, 'random_strength': 1.4603121918478021, 'border_count': 128, 'bagging_temperature': 0.6050967208199737}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:01:08,565] Trial 89 finished with value: 0.7265614538498326 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2800, 'depth': 5, 'learning_rate': 0.09110575825256625, 'l2_leaf_reg': 2.026250956984909, 'random_strength': 0.38248271359005714, 'border_count': 64, 'bagging_temperature': 0.750380497476442}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:01:23,548] Trial 90 finished with value: 0.7187565758010521 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.3, '1': 1.0, '2': 1.0}, 'iterations': 3800, 'depth': 6, 'learning_rate': 0.08059292551551579, 'l2_leaf_reg': 2.7715304722119676, 'random_strength': 0.130686419665782, 'border_count': 128, 'bagging_temperature': 0.8775314198852119}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:01:35,295] Trial 91 finished with value: 0.7265758010521282 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3100, 'depth': 6, 'learning_rate': 0.10572085374216404, 'l2_leaf_reg': 2.556824780314301, 'random_strength': 0.9002738958176636, 'border_count': 128, 'bagging_temperature': 0.7934779179947873}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:01:48,485] Trial 92 finished with value: 0.7294930655188905 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 3500, 'depth': 7, 'learning_rate': 0.125091019113985, 'l2_leaf_reg': 2.5337428648699474, 'random_strength': 1.9095260411235162, 'border_count': 128, 'bagging_temperature': 0.8425573391568146}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:01:58,535] Trial 93 finished with value: 0.7324342419894787 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2500, 'depth': 5, 'learning_rate': 0.10907693639490111, 'l2_leaf_reg': 3.226281760481336, 'random_strength': 0.8772890252684445, 'border_count': 128, 'bagging_temperature': 0.7729164812431114}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:02:11,015] Trial 94 finished with value: 0.7353371592539455 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2600, 'depth': 5, 'learning_rate': 0.09807033052999556, 'l2_leaf_reg': 3.2127549764814973, 'random_strength': 1.2726316344708413, 'border_count': 128, 'bagging_temperature': 0.719303246967346}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:02:22,584] Trial 95 finished with value: 0.720726924916308 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.2, '1': 1.1, '2': 1.0}, 'iterations': 2300, 'depth': 5, 'learning_rate': 0.11326482621480514, 'l2_leaf_reg': 4.166655141258105, 'random_strength': 1.1956551553705526, 'border_count': 128, 'bagging_temperature': 0.7729652105600048}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:02:33,033] Trial 96 finished with value: 0.7236441893830703 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2600, 'depth': 5, 'learning_rate': 0.09420518928757746, 'l2_leaf_reg': 4.813944431661598, 'random_strength': 2.1712270760772387, 'border_count': 128, 'bagging_temperature': 0.8026135061628786}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:02:43,031] Trial 97 finished with value: 0.7158297465327594 and parameters: {'task_type': 'GPU', 'class_weight_config': {'0': 1.1, '1': 1.0, '2': 1.0}, 'iterations': 2100, 'depth': 4, 'learning_rate': 0.08496044551409808, 'l2_leaf_reg': 3.5994766765948714, 'random_strength': 0.6016939333677486, 'border_count': 64, 'bagging_temperature': 0.7268779245299918}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:02:54,460] Trial 98 finished with value: 0.7353658536585366 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2500, 'depth': 4, 'learning_rate': 0.13195165671681489, 'l2_leaf_reg': 3.2980386001854685, 'random_strength': 3.1411038774888347, 'border_count': 128, 'bagging_temperature': 0.6982899160567739}. Best is trial 54 with value: 0.7412195121951219.\n",
      "[I 2025-04-28 18:03:02,788] Trial 99 finished with value: 0.7099713055954089 and parameters: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 2900, 'depth': 4, 'learning_rate': 0.1290656051088752, 'l2_leaf_reg': 2.9359754568157648, 'random_strength': 5.242279307746062, 'border_count': 254, 'bagging_temperature': 0.6914330475545662}. Best is trial 54 with value: 0.7412195121951219.\n",
      "2025-04-28 18:03:02,847 - INFO - Opt complete catboost. Best CV score: 0.74122. Best params: {'task_type': 'GPU', 'class_weight_config': 'Balanced', 'iterations': 4100, 'depth': 5, 'learning_rate': 0.08465692030097485, 'l2_leaf_reg': 3.061256001744908, 'random_strength': 0.002785288110771944, 'border_count': 128, 'bagging_temperature': 0.3850038845369453}\n",
      "2025-04-28 18:03:02,851 - INFO - Saved Optuna summary: optuna_trials/catboost_study_summary_20250428_153717.txt\n",
      "2025-04-28 18:03:02,852 - INFO - Instantiating final catboost model...\n",
      "2025-04-28 18:03:02,855 - INFO - CatBoost using auto_class_weights=Balanced\n",
      "2025-04-28 18:03:02,856 - INFO - Fitting final catboost model...\n",
      "2025-04-28 18:03:55,008 - INFO - Final catboost fitted in 52.15s.\n",
      "2025-04-28 18:03:55,011 - INFO - Saving final catboost model...\n",
      "2025-04-28 18:03:55,041 - INFO - Saved final catboost via joblib: models/catboost_20250428_153717.joblib\n",
      "2025-04-28 18:03:55,044 - INFO - Attempting calibration for catboost...\n",
      "2025-04-28 18:10:32,999 - INFO - Saved calibrated model: calibrated_models/catboost_calibrated_20250428_153717.joblib\n",
      "2025-04-28 18:10:33,011 - INFO - Saving importance catboost...\n",
      "2025-04-28 18:10:33,438 - INFO - Saved importance plot: plots\\catboost_feature_importance_20250428_153717.png\n",
      "2025-04-28 18:10:33,449 - INFO - Saved importance csv: results\\catboost_feature_importance_20250428_153717.csv\n",
      "2025-04-28 18:10:33,449 - INFO - Qualification Check for catboost: final_model exists? True, best_cv_score=0.7412195, threshold=0.72, comparison result: True\n",
      "2025-04-28 18:10:33,459 - INFO - +++ QUALIFIED: catboost (CV Score: 0.74122)\n",
      "2025-04-28 18:10:33,459 - INFO - Evaluating model 'catboost_qualified_holdout_eval'...\n",
      "2025-04-28 18:10:33,487 - INFO - Evaluation Results for 'catboost_qualified_holdout_eval':\n",
      "2025-04-28 18:10:33,489 - INFO -   Accuracy: 0.77734\n",
      "2025-04-28 18:10:33,492 - INFO - Saved evaluation summary: results\\catboost_qualified_holdout_eval_evaluation_20250428_153717.txt\n",
      "2025-04-28 18:10:33,605 - INFO - Saved confusion matrix plot: plots\\catboost_qualified_holdout_eval_confusion_matrix_20250428_153717.png\n",
      "2025-04-28 18:10:33,605 - INFO - Hold-out Acc (catboost): 0.77734\n",
      "2025-04-28 18:10:33,605 - INFO - --- Generating individual predictions for catboost ---\n",
      "2025-04-28 18:10:33,605 - INFO - Generating test predictions using catboost_qual_individual_pred...\n",
      "2025-04-28 18:10:33,615 - INFO - Saved submission: submissions\\solution_catboost_qual_individual_pred_20250428_153717.csv\n",
      "2025-04-28 18:10:33,615 - INFO - Test prediction distribution for 'catboost_qual_individual_pred': {'Medium': 306, 'High': 296, 'Low': 252}\n",
      "2025-04-28 18:10:33,615 - INFO - Saved test prediction summary: results\\catboost_qual_individual_pred_test_prediction_summary_20250428_153717.txt\n",
      "2025-04-28 18:10:33,615 - INFO - Individual prediction file saved for catboost.\n",
      "2025-04-28 18:10:33,615 - INFO - --- Optimizing xgboost (100 trials) ---\n",
      "2025-04-28 18:10:33,615 - INFO - Starting xgboost optimization (100 trials)...\n",
      "2025-04-28 18:10:33,625 - INFO - Optuna timeout for xgboost: 7200s.\n",
      "[I 2025-04-28 18:10:33,847] A new study created in RDB with name: xgboost_opt_20250428_153717\n",
      "2025-04-28 18:10:33,857 - INFO - Setting Optuna timeout 7200s.\n",
      "[I 2025-04-28 18:10:50,083] Trial 0 finished with value: 0.6650358680057389 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 700, 'max_depth': 15, 'learning_rate': 0.035703031172622045, 'subsample': 0.6338529480617128, 'colsample_bytree': 0.911366828706885, 'min_child_weight': 16, 'gamma': 3.722280860980181e-07, 'reg_alpha': 4.489938208428767, 'reg_lambda': 0.1723839416338257}. Best is trial 0 with value: 0.6650358680057389.\n",
      "[I 2025-04-28 18:11:41,115] Trial 1 finished with value: 0.6875179340028694 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 3400, 'max_depth': 10, 'learning_rate': 0.09849205664087277, 'subsample': 0.8411900006396739, 'colsample_bytree': 0.470848023022854, 'min_child_weight': 9, 'gamma': 1.0423454024770742e-07, 'reg_alpha': 1.6528600465611063, 'reg_lambda': 1.0769229376952693e-06}. Best is trial 1 with value: 0.6875179340028694.\n",
      "[I 2025-04-28 18:12:22,074] Trial 2 finished with value: 0.7129268292682926 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2100, 'max_depth': 22, 'learning_rate': 0.0738409194074821, 'subsample': 0.903410553145874, 'colsample_bytree': 0.4607391615393314, 'min_child_weight': 1, 'gamma': 1.286275094892832e-05, 'reg_alpha': 0.4931920526086768, 'reg_lambda': 0.0006401473907617786}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:13:33,995] Trial 3 finished with value: 0.7002008608321377 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 4500, 'max_depth': 6, 'learning_rate': 0.06650839816795588, 'subsample': 0.6394144514074447, 'colsample_bytree': 0.6999921540216386, 'min_child_weight': 4, 'gamma': 6.898630305572115e-06, 'reg_alpha': 0.049478842562448465, 'reg_lambda': 4.4323713183711643e-07}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:13:36,589] Trial 4 finished with value: 0.6601578192252511 and parameters: {'tree_method': 'hist', 'n_estimators': 400, 'max_depth': 23, 'learning_rate': 0.03412676380926697, 'subsample': 0.5300658320305419, 'colsample_bytree': 0.43540010408107277, 'min_child_weight': 18, 'gamma': 2.8650112445383095e-05, 'reg_alpha': 0.3183194491089885, 'reg_lambda': 0.5823086839728829}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:14:56,868] Trial 5 finished with value: 0.683610712577714 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 4800, 'max_depth': 24, 'learning_rate': 0.07282959417322606, 'subsample': 0.9954662360192852, 'colsample_bytree': 0.8203005701714954, 'min_child_weight': 8, 'gamma': 1.2416853285562938e-07, 'reg_alpha': 0.0006192478355636428, 'reg_lambda': 1.5277403278281488e-07}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:15:35,984] Trial 6 finished with value: 0.667948350071736 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2000, 'max_depth': 13, 'learning_rate': 0.03866022915661123, 'subsample': 0.7497046628331051, 'colsample_bytree': 0.8319102212236578, 'min_child_weight': 19, 'gamma': 9.357732057594453e-07, 'reg_alpha': 0.0018752711722795472, 'reg_lambda': 1.6034021195067376}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:15:38,452] Trial 7 finished with value: 0.7060879961740794 and parameters: {'tree_method': 'hist', 'n_estimators': 400, 'max_depth': 15, 'learning_rate': 0.03179970193182553, 'subsample': 0.878951565972022, 'colsample_bytree': 0.4440889301634966, 'min_child_weight': 8, 'gamma': 0.6034264639461518, 'reg_alpha': 4.857496013152153, 'reg_lambda': 0.0001888517250332504}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:16:13,465] Trial 8 finished with value: 0.6679626972740316 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 1400, 'max_depth': 26, 'learning_rate': 0.009902874001819275, 'subsample': 0.5965782666765371, 'colsample_bytree': 0.6654254459173037, 'min_child_weight': 15, 'gamma': 3.346108368947204e-05, 'reg_alpha': 0.3506319699062922, 'reg_lambda': 0.3906866769014776}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:16:48,909] Trial 9 finished with value: 0.7031276901004304 and parameters: {'tree_method': 'hist', 'n_estimators': 3600, 'max_depth': 13, 'learning_rate': 0.008906199288009415, 'subsample': 0.5234223075417574, 'colsample_bytree': 0.49284667442638525, 'min_child_weight': 5, 'gamma': 0.003255562169571613, 'reg_alpha': 0.0011015294634077115, 'reg_lambda': 5.812967715274176e-07}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:17:00,985] Trial 10 finished with value: 0.6924007651841224 and parameters: {'tree_method': 'hist', 'n_estimators': 2400, 'max_depth': 20, 'learning_rate': 0.14399753832535986, 'subsample': 0.9606188438847958, 'colsample_bytree': 0.5946755425335577, 'min_child_weight': 3, 'gamma': 0.002084235698071641, 'reg_alpha': 1.8794464192049723e-06, 'reg_lambda': 0.0015582574649176634}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:17:07,153] Trial 11 finished with value: 0.670908656145385 and parameters: {'tree_method': 'hist', 'n_estimators': 1400, 'max_depth': 20, 'learning_rate': 0.016857069947413122, 'subsample': 0.8687229752906892, 'colsample_bytree': 0.5619268162292098, 'min_child_weight': 1, 'gamma': 0.6163807290462534, 'reg_alpha': 14.60551109412424, 'reg_lambda': 7.585725688615304e-05}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:17:19,921] Trial 12 finished with value: 0.6826303204208513 and parameters: {'tree_method': 'hist', 'n_estimators': 3100, 'max_depth': 18, 'learning_rate': 0.02064579137706348, 'subsample': 0.8806195216365631, 'colsample_bytree': 0.43128072497812703, 'min_child_weight': 13, 'gamma': 0.8339404412111077, 'reg_alpha': 0.01187452801425432, 'reg_lambda': 0.0013304481151511407}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:17:28,937] Trial 13 finished with value: 0.6845863223338116 and parameters: {'tree_method': 'hist', 'n_estimators': 1300, 'max_depth': 17, 'learning_rate': 0.05542381171728397, 'subsample': 0.7769427745382016, 'colsample_bytree': 0.5597899281485014, 'min_child_weight': 7, 'gamma': 0.07331855803033958, 'reg_alpha': 3.7267343296408276e-05, 'reg_lambda': 6.280141974642692e-05}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:19:22,165] Trial 14 finished with value: 0.6787135341941655 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2000, 'max_depth': 9, 'learning_rate': 0.021365957671167005, 'subsample': 0.9335051369463483, 'colsample_bytree': 0.4093391311713812, 'min_child_weight': 12, 'gamma': 0.000865753944706232, 'reg_alpha': 0.06489697493160203, 'reg_lambda': 0.005593285092971888}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:19:44,814] Trial 15 finished with value: 0.6875274988043998 and parameters: {'tree_method': 'hist', 'n_estimators': 900, 'max_depth': 22, 'learning_rate': 0.04991473506014224, 'subsample': 0.7775909463042094, 'colsample_bytree': 0.6309809340024235, 'min_child_weight': 1, 'gamma': 0.020399472500963276, 'reg_alpha': 15.539280985209404, 'reg_lambda': 2.3099429084248513e-05}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:20:59,703] Trial 16 finished with value: 0.6962984218077475 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 4200, 'max_depth': 4, 'learning_rate': 0.10711461409732556, 'subsample': 0.9176417720135579, 'colsample_bytree': 0.5176583471572164, 'min_child_weight': 6, 'gamma': 0.00019468280365666637, 'reg_alpha': 0.8240600248858378, 'reg_lambda': 0.014250551761668888}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:21:22,504] Trial 17 finished with value: 0.6924055475848877 and parameters: {'tree_method': 'hist', 'n_estimators': 2700, 'max_depth': 15, 'learning_rate': 0.02478613944618717, 'subsample': 0.8186748735980778, 'colsample_bytree': 0.7511399469481862, 'min_child_weight': 10, 'gamma': 3.524280686671923e-06, 'reg_alpha': 1.1635201386685438e-07, 'reg_lambda': 7.478827257697567e-06}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:21:53,366] Trial 18 finished with value: 0.7089909134385461 and parameters: {'tree_method': 'hist', 'n_estimators': 1900, 'max_depth': 11, 'learning_rate': 0.015392224174536456, 'subsample': 0.717866772499174, 'colsample_bytree': 0.9641552532762055, 'min_child_weight': 3, 'gamma': 0.00016332562937794467, 'reg_alpha': 0.021519333644994552, 'reg_lambda': 0.0002973048713177893}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:23:03,195] Trial 19 finished with value: 0.7041176470588235 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2000, 'max_depth': 10, 'learning_rate': 0.013102450212456354, 'subsample': 0.7021097505729695, 'colsample_bytree': 0.977608378875217, 'min_child_weight': 3, 'gamma': 0.00016033556642871416, 'reg_alpha': 5.858871504142103e-05, 'reg_lambda': 12.578851308676501}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:24:12,200] Trial 20 finished with value: 0.7129029172644668 and parameters: {'tree_method': 'hist', 'n_estimators': 3000, 'max_depth': 7, 'learning_rate': 0.007425747475973259, 'subsample': 0.7094265726875296, 'colsample_bytree': 0.9649648167316305, 'min_child_weight': 1, 'gamma': 3.476627634219805e-05, 'reg_alpha': 0.011569937577894257, 'reg_lambda': 0.03817990359571347}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:25:04,139] Trial 21 finished with value: 0.7119273075083692 and parameters: {'tree_method': 'hist', 'n_estimators': 2900, 'max_depth': 7, 'learning_rate': 0.012415984974107959, 'subsample': 0.6875677400284953, 'colsample_bytree': 0.9944629951843995, 'min_child_weight': 1, 'gamma': 3.0146777812195764e-05, 'reg_alpha': 0.012352900530956315, 'reg_lambda': 0.03993093495028378}. Best is trial 2 with value: 0.7129268292682926.\n",
      "[I 2025-04-28 18:26:02,947] Trial 22 finished with value: 0.7148637015781922 and parameters: {'tree_method': 'hist', 'n_estimators': 2700, 'max_depth': 7, 'learning_rate': 0.007393574800629698, 'subsample': 0.6618769018619627, 'colsample_bytree': 0.8799355821535892, 'min_child_weight': 1, 'gamma': 2.3964638047228112e-05, 'reg_alpha': 0.007590121439418437, 'reg_lambda': 0.06856099553565588}. Best is trial 22 with value: 0.7148637015781922.\n",
      "[I 2025-04-28 18:26:46,306] Trial 23 finished with value: 0.714858919177427 and parameters: {'tree_method': 'hist', 'n_estimators': 3700, 'max_depth': 4, 'learning_rate': 0.00724171574457941, 'subsample': 0.6581654639871064, 'colsample_bytree': 0.8937255185372883, 'min_child_weight': 1, 'gamma': 4.844610436244363e-06, 'reg_alpha': 0.0001180641545542732, 'reg_lambda': 0.054887841162034476}. Best is trial 22 with value: 0.7148637015781922.\n",
      "[I 2025-04-28 18:27:18,520] Trial 24 finished with value: 0.6982544237207078 and parameters: {'tree_method': 'hist', 'n_estimators': 3700, 'max_depth': 4, 'learning_rate': 0.008456078606615078, 'subsample': 0.5771682520597493, 'colsample_bytree': 0.8795984394512568, 'min_child_weight': 5, 'gamma': 3.34926329466412e-06, 'reg_alpha': 0.00013411139369489016, 'reg_lambda': 0.0018900444939467394}. Best is trial 22 with value: 0.7148637015781922.\n",
      "[I 2025-04-28 18:28:06,363] Trial 25 finished with value: 0.7002152080344333 and parameters: {'tree_method': 'hist', 'n_estimators': 4000, 'max_depth': 6, 'learning_rate': 0.0070814009828635924, 'subsample': 0.6569000288726882, 'colsample_bytree': 0.7673764701762936, 'min_child_weight': 3, 'gamma': 8.617844079701885e-07, 'reg_alpha': 8.9747738265724e-06, 'reg_lambda': 0.07643800007748736}. Best is trial 22 with value: 0.7148637015781922.\n",
      "[I 2025-04-28 18:29:01,148] Trial 26 finished with value: 0.7050836920133907 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2500, 'max_depth': 4, 'learning_rate': 0.011076068445311171, 'subsample': 0.5873670540624065, 'colsample_bytree': 0.9094358186467318, 'min_child_weight': 2, 'gamma': 1.0278885828795003e-05, 'reg_alpha': 0.003756595402324647, 'reg_lambda': 2.948395915818558}. Best is trial 22 with value: 0.7148637015781922.\n",
      "[I 2025-04-28 18:29:19,810] Trial 27 finished with value: 0.6914060258249641 and parameters: {'tree_method': 'hist', 'n_estimators': 3300, 'max_depth': 8, 'learning_rate': 0.08885017116484045, 'subsample': 0.6577889587905561, 'colsample_bytree': 0.7782865776523562, 'min_child_weight': 5, 'gamma': 1.3311972790185992e-06, 'reg_alpha': 0.00033834252742857715, 'reg_lambda': 0.020076547040138375}. Best is trial 22 with value: 0.7148637015781922.\n",
      "[I 2025-04-28 18:30:06,539] Trial 28 finished with value: 0.6865231946437111 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2400, 'max_depth': 26, 'learning_rate': 0.04797020406151929, 'subsample': 0.746041685979667, 'colsample_bytree': 0.8288880682183736, 'min_child_weight': 6, 'gamma': 8.115511104798327e-05, 'reg_alpha': 1.4355850762246581e-05, 'reg_lambda': 0.006920293031445582}. Best is trial 22 with value: 0.7148637015781922.\n",
      "[I 2025-04-28 18:31:42,075] Trial 29 finished with value: 0.7109373505499761 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 3900, 'max_depth': 12, 'learning_rate': 0.015937473201351807, 'subsample': 0.6152502389405898, 'colsample_bytree': 0.9005273347135547, 'min_child_weight': 2, 'gamma': 0.0006507998109407925, 'reg_alpha': 1.0528460677254975e-06, 'reg_lambda': 0.1894352568898068}. Best is trial 22 with value: 0.7148637015781922.\n",
      "[I 2025-04-28 18:32:02,857] Trial 30 finished with value: 0.6933572453371593 and parameters: {'tree_method': 'hist', 'n_estimators': 5000, 'max_depth': 17, 'learning_rate': 0.12442970997522262, 'subsample': 0.5552045907510597, 'colsample_bytree': 0.8578392186757026, 'min_child_weight': 4, 'gamma': 4.1231860937498494e-07, 'reg_alpha': 0.12230904344701216, 'reg_lambda': 0.0005821013795545011}. Best is trial 22 with value: 0.7148637015781922.\n",
      "[I 2025-04-28 18:33:05,037] Trial 31 finished with value: 0.7197465327594452 and parameters: {'tree_method': 'hist', 'n_estimators': 2900, 'max_depth': 6, 'learning_rate': 0.007186288487661066, 'subsample': 0.6860791158467656, 'colsample_bytree': 0.930833997292163, 'min_child_weight': 1, 'gamma': 2.039992636131286e-05, 'reg_alpha': 0.005822795272857608, 'reg_lambda': 0.10329976993410896}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:33:59,597] Trial 32 finished with value: 0.7158297465327594 and parameters: {'tree_method': 'hist', 'n_estimators': 3300, 'max_depth': 6, 'learning_rate': 0.0070225417571495264, 'subsample': 0.6713577778586394, 'colsample_bytree': 0.9297462024853583, 'min_child_weight': 2, 'gamma': 9.306679351224656e-06, 'reg_alpha': 0.0002521359362974135, 'reg_lambda': 0.10510753840156034}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:34:47,800] Trial 33 finished with value: 0.7128981348637016 and parameters: {'tree_method': 'hist', 'n_estimators': 3400, 'max_depth': 6, 'learning_rate': 0.008946960572184281, 'subsample': 0.6614861068478618, 'colsample_bytree': 0.9329746652306321, 'min_child_weight': 2, 'gamma': 3.2715405854068207e-06, 'reg_alpha': 0.00024258592290187035, 'reg_lambda': 0.10273108932185203}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:35:31,365] Trial 34 finished with value: 0.7002008608321377 and parameters: {'tree_method': 'hist', 'n_estimators': 4300, 'max_depth': 5, 'learning_rate': 0.010680728241125544, 'subsample': 0.6284219297223442, 'colsample_bytree': 0.9341071855596892, 'min_child_weight': 4, 'gamma': 9.367137517307412e-06, 'reg_alpha': 0.0021060603210444465, 'reg_lambda': 1.1371213225345025}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:36:34,741] Trial 35 finished with value: 0.7138928742228599 and parameters: {'tree_method': 'hist', 'n_estimators': 2800, 'max_depth': 8, 'learning_rate': 0.008459039594674287, 'subsample': 0.6658432056719206, 'colsample_bytree': 0.8676453845177297, 'min_child_weight': 2, 'gamma': 6.617539878434103e-05, 'reg_alpha': 0.003812953960026766, 'reg_lambda': 5.434678338409747}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:37:17,955] Trial 36 finished with value: 0.7080200860832138 and parameters: {'tree_method': 'hist', 'n_estimators': 3200, 'max_depth': 5, 'learning_rate': 0.007318239951604147, 'subsample': 0.7271235573012478, 'colsample_bytree': 0.9405844699836915, 'min_child_weight': 4, 'gamma': 1.3478154527269295e-05, 'reg_alpha': 0.0005654623957021696, 'reg_lambda': 0.2673428140242425}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:38:07,661] Trial 37 finished with value: 0.7099760879961741 and parameters: {'tree_method': 'hist', 'n_estimators': 3500, 'max_depth': 9, 'learning_rate': 0.013115719151268834, 'subsample': 0.8041443048907793, 'colsample_bytree': 0.7964630521288265, 'min_child_weight': 1, 'gamma': 1.4218346025861856e-07, 'reg_alpha': 9.962425903904234e-05, 'reg_lambda': 0.009374387390320304}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:38:34,229] Trial 38 finished with value: 0.6797082735533237 and parameters: {'tree_method': 'hist', 'n_estimators': 3900, 'max_depth': 7, 'learning_rate': 0.010102047214545957, 'subsample': 0.6866827321781857, 'colsample_bytree': 0.7336208840343895, 'min_child_weight': 16, 'gamma': 1.9379321683551007e-06, 'reg_alpha': 8.600661709714477e-06, 'reg_lambda': 0.7642345560733665}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:39:02,620] Trial 39 finished with value: 0.6699091343854615 and parameters: {'tree_method': 'hist', 'n_estimators': 4600, 'max_depth': 5, 'learning_rate': 0.00805029848278719, 'subsample': 0.6128774995801395, 'colsample_bytree': 0.8446369417182037, 'min_child_weight': 20, 'gamma': 5.20640520796003e-07, 'reg_alpha': 0.004049733850382889, 'reg_lambda': 0.06234693336101531}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:39:31,773] Trial 40 finished with value: 0.6933668101386896 and parameters: {'tree_method': 'hist', 'n_estimators': 2600, 'max_depth': 10, 'learning_rate': 0.01146681672250708, 'subsample': 0.5529045645694415, 'colsample_bytree': 0.9210990953581097, 'min_child_weight': 6, 'gamma': 1.7281907367423977e-05, 'reg_alpha': 2.2527852836208215e-05, 'reg_lambda': 0.0037387901674090685}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:41:34,256] Trial 41 finished with value: 0.7158393113342898 and parameters: {'tree_method': 'hist', 'n_estimators': 2700, 'max_depth': 8, 'learning_rate': 0.009101449561157455, 'subsample': 0.6758384970315954, 'colsample_bytree': 0.867017225699157, 'min_child_weight': 2, 'gamma': 7.183776734924291e-05, 'reg_alpha': 0.0012349940295069118, 'reg_lambda': 6.223587388396923}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:45:33,506] Trial 42 finished with value: 0.711922525107604 and parameters: {'tree_method': 'hist', 'n_estimators': 2300, 'max_depth': 8, 'learning_rate': 0.007095940263854492, 'subsample': 0.6826553878247981, 'colsample_bytree': 0.8844537034632147, 'min_child_weight': 2, 'gamma': 7.365796821123985e-05, 'reg_alpha': 0.0008144580452233913, 'reg_lambda': 3.1140359355940066}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:48:57,197] Trial 43 finished with value: 0.703142037302726 and parameters: {'tree_method': 'hist', 'n_estimators': 3000, 'max_depth': 6, 'learning_rate': 0.009563785200956122, 'subsample': 0.6362027222224151, 'colsample_bytree': 0.7992374769001358, 'min_child_weight': 3, 'gamma': 5.878906374082677e-06, 'reg_alpha': 0.000197354936568331, 'reg_lambda': 12.044324231899601}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:52:03,128] Trial 44 finished with value: 0.7002056432329029 and parameters: {'tree_method': 'hist', 'n_estimators': 3600, 'max_depth': 9, 'learning_rate': 0.008319293575535425, 'subsample': 0.7378890339385648, 'colsample_bytree': 0.704726561151932, 'min_child_weight': 4, 'gamma': 0.0006248017459348465, 'reg_alpha': 0.0014544057816234235, 'reg_lambda': 0.4689986387191709}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:54:51,538] Trial 45 finished with value: 0.7050884744141559 and parameters: {'tree_method': 'hist', 'n_estimators': 3200, 'max_depth': 5, 'learning_rate': 0.009528903592770558, 'subsample': 0.6491240818997247, 'colsample_bytree': 0.8955585478342368, 'min_child_weight': 2, 'gamma': 1.950992322787434e-05, 'reg_alpha': 0.04232151290951447, 'reg_lambda': 0.15416947963364425}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:56:19,578] Trial 46 finished with value: 0.6972692491630799 and parameters: {'tree_method': 'hist', 'n_estimators': 1700, 'max_depth': 12, 'learning_rate': 0.007011566781054395, 'subsample': 0.762549424959977, 'colsample_bytree': 0.9528325687141564, 'min_child_weight': 9, 'gamma': 5.6072255313972684e-06, 'reg_alpha': 0.005922286337602871, 'reg_lambda': 0.02588259318512419}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:57:52,152] Trial 47 finished with value: 0.7031324725011956 and parameters: {'tree_method': 'hist', 'n_estimators': 2800, 'max_depth': 4, 'learning_rate': 0.014588647531754158, 'subsample': 0.5054027294050413, 'colsample_bytree': 0.8532282220143608, 'min_child_weight': 1, 'gamma': 4.32051270015299e-05, 'reg_alpha': 0.0004353976950907969, 'reg_lambda': 1.4518245848741165}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 18:59:54,258] Trial 48 finished with value: 0.6787374461979915 and parameters: {'tree_method': 'hist', 'n_estimators': 2300, 'max_depth': 7, 'learning_rate': 0.011143470536047391, 'subsample': 0.6757854194136159, 'colsample_bytree': 0.9934025839695574, 'min_child_weight': 12, 'gamma': 0.00025348592119641506, 'reg_alpha': 0.21736535138606983, 'reg_lambda': 0.34651347930741594}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:02:39,687] Trial 49 finished with value: 0.6982592061214731 and parameters: {'tree_method': 'hist', 'n_estimators': 3700, 'max_depth': 14, 'learning_rate': 0.019977080785721593, 'subsample': 0.7021594377612589, 'colsample_bytree': 0.8124930595892831, 'min_child_weight': 5, 'gamma': 9.350757490255415e-05, 'reg_alpha': 6.849112098897333e-05, 'reg_lambda': 5.377611759992626}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:05:04,060] Trial 50 finished with value: 0.6884839789574366 and parameters: {'tree_method': 'hist', 'n_estimators': 2600, 'max_depth': 10, 'learning_rate': 0.0300994061688724, 'subsample': 0.6065725511342, 'colsample_bytree': 0.9072019186379642, 'min_child_weight': 7, 'gamma': 1.908986399445148e-06, 'reg_alpha': 0.03649052255747344, 'reg_lambda': 19.53127534949259}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:07:28,254] Trial 51 finished with value: 0.7070492587278814 and parameters: {'tree_method': 'hist', 'n_estimators': 2800, 'max_depth': 8, 'learning_rate': 0.008322056630662473, 'subsample': 0.6290464335715418, 'colsample_bytree': 0.8723524441423348, 'min_child_weight': 2, 'gamma': 5.411727208988217e-05, 'reg_alpha': 0.0019112317186281032, 'reg_lambda': 3.314773895918619}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:11:34,536] Trial 52 finished with value: 0.7070349115255858 and parameters: {'tree_method': 'hist', 'n_estimators': 3100, 'max_depth': 8, 'learning_rate': 0.009223962855413811, 'subsample': 0.6743842995416433, 'colsample_bytree': 0.873739404489429, 'min_child_weight': 3, 'gamma': 1.886389897042654e-05, 'reg_alpha': 0.004638081900970645, 'reg_lambda': 0.7537293170082017}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:18:07,078] Trial 53 finished with value: 0.7119416547106647 and parameters: {'tree_method': 'hist', 'n_estimators': 3300, 'max_depth': 6, 'learning_rate': 0.008014815598551758, 'subsample': 0.7171279687044174, 'colsample_bytree': 0.831549039671769, 'min_child_weight': 1, 'gamma': 0.0022417455178911122, 'reg_alpha': 0.0010150903793160074, 'reg_lambda': 6.2227633509615154}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:22:15,001] Trial 54 finished with value: 0.7109469153515064 and parameters: {'tree_method': 'hist', 'n_estimators': 2200, 'max_depth': 9, 'learning_rate': 0.010217783975479556, 'subsample': 0.648907476423606, 'colsample_bytree': 0.9208827418599148, 'min_child_weight': 2, 'gamma': 9.371342567936786e-05, 'reg_alpha': 0.00799433768572675, 'reg_lambda': 1.9602813241939099}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:25:07,786] Trial 55 finished with value: 0.7099713055954089 and parameters: {'tree_method': 'hist', 'n_estimators': 2800, 'max_depth': 7, 'learning_rate': 0.007922983609420535, 'subsample': 0.6941602866438877, 'colsample_bytree': 0.9550477969914895, 'min_child_weight': 3, 'gamma': 6.2944823287125325e-06, 'reg_alpha': 0.02391910279279963, 'reg_lambda': 0.10674246395329756}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:28:25,902] Trial 56 finished with value: 0.7168149210903874 and parameters: {'tree_method': 'hist', 'n_estimators': 2900, 'max_depth': 5, 'learning_rate': 0.012351860322288083, 'subsample': 0.6685395621738212, 'colsample_bytree': 0.8596240998263303, 'min_child_weight': 1, 'gamma': 0.00033483499162166504, 'reg_alpha': 0.09006126484283358, 'reg_lambda': 0.014164920238109235}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:29:42,619] Trial 57 finished with value: 0.7089909134385461 and parameters: {'tree_method': 'hist', 'n_estimators': 1800, 'max_depth': 5, 'learning_rate': 0.01792390213263094, 'subsample': 0.6327800311437135, 'colsample_bytree': 0.9804668946289541, 'min_child_weight': 1, 'gamma': 0.00040567913705248616, 'reg_alpha': 0.11308711071049263, 'reg_lambda': 0.004125246871775773}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:32:27,950] Trial 58 finished with value: 0.6992252510760402 and parameters: {'tree_method': 'hist', 'n_estimators': 3400, 'max_depth': 4, 'learning_rate': 0.012493419664359733, 'subsample': 0.5720612695172871, 'colsample_bytree': 0.8488588198031205, 'min_child_weight': 4, 'gamma': 0.0010973661589034147, 'reg_alpha': 3.0616764405240757, 'reg_lambda': 0.01676787258378779}. Best is trial 31 with value: 0.7197465327594452.\n",
      "[I 2025-04-28 19:34:39,442] Trial 59 finished with value: 0.7207221425155428 and parameters: {'tree_method': 'hist', 'n_estimators': 3000, 'max_depth': 6, 'learning_rate': 0.008994695843375951, 'subsample': 0.7311259979439593, 'colsample_bytree': 0.8893468920213988, 'min_child_weight': 1, 'gamma': 2.8926893286187967e-05, 'reg_alpha': 0.022158900325971953, 'reg_lambda': 0.051080227833817736}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 19:36:12,608] Trial 60 finished with value: 0.6748158775705404 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 2500, 'max_depth': 6, 'learning_rate': 0.013129118702473514, 'subsample': 0.7643054839748544, 'colsample_bytree': 0.9483618674094958, 'min_child_weight': 18, 'gamma': 0.00013357032604413162, 'reg_alpha': 1.1048948229205766, 'reg_lambda': 0.0018830465698699344}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 19:39:41,992] Trial 61 finished with value: 0.7177953132472501 and parameters: {'tree_method': 'hist', 'n_estimators': 3000, 'max_depth': 6, 'learning_rate': 0.009171813944383924, 'subsample': 0.7300008754678192, 'colsample_bytree': 0.8900416709555898, 'min_child_weight': 1, 'gamma': 3.252112116250104e-05, 'reg_alpha': 0.07537787483829422, 'reg_lambda': 0.06411627801619436}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 19:42:36,291] Trial 62 finished with value: 0.7011908177905308 and parameters: {'tree_method': 'hist', 'n_estimators': 3000, 'max_depth': 7, 'learning_rate': 0.009510512198597623, 'subsample': 0.7404274131386702, 'colsample_bytree': 0.9201968091152465, 'min_child_weight': 3, 'gamma': 0.007421414110223378, 'reg_alpha': 0.07959270815739349, 'reg_lambda': 0.03348275206018845}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 19:46:05,861] Trial 63 finished with value: 0.7178048780487805 and parameters: {'tree_method': 'hist', 'n_estimators': 3000, 'max_depth': 6, 'learning_rate': 0.011888655764492188, 'subsample': 0.7215343960580535, 'colsample_bytree': 0.816341204387261, 'min_child_weight': 1, 'gamma': 2.4956120195271036e-05, 'reg_alpha': 0.3683174683501311, 'reg_lambda': 0.008998914610651294}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 19:47:50,240] Trial 64 finished with value: 0.680674318507891 and parameters: {'tree_method': 'hist', 'n_estimators': 3000, 'max_depth': 6, 'learning_rate': 0.011628100274658182, 'subsample': 0.7923258052980122, 'colsample_bytree': 0.8143961988933733, 'min_child_weight': 14, 'gamma': 3.484011456255377e-05, 'reg_alpha': 0.4197218075752229, 'reg_lambda': 0.013224014833209502}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 19:50:29,443] Trial 65 finished with value: 0.6992348158775705 and parameters: {'tree_method': 'hist', 'n_estimators': 3200, 'max_depth': 5, 'learning_rate': 0.014398257842447719, 'subsample': 0.728204633457168, 'colsample_bytree': 0.842994371661623, 'min_child_weight': 2, 'gamma': 2.8314182520541094e-05, 'reg_alpha': 0.22009229931606972, 'reg_lambda': 0.1609510566200201}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 19:52:57,140] Trial 66 finished with value: 0.7041224294595887 and parameters: {'tree_method': 'hist', 'n_estimators': 2900, 'max_depth': 11, 'learning_rate': 0.025114984239624077, 'subsample': 0.7100231903108637, 'colsample_bytree': 0.6448846240737781, 'min_child_weight': 1, 'gamma': 0.0003784478463585879, 'reg_alpha': 0.6315403544559073, 'reg_lambda': 0.0086821056350188}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 19:56:04,210] Trial 67 finished with value: 0.7128933524629364 and parameters: {'tree_method': 'hist', 'n_estimators': 2600, 'max_depth': 6, 'learning_rate': 0.010463172365682658, 'subsample': 0.8282153366933667, 'colsample_bytree': 0.8978404529265646, 'min_child_weight': 1, 'gamma': 1.316910655185262e-05, 'reg_alpha': 0.019967163499377362, 'reg_lambda': 0.004146048772047568}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 19:58:52,900] Trial 68 finished with value: 0.7060593017694883 and parameters: {'tree_method': 'gpu_hist', 'n_estimators': 3500, 'max_depth': 5, 'learning_rate': 0.009136540847652103, 'subsample': 0.7566070530480272, 'colsample_bytree': 0.7767902704088598, 'min_child_weight': 3, 'gamma': 0.00013959344972332025, 'reg_alpha': 5.635937928750892, 'reg_lambda': 0.036554664504083303}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 20:00:59,002] Trial 69 finished with value: 0.716805356288857 and parameters: {'tree_method': 'hist', 'n_estimators': 3100, 'max_depth': 4, 'learning_rate': 0.012005362228514489, 'subsample': 0.697805696508877, 'colsample_bytree': 0.7393266737823487, 'min_child_weight': 2, 'gamma': 9.470386159925767e-06, 'reg_alpha': 0.015032456296543656, 'reg_lambda': 0.0008443934824950541}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 20:02:14,525] Trial 70 finished with value: 0.7109516977522716 and parameters: {'tree_method': 'hist', 'n_estimators': 2100, 'max_depth': 4, 'learning_rate': 0.017419595952477086, 'subsample': 0.7835502591768061, 'colsample_bytree': 0.7353548240933084, 'min_child_weight': 2, 'gamma': 4.699157656812084e-05, 'reg_alpha': 0.20065426114536075, 'reg_lambda': 9.939246623181605e-05}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 20:03:00,326] Trial 71 finished with value: 0.7177857484457197 and parameters: {'tree_method': 'hist', 'n_estimators': 3100, 'max_depth': 6, 'learning_rate': 0.012175927566156621, 'subsample': 0.697354747203952, 'colsample_bytree': 0.80367648184579, 'min_child_weight': 1, 'gamma': 8.733431950133801e-06, 'reg_alpha': 0.01592256300892315, 'reg_lambda': 0.0005234613385032318}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 20:03:31,523] Trial 72 finished with value: 0.7158393113342898 and parameters: {'tree_method': 'hist', 'n_estimators': 2900, 'max_depth': 4, 'learning_rate': 0.011602310534667485, 'subsample': 0.7259641020844119, 'colsample_bytree': 0.7048554546363875, 'min_child_weight': 1, 'gamma': 3.0950316933817587e-06, 'reg_alpha': 0.01748844871754885, 'reg_lambda': 0.0009759248229408464}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 20:04:05,621] Trial 73 finished with value: 0.7060593017694883 and parameters: {'tree_method': 'hist', 'n_estimators': 3100, 'max_depth': 5, 'learning_rate': 0.01349163370729704, 'subsample': 0.6979237687766051, 'colsample_bytree': 0.7917111148035632, 'min_child_weight': 2, 'gamma': 0.00024105563020430029, 'reg_alpha': 0.0628797235610461, 'reg_lambda': 0.00033385094912789727}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 20:04:36,212] Trial 74 finished with value: 0.707039693926351 and parameters: {'tree_method': 'hist', 'n_estimators': 2700, 'max_depth': 7, 'learning_rate': 0.014533024078270855, 'subsample': 0.7136026251939565, 'colsample_bytree': 0.7539480397769731, 'min_child_weight': 3, 'gamma': 9.613793944174641e-06, 'reg_alpha': 0.011979237952174123, 'reg_lambda': 0.0006439054595482505}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 20:05:50,403] Trial 75 finished with value: 0.7119273075083692 and parameters: {'tree_method': 'hist', 'n_estimators': 2500, 'max_depth': 8, 'learning_rate': 0.012315264912470762, 'subsample': 0.6894523829152263, 'colsample_bytree': 0.8212086287797611, 'min_child_weight': 1, 'gamma': 3.079071164212647e-05, 'reg_alpha': 0.036646148377509825, 'reg_lambda': 0.00013728257807277722}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 20:10:11,406] Trial 76 finished with value: 0.7109564801530368 and parameters: {'tree_method': 'hist', 'n_estimators': 3100, 'max_depth': 6, 'learning_rate': 0.010510689695809488, 'subsample': 0.7495096335787728, 'colsample_bytree': 0.8623460196537983, 'min_child_weight': 1, 'gamma': 1.526167338803637e-05, 'reg_alpha': 0.11404219839312457, 'reg_lambda': 1.5416871364748842e-05}. Best is trial 59 with value: 0.7207221425155428.\n",
      "[I 2025-04-28 20:11:53,132] Trial 77 finished with value: 0.6962984218077475 and parameters: {'tree_method': 'hist', 'n_estimators': 2400, 'max_depth': 5, 'learning_rate': 0.01618567204891837, 'subsample': 0.7329933082135242, 'colsample_bytree': 0.7354533152668856, 'min_child_weight': 4, 'gamma': 2.3888985159804116e-05, 'reg_alpha': 0.0030683705403100193, 'reg_lambda': 0.0025956184565648415}. Best is trial 59 with value: 0.7207221425155428.\n",
      "2025-04-28 20:11:53,197 - INFO - Opt complete xgboost. Best CV score: 0.72072. Best params: {'tree_method': 'hist', 'n_estimators': 3000, 'max_depth': 6, 'learning_rate': 0.008994695843375951, 'subsample': 0.7311259979439593, 'colsample_bytree': 0.8893468920213988, 'min_child_weight': 1, 'gamma': 2.8926893286187967e-05, 'reg_alpha': 0.022158900325971953, 'reg_lambda': 0.051080227833817736}\n",
      "2025-04-28 20:11:53,197 - INFO - Saved Optuna summary: optuna_trials/xgboost_study_summary_20250428_153717.txt\n",
      "2025-04-28 20:11:53,213 - INFO - Instantiating final xgboost model...\n",
      "2025-04-28 20:11:53,213 - INFO - XGBoost final model - balancing via sample_weight in fit.\n",
      "2025-04-28 20:11:53,228 - INFO - Fitting final xgboost model...\n",
      "2025-04-28 20:11:53,228 - INFO - Fitting xgboost with additional fit parameters: ['sample_weight']\n",
      "2025-04-28 20:12:43,649 - INFO - Final xgboost fitted in 50.42s.\n",
      "2025-04-28 20:12:43,649 - INFO - Saving final xgboost model...\n",
      "2025-04-28 20:12:44,172 - INFO - Saved final xgboost via joblib: models/xgboost_20250428_153717.joblib\n",
      "2025-04-28 20:12:44,172 - INFO - Attempting calibration for xgboost...\n",
      "2025-04-28 20:14:10,528 - INFO - Saved calibrated model: calibrated_models/xgboost_calibrated_20250428_153717.joblib\n",
      "2025-04-28 20:14:10,528 - INFO - Saving importance xgboost...\n",
      "2025-04-28 20:14:12,004 - INFO - Saved importance plot: plots\\xgboost_feature_importance_20250428_153717.png\n",
      "2025-04-28 20:14:12,020 - INFO - Saved importance csv: results\\xgboost_feature_importance_20250428_153717.csv\n",
      "2025-04-28 20:14:12,041 - INFO - Qualification Check for xgboost: final_model exists? True, best_cv_score=0.7207221, threshold=0.72, comparison result: True\n",
      "2025-04-28 20:14:12,041 - INFO - +++ QUALIFIED: xgboost (CV Score: 0.72072)\n",
      "2025-04-28 20:14:12,041 - INFO - Evaluating model 'xgboost_qualified_holdout_eval'...\n",
      "2025-04-28 20:14:12,306 - INFO - Evaluation Results for 'xgboost_qualified_holdout_eval':\n",
      "2025-04-28 20:14:12,322 - INFO -   Accuracy: 0.77344\n",
      "2025-04-28 20:14:12,322 - INFO - Saved evaluation summary: results\\xgboost_qualified_holdout_eval_evaluation_20250428_153717.txt\n",
      "2025-04-28 20:14:12,909 - INFO - Saved confusion matrix plot: plots\\xgboost_qualified_holdout_eval_confusion_matrix_20250428_153717.png\n",
      "2025-04-28 20:14:12,909 - INFO - Hold-out Acc (xgboost): 0.77344\n",
      "2025-04-28 20:14:12,909 - INFO - --- Generating individual predictions for xgboost ---\n",
      "2025-04-28 20:14:12,909 - INFO - Generating test predictions using xgboost_qual_individual_pred...\n",
      "2025-04-28 20:14:13,447 - INFO - Saved submission: submissions\\solution_xgboost_qual_individual_pred_20250428_153717.csv\n",
      "2025-04-28 20:14:13,463 - INFO - Test prediction distribution for 'xgboost_qual_individual_pred': {'Medium': 343, 'High': 271, 'Low': 240}\n",
      "2025-04-28 20:14:13,463 - INFO - Saved test prediction summary: results\\xgboost_qual_individual_pred_test_prediction_summary_20250428_153717.txt\n",
      "2025-04-28 20:14:13,463 - INFO - Individual prediction file saved for xgboost.\n",
      "2025-04-28 20:14:13,463 - INFO - --- Optimizing lightgbm (100 trials) ---\n",
      "2025-04-28 20:14:13,479 - INFO - Starting lightgbm optimization (100 trials)...\n",
      "2025-04-28 20:14:13,479 - INFO - Optuna timeout for lightgbm: 7200s.\n",
      "[I 2025-04-28 20:14:13,956] A new study created in RDB with name: lightgbm_opt_20250428_153717\n",
      "2025-04-28 20:14:13,975 - INFO - Setting Optuna timeout 7200s.\n",
      "[I 2025-04-28 20:21:17,428] Trial 0 finished with value: 0.6904399808703969 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2400, 'learning_rate': 0.029007848309083893, 'num_leaves': 80, 'max_depth': 6, 'subsample': 0.6662768227880735, 'colsample_bytree': 0.7082716594145941, 'reg_alpha': 0.0867758226685185, 'reg_lambda': 3.205259715199613e-08, 'min_child_samples': 35, 'boosting_type': 'dart'}. Best is trial 0 with value: 0.6904399808703969.\n",
      "[I 2025-04-28 20:22:10,329] Trial 1 finished with value: 0.7207364897178383 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 4000, 'learning_rate': 0.04344128974282905, 'num_leaves': 425, 'max_depth': 32, 'subsample': 0.54698867642646, 'colsample_bytree': 0.6590145277797956, 'reg_alpha': 0.0015412850330777277, 'reg_lambda': 9.562379722123913e-06, 'min_child_samples': 3, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:32:06,198] Trial 2 finished with value: 0.6884935437589671 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3200, 'learning_rate': 0.07375427057559576, 'num_leaves': 160, 'max_depth': 48, 'subsample': 0.8508139543215294, 'colsample_bytree': 0.5306742045491682, 'reg_alpha': 5.104313002320244e-07, 'reg_lambda': 3.3310213293558023e-06, 'min_child_samples': 44, 'boosting_type': 'dart'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:36:01,283] Trial 3 finished with value: 0.6806695361071258 and parameters: {'class_weight_option': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 4900, 'learning_rate': 0.13898103844000936, 'num_leaves': 485, 'max_depth': 34, 'subsample': 0.7962721507273092, 'colsample_bytree': 0.6946466417457937, 'reg_alpha': 1.3000827530051624e-05, 'reg_lambda': 6.039673266339921e-08, 'min_child_samples': 44, 'boosting_type': 'dart'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:36:02,280] Trial 4 finished with value: 0.6923720707795313 and parameters: {'class_weight_option': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 5000, 'learning_rate': 0.041945994528447814, 'num_leaves': 125, 'max_depth': 23, 'subsample': 0.7723027112857308, 'colsample_bytree': 0.7231648519127745, 'reg_alpha': 0.00026106615756653893, 'reg_lambda': 2.900683517989533e-08, 'min_child_samples': 26, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:36:03,389] Trial 5 finished with value: 0.6738498326159732 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 1400, 'learning_rate': 0.04193175312863037, 'num_leaves': 285, 'max_depth': 37, 'subsample': 0.7668023375319211, 'colsample_bytree': 0.8863335276245656, 'reg_alpha': 12.88005530797913, 'reg_lambda': 0.043167272483080575, 'min_child_samples': 12, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:39:43,900] Trial 6 finished with value: 0.7158297465327594 and parameters: {'class_weight_option': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 4600, 'learning_rate': 0.007247610596487866, 'num_leaves': 60, 'max_depth': 16, 'subsample': 0.685590667791222, 'colsample_bytree': 0.9200775200135733, 'reg_alpha': 1.6183616292760326e-08, 'reg_lambda': 5.547902550211449e-07, 'min_child_samples': 3, 'boosting_type': 'dart'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:49:33,265] Trial 7 finished with value: 0.702156862745098 and parameters: {'class_weight_option': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 4900, 'learning_rate': 0.14845945020842288, 'num_leaves': 440, 'max_depth': 18, 'subsample': 0.7491077206272385, 'colsample_bytree': 0.5569086913518044, 'reg_alpha': 0.9200883121298778, 'reg_lambda': 0.013557427449886243, 'min_child_samples': 4, 'boosting_type': 'dart'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:50:27,496] Trial 8 finished with value: 0.6767623146819703 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 1000, 'learning_rate': 0.007899675910398031, 'num_leaves': 420, 'max_depth': 24, 'subsample': 0.628589763211415, 'colsample_bytree': 0.5128896093752889, 'reg_alpha': 0.0005240071960689967, 'reg_lambda': 0.015528688482084824, 'min_child_samples': 40, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:51:46,224] Trial 9 finished with value: 0.6855571496891439 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 600, 'learning_rate': 0.04786263566898631, 'num_leaves': 375, 'max_depth': 49, 'subsample': 0.8906426619234337, 'colsample_bytree': 0.7409720871095262, 'reg_alpha': 0.08131346970917146, 'reg_lambda': 2.6964315954199342e-05, 'min_child_samples': 48, 'boosting_type': 'dart'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:52:11,169] Trial 10 finished with value: 0.673845050215208 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 3600, 'learning_rate': 0.017409587519359546, 'num_leaves': 305, 'max_depth': 38, 'subsample': 0.5146188836163702, 'colsample_bytree': 0.6243424783789318, 'reg_alpha': 0.00101715447151027, 'reg_lambda': 6.40681673055976, 'min_child_samples': 60, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:53:35,747] Trial 11 finished with value: 0.7158345289335246 and parameters: {'class_weight_option': None, 'n_estimators': 3900, 'learning_rate': 0.008266722184855167, 'num_leaves': 200, 'max_depth': 11, 'subsample': 0.5128297471583758, 'colsample_bytree': 0.9669711061515243, 'reg_alpha': 6.492173108062758e-08, 'reg_lambda': 4.89330821961207e-06, 'min_child_samples': 17, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:53:59,722] Trial 12 finished with value: 0.7128933524629364 and parameters: {'class_weight_option': None, 'n_estimators': 3900, 'learning_rate': 0.015428300782350968, 'num_leaves': 220, 'max_depth': 5, 'subsample': 0.5013323779762358, 'colsample_bytree': 0.9950840181894651, 'reg_alpha': 4.0853779776357846e-06, 'reg_lambda': 0.00012650239535230156, 'min_child_samples': 19, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:54:49,505] Trial 13 finished with value: 0.713868962219034 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2200, 'learning_rate': 0.020254276160284558, 'num_leaves': 215, 'max_depth': 30, 'subsample': 0.5876105353667513, 'colsample_bytree': 0.8189774283896034, 'reg_alpha': 2.3567851226375162e-08, 'reg_lambda': 3.924072171203733e-06, 'min_child_samples': 15, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:55:40,069] Trial 14 finished with value: 0.714858919177427 and parameters: {'class_weight_option': None, 'n_estimators': 4200, 'learning_rate': 0.011448748828640507, 'num_leaves': 345, 'max_depth': 12, 'subsample': 0.5566122605491735, 'colsample_bytree': 0.6277822198897076, 'reg_alpha': 0.003947492313549144, 'reg_lambda': 0.0006305036223633271, 'min_child_samples': 24, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:56:06,450] Trial 15 finished with value: 0.7177905308464849 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2900, 'learning_rate': 0.0771364924918981, 'num_leaves': 220, 'max_depth': 43, 'subsample': 0.9725845454661288, 'colsample_bytree': 0.7997107945561854, 'reg_alpha': 2.1634523031824187e-05, 'reg_lambda': 0.0005035809612604505, 'min_child_samples': 10, 'boosting_type': 'gbdt'}. Best is trial 1 with value: 0.7207364897178383.\n",
      "[I 2025-04-28 20:56:32,777] Trial 16 finished with value: 0.7246293639406982 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2700, 'learning_rate': 0.08517502951628371, 'num_leaves': 350, 'max_depth': 43, 'subsample': 0.9802761356323292, 'colsample_bytree': 0.8361290905348986, 'reg_alpha': 4.0416022686210116e-05, 'reg_lambda': 0.0014052167182215848, 'min_child_samples': 7, 'boosting_type': 'gbdt'}. Best is trial 16 with value: 0.7246293639406982.\n",
      "[I 2025-04-28 20:56:57,124] Trial 17 finished with value: 0.7177857484457197 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 1800, 'learning_rate': 0.09142715281893021, 'num_leaves': 495, 'max_depth': 41, 'subsample': 0.9752396872142173, 'colsample_bytree': 0.8226267507351153, 'reg_alpha': 0.012282772159493109, 'reg_lambda': 0.45995049120184217, 'min_child_samples': 7, 'boosting_type': 'gbdt'}. Best is trial 16 with value: 0.7246293639406982.\n",
      "[I 2025-04-28 20:57:10,322] Trial 18 finished with value: 0.7090004782400765 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 5500, 'learning_rate': 0.06387803166068738, 'num_leaves': 370, 'max_depth': 31, 'subsample': 0.8924448790201781, 'colsample_bytree': 0.628303284549512, 'reg_alpha': 8.919178265484135e-05, 'reg_lambda': 0.001704705780392287, 'min_child_samples': 24, 'boosting_type': 'gbdt'}. Best is trial 16 with value: 0.7246293639406982.\n",
      "[I 2025-04-28 20:57:30,163] Trial 19 finished with value: 0.6972692491630799 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2900, 'learning_rate': 0.02928871206891627, 'num_leaves': 420, 'max_depth': 44, 'subsample': 0.6872678481577663, 'colsample_bytree': 0.7841451581037168, 'reg_alpha': 7.558978987559028e-07, 'reg_lambda': 3.241241760088384e-05, 'min_child_samples': 32, 'boosting_type': 'gbdt'}. Best is trial 16 with value: 0.7246293639406982.\n",
      "[I 2025-04-28 20:57:52,400] Trial 20 finished with value: 0.7080105212816834 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 3300, 'learning_rate': 0.09328138430529559, 'num_leaves': 330, 'max_depth': 26, 'subsample': 0.922597736044909, 'colsample_bytree': 0.8650400286342894, 'reg_alpha': 0.005899527759012208, 'reg_lambda': 3.757595970084347e-07, 'min_child_samples': 9, 'boosting_type': 'gbdt'}. Best is trial 16 with value: 0.7246293639406982.\n",
      "[I 2025-04-28 20:58:20,510] Trial 21 finished with value: 0.7099856527977044 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2800, 'learning_rate': 0.06012490749439684, 'num_leaves': 250, 'max_depth': 44, 'subsample': 0.9991838417019636, 'colsample_bytree': 0.7737180567694671, 'reg_alpha': 2.9676846861587395e-05, 'reg_lambda': 0.0022744691192909065, 'min_child_samples': 10, 'boosting_type': 'gbdt'}. Best is trial 16 with value: 0.7246293639406982.\n",
      "[I 2025-04-28 20:58:59,457] Trial 22 finished with value: 0.7070540411286466 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2400, 'learning_rate': 0.10486614230181789, 'num_leaves': 270, 'max_depth': 44, 'subsample': 0.9520343585467849, 'colsample_bytree': 0.6619576147375805, 'reg_alpha': 5.169060259642818e-06, 'reg_lambda': 0.00019672394672945234, 'min_child_samples': 3, 'boosting_type': 'gbdt'}. Best is trial 16 with value: 0.7246293639406982.\n",
      "[I 2025-04-28 20:59:18,486] Trial 23 finished with value: 0.7129029172644668 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 1900, 'learning_rate': 0.056558412315827804, 'num_leaves': 395, 'max_depth': 36, 'subsample': 0.8513116051615439, 'colsample_bytree': 0.8219120396216621, 'reg_alpha': 6.751140191576249e-07, 'reg_lambda': 0.17114757794148325, 'min_child_samples': 14, 'boosting_type': 'gbdt'}. Best is trial 16 with value: 0.7246293639406982.\n",
      "[I 2025-04-28 20:59:39,649] Trial 24 finished with value: 0.7167862266857963 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2800, 'learning_rate': 0.034728339001903155, 'num_leaves': 465, 'max_depth': 41, 'subsample': 0.937187943868928, 'colsample_bytree': 0.8787993722537938, 'reg_alpha': 7.346007288918638e-05, 'reg_lambda': 4.287583381379187e-05, 'min_child_samples': 20, 'boosting_type': 'gbdt'}. Best is trial 16 with value: 0.7246293639406982.\n",
      "[I 2025-04-28 20:59:51,963] Trial 25 finished with value: 0.7197274031563845 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3300, 'learning_rate': 0.07502510462952638, 'num_leaves': 20, 'max_depth': 50, 'subsample': 0.8279846533397455, 'colsample_bytree': 0.7797732589697876, 'reg_alpha': 0.0009129289449510207, 'reg_lambda': 0.004103333122295013, 'min_child_samples': 8, 'boosting_type': 'gbdt'}. Best is trial 16 with value: 0.7246293639406982.\n",
      "[I 2025-04-28 21:00:03,538] Trial 26 finished with value: 0.7265901482544237 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3500, 'learning_rate': 0.09920114383403264, 'num_leaves': 25, 'max_depth': 48, 'subsample': 0.8266486110371465, 'colsample_bytree': 0.6641968347138931, 'reg_alpha': 0.0541188963314612, 'reg_lambda': 0.006493388453525702, 'min_child_samples': 7, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:00:22,511] Trial 27 finished with value: 0.7021759923481588 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 4300, 'learning_rate': 0.11876778938450756, 'num_leaves': 330, 'max_depth': 47, 'subsample': 0.7180120900501477, 'colsample_bytree': 0.5784558365362588, 'reg_alpha': 0.05268541631215574, 'reg_lambda': 0.008402351476972504, 'min_child_samples': 6, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:00:32,032] Trial 28 finished with value: 0.7070444763271162 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3700, 'learning_rate': 0.11549681408240545, 'num_leaves': 140, 'max_depth': 31, 'subsample': 0.6227697753701557, 'colsample_bytree': 0.6675655968863345, 'reg_alpha': 0.9392979298734954, 'reg_lambda': 0.5996286765783623, 'min_child_samples': 20, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:10:28,129] Trial 29 finished with value: 0.7070349115255858 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2400, 'learning_rate': 0.0232444766145758, 'num_leaves': 85, 'max_depth': 40, 'subsample': 0.897009102094013, 'colsample_bytree': 0.6957934477513953, 'reg_alpha': 0.49094282239274273, 'reg_lambda': 5.727823788202389, 'min_child_samples': 13, 'boosting_type': 'dart'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:10:41,811] Trial 30 finished with value: 0.6884887613582018 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 4400, 'learning_rate': 0.051266416758303186, 'num_leaves': 385, 'max_depth': 35, 'subsample': 0.8106155617664048, 'colsample_bytree': 0.5937079920763975, 'reg_alpha': 0.2510647327644936, 'reg_lambda': 0.043709327285002285, 'min_child_samples': 32, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:10:57,455] Trial 31 finished with value: 0.7236489717838355 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3400, 'learning_rate': 0.07807518548294991, 'num_leaves': 25, 'max_depth': 50, 'subsample': 0.8397875749322306, 'colsample_bytree': 0.7541021421002355, 'reg_alpha': 0.0016482262055152815, 'reg_lambda': 0.003922310965365573, 'min_child_samples': 7, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:11:14,309] Trial 32 finished with value: 0.7168101386896222 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3300, 'learning_rate': 0.0875582190109211, 'num_leaves': 30, 'max_depth': 47, 'subsample': 0.8666879253701522, 'colsample_bytree': 0.7280354026750335, 'reg_alpha': 0.023758080491316495, 'reg_lambda': 0.001070720479888655, 'min_child_samples': 7, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:11:53,228] Trial 33 finished with value: 0.713897656623625 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3600, 'learning_rate': 0.06646559262305862, 'num_leaves': 85, 'max_depth': 47, 'subsample': 0.7304231233449202, 'colsample_bytree': 0.648552977583291, 'reg_alpha': 0.002978625627811953, 'reg_lambda': 0.00017796508335421725, 'min_child_samples': 3, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:24:19,246] Trial 34 finished with value: 0.7158153993304639 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3900, 'learning_rate': 0.0360281679322867, 'num_leaves': 115, 'max_depth': 50, 'subsample': 0.8090084091183494, 'colsample_bytree': 0.6931312155477138, 'reg_alpha': 0.00025557924139602565, 'reg_lambda': 0.07344171412485483, 'min_child_samples': 13, 'boosting_type': 'dart'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:24:20,182] Trial 35 finished with value: 0.7080296508847441 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3100, 'learning_rate': 0.12468775590582414, 'num_leaves': 55, 'max_depth': 45, 'subsample': 0.7786481550115734, 'colsample_bytree': 0.8503143005018612, 'reg_alpha': 0.00010710719755839882, 'reg_lambda': 1.2096592963800573e-05, 'min_child_samples': 17, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:24:21,658] Trial 36 finished with value: 0.6415973218555715 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 2500, 'learning_rate': 0.04489392277433664, 'num_leaves': 170, 'max_depth': 38, 'subsample': 0.8404356398605657, 'colsample_bytree': 0.7470935136044129, 'reg_alpha': 17.93010789074167, 'reg_lambda': 0.010815421396249221, 'min_child_samples': 11, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:27:28,694] Trial 37 finished with value: 0.6992013390722143 and parameters: {'class_weight_option': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 4700, 'learning_rate': 0.080015349839815, 'num_leaves': 50, 'max_depth': 33, 'subsample': 0.8753538600253739, 'colsample_bytree': 0.9133386516326395, 'reg_alpha': 0.0016749589154498332, 'reg_lambda': 1.1417569560663394e-06, 'min_child_samples': 6, 'boosting_type': 'dart'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:27:29,960] Trial 38 finished with value: 0.6787087517934003 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 4100, 'learning_rate': 0.07004284030725463, 'num_leaves': 455, 'max_depth': 46, 'subsample': 0.650279907840613, 'colsample_bytree': 0.6781706593469783, 'reg_alpha': 0.03235920675098137, 'reg_lambda': 5.898467946786475e-08, 'min_child_samples': 28, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:28:57,477] Trial 39 finished with value: 0.6826255380200861 and parameters: {'class_weight_option': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 5200, 'learning_rate': 0.09961331432016911, 'num_leaves': 115, 'max_depth': 41, 'subsample': 0.7910384038116517, 'colsample_bytree': 0.7140017342087738, 'reg_alpha': 5.120996517963932, 'reg_lambda': 0.004548839110771376, 'min_child_samples': 51, 'boosting_type': 'dart'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:28:58,296] Trial 40 finished with value: 0.6689670014347202 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 3500, 'learning_rate': 0.14586022628027995, 'num_leaves': 415, 'max_depth': 23, 'subsample': 0.7532434496858601, 'colsample_bytree': 0.7639970711428926, 'reg_alpha': 0.00029321684860612857, 'reg_lambda': 9.652196484934779e-05, 'min_child_samples': 39, 'boosting_type': 'gbdt'}. Best is trial 26 with value: 0.7265901482544237.\n",
      "[I 2025-04-28 21:28:59,418] Trial 41 finished with value: 0.730468675274988 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3200, 'learning_rate': 0.07959834527052934, 'num_leaves': 20, 'max_depth': 50, 'subsample': 0.84452229231298, 'colsample_bytree': 0.8453972857371788, 'reg_alpha': 0.0008056515374118815, 'reg_lambda': 0.004355046968073931, 'min_child_samples': 8, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:29:01,340] Trial 42 finished with value: 0.7255810616929699 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3100, 'learning_rate': 0.052601675255457926, 'num_leaves': 40, 'max_depth': 50, 'subsample': 0.8206106508927321, 'colsample_bytree': 0.8491101604527018, 'reg_alpha': 0.013636197183639108, 'reg_lambda': 0.022724077292570775, 'min_child_samples': 5, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:29:03,153] Trial 43 finished with value: 0.716805356288857 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2600, 'learning_rate': 0.054849835288663606, 'num_leaves': 40, 'max_depth': 50, 'subsample': 0.823868112508193, 'colsample_bytree': 0.9204710512538294, 'reg_alpha': 0.013011781269721703, 'reg_lambda': 0.024116178242904414, 'min_child_samples': 7, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:29:04,776] Trial 44 finished with value: 0.7255906264945002 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2100, 'learning_rate': 0.0823863448498299, 'num_leaves': 70, 'max_depth': 48, 'subsample': 0.9102481272953482, 'colsample_bytree': 0.8543169230094105, 'reg_alpha': 0.21428821862134578, 'reg_lambda': 0.13204993188540376, 'min_child_samples': 5, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:29:06,304] Trial 45 finished with value: 0.7187517934002869 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2100, 'learning_rate': 0.10849277071007131, 'num_leaves': 70, 'max_depth': 48, 'subsample': 0.9115500020488088, 'colsample_bytree': 0.8446791173537119, 'reg_alpha': 0.18435262446747333, 'reg_lambda': 1.2168698625154393, 'min_child_samples': 5, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:29:07,307] Trial 46 finished with value: 0.7080200860832138 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 1200, 'learning_rate': 0.08447254459973988, 'num_leaves': 80, 'max_depth': 48, 'subsample': 0.860780777341642, 'colsample_bytree': 0.9088966561838687, 'reg_alpha': 1.6850882957409388, 'reg_lambda': 0.1255085461207662, 'min_child_samples': 16, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:29:09,177] Trial 47 finished with value: 0.7148780487804878 and parameters: {'class_weight_option': None, 'n_estimators': 1600, 'learning_rate': 0.0394786240978655, 'num_leaves': 100, 'max_depth': 43, 'subsample': 0.9481111153018086, 'colsample_bytree': 0.9409921347543211, 'reg_alpha': 0.261110715088591, 'reg_lambda': 1.02234507075827e-08, 'min_child_samples': 11, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:29:10,896] Trial 48 finished with value: 0.7236585365853658 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3100, 'learning_rate': 0.13010424815142288, 'num_leaves': 170, 'max_depth': 46, 'subsample': 0.9238431389564479, 'colsample_bytree': 0.8850014186634001, 'reg_alpha': 0.09674744657568674, 'reg_lambda': 0.02718556982949434, 'min_child_samples': 3, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:30:52,692] Trial 49 finished with value: 0.7168005738880918 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2100, 'learning_rate': 0.04812639532644377, 'num_leaves': 135, 'max_depth': 42, 'subsample': 0.9984088264223713, 'colsample_bytree': 0.8018463225888341, 'reg_alpha': 0.010993971648851839, 'reg_lambda': 0.31058150059302814, 'min_child_samples': 9, 'boosting_type': 'dart'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:30:53,791] Trial 50 finished with value: 0.6943615494978479 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2700, 'learning_rate': 0.06386566502719108, 'num_leaves': 50, 'max_depth': 39, 'subsample': 0.9716211400467712, 'colsample_bytree': 0.8436705868290914, 'reg_alpha': 4.499725750259121, 'reg_lambda': 0.06507606854402506, 'min_child_samples': 12, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:30:55,485] Trial 51 finished with value: 0.7148493543758967 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3100, 'learning_rate': 0.13105586837585811, 'num_leaves': 65, 'max_depth': 46, 'subsample': 0.9116583204514982, 'colsample_bytree': 0.8907485294475299, 'reg_alpha': 0.11120459107847441, 'reg_lambda': 0.02321000016895184, 'min_child_samples': 3, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:30:57,770] Trial 52 finished with value: 0.7207221425155428 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3100, 'learning_rate': 0.09982655048801549, 'num_leaves': 175, 'max_depth': 48, 'subsample': 0.8823112216220286, 'colsample_bytree': 0.9439569552944859, 'reg_alpha': 0.0732043316643148, 'reg_lambda': 1.7302799864506844, 'min_child_samples': 4, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:30:59,384] Trial 53 finished with value: 0.7099856527977044 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3800, 'learning_rate': 0.13342786137262558, 'num_leaves': 150, 'max_depth': 46, 'subsample': 0.9309572457032411, 'colsample_bytree': 0.8641963270735314, 'reg_alpha': 0.025591443292959187, 'reg_lambda': 0.0013032830615057483, 'min_child_samples': 5, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:00,410] Trial 54 finished with value: 0.71974175035868 and parameters: {'class_weight_option': None, 'n_estimators': 3000, 'learning_rate': 0.09062358466702855, 'num_leaves': 20, 'max_depth': 45, 'subsample': 0.9572467476047237, 'colsample_bytree': 0.8940003172736287, 'reg_alpha': 0.006235249128498096, 'reg_lambda': 0.0004586227508926244, 'min_child_samples': 9, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:01,098] Trial 55 finished with value: 0.6748206599713056 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2300, 'learning_rate': 0.10969589804721881, 'num_leaves': 195, 'max_depth': 49, 'subsample': 0.8999121081850284, 'colsample_bytree': 0.8017714509830003, 'reg_alpha': 7.712942065553701e-06, 'reg_lambda': 0.009088837937145289, 'min_child_samples': 60, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:03,575] Trial 56 finished with value: 0.7216929698708752 and parameters: {'class_weight_option': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 600, 'learning_rate': 0.06977835299175571, 'num_leaves': 240, 'max_depth': 43, 'subsample': 0.9805324062713748, 'colsample_bytree': 0.8682804169553756, 'reg_alpha': 0.0005250760787314512, 'reg_lambda': 0.03007924802180446, 'min_child_samples': 5, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:04,962] Trial 57 finished with value: 0.7099569583931133 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 1900, 'learning_rate': 0.09531747680927512, 'num_leaves': 295, 'max_depth': 48, 'subsample': 0.9242615625635645, 'colsample_bytree': 0.8357325868061632, 'reg_alpha': 2.680045862425336e-06, 'reg_lambda': 0.1655201489520973, 'min_child_samples': 14, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:06,172] Trial 58 finished with value: 0.7129076996652319 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2600, 'learning_rate': 0.08450725365507082, 'num_leaves': 110, 'max_depth': 46, 'subsample': 0.7929556479981017, 'colsample_bytree': 0.9487889086607815, 'reg_alpha': 0.0512991502954127, 'reg_lambda': 0.0032264982131898134, 'min_child_samples': 18, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:07,590] Trial 59 finished with value: 0.7099569583931133 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3500, 'learning_rate': 0.05914738110332478, 'num_leaves': 40, 'max_depth': 44, 'subsample': 0.853508321909853, 'colsample_bytree': 0.8337387314425896, 'reg_alpha': 0.7929197319193816, 'reg_lambda': 0.0007839966319906609, 'min_child_samples': 10, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:08,566] Trial 60 finished with value: 0.714858919177427 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2900, 'learning_rate': 0.11996005898380362, 'num_leaves': 100, 'max_depth': 17, 'subsample': 0.8192633039778153, 'colsample_bytree': 0.9912081239625508, 'reg_alpha': 4.92375630078457e-05, 'reg_lambda': 0.013910548700335692, 'min_child_samples': 23, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:09,525] Trial 61 finished with value: 0.7255810616929699 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3300, 'learning_rate': 0.07848980697384954, 'num_leaves': 20, 'max_depth': 50, 'subsample': 0.8475919720497398, 'colsample_bytree': 0.8194182277207738, 'reg_alpha': 0.0016418696729757836, 'reg_lambda': 0.004758490375054084, 'min_child_samples': 7, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:11,004] Trial 62 finished with value: 0.7119416547106647 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3200, 'learning_rate': 0.07522521348901998, 'num_leaves': 40, 'max_depth': 50, 'subsample': 0.8712918018533179, 'colsample_bytree': 0.8087142686496667, 'reg_alpha': 0.0037339979937277227, 'reg_lambda': 0.0019146475586500965, 'min_child_samples': 8, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:12,328] Trial 63 finished with value: 0.7138737446197991 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3700, 'learning_rate': 0.14947347213003065, 'num_leaves': 75, 'max_depth': 49, 'subsample': 0.7737723436721357, 'colsample_bytree': 0.8558383403094885, 'reg_alpha': 0.13801495915702122, 'reg_lambda': 0.06691842382566672, 'min_child_samples': 3, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:13,307] Trial 64 finished with value: 0.7197369679579149 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2800, 'learning_rate': 0.09864724200377288, 'num_leaves': 20, 'max_depth': 45, 'subsample': 0.8937689198778251, 'colsample_bytree': 0.8980271877344331, 'reg_alpha': 0.007610941493834882, 'reg_lambda': 0.004949512254479076, 'min_child_samples': 5, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:14,793] Trial 65 finished with value: 0.7197274031563845 and parameters: {'class_weight_option': None, 'n_estimators': 3000, 'learning_rate': 0.08257320972588372, 'num_leaves': 55, 'max_depth': 48, 'subsample': 0.8384932486147846, 'colsample_bytree': 0.8716933299159075, 'reg_alpha': 0.0006575581837456739, 'reg_lambda': 0.000255161122899284, 'min_child_samples': 12, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:16,273] Trial 66 finished with value: 0.7109564801530368 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3400, 'learning_rate': 0.10645239108651317, 'num_leaves': 95, 'max_depth': 42, 'subsample': 0.9429289080850018, 'colsample_bytree': 0.785588717904729, 'reg_alpha': 0.0002356790738199492, 'reg_lambda': 0.007815185697005223, 'min_child_samples': 9, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:31:17,734] Trial 67 finished with value: 0.7207125777140124 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2700, 'learning_rate': 0.06273754174504553, 'num_leaves': 65, 'max_depth': 47, 'subsample': 0.9106437995705441, 'colsample_bytree': 0.8185751430754898, 'reg_alpha': 0.016185367668434283, 'reg_lambda': 0.01975900054606257, 'min_child_samples': 15, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:26,100] Trial 68 finished with value: 0.6806886657101866 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 4000, 'learning_rate': 0.025558604333997516, 'num_leaves': 40, 'max_depth': 50, 'subsample': 0.8050792581648989, 'colsample_bytree': 0.5351478085043797, 'reg_alpha': 0.00217061576571888, 'reg_lambda': 0.326842147141121, 'min_child_samples': 56, 'boosting_type': 'dart'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:27,577] Trial 69 finished with value: 0.7217073170731707 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3200, 'learning_rate': 0.07070009343339491, 'num_leaves': 265, 'max_depth': 11, 'subsample': 0.9631944458257014, 'colsample_bytree': 0.8850569653451271, 'reg_alpha': 1.9070539820366944e-05, 'reg_lambda': 0.03938025036132135, 'min_child_samples': 6, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:28,796] Trial 70 finished with value: 0.7187517934002869 and parameters: {'class_weight_option': {'0': 1.4, '1': 1.0, '2': 1.0}, 'n_estimators': 1600, 'learning_rate': 0.054544399905999505, 'num_leaves': 35, 'max_depth': 21, 'subsample': 0.7559503558900624, 'colsample_bytree': 0.8315071423085052, 'reg_alpha': 0.4665237726501599, 'reg_lambda': 0.0012629142228583426, 'min_child_samples': 7, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:29,685] Trial 71 finished with value: 0.7158345289335246 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3400, 'learning_rate': 0.07737161753447752, 'num_leaves': 25, 'max_depth': 49, 'subsample': 0.8551398162019374, 'colsample_bytree': 0.767070892863195, 'reg_alpha': 0.001416248179761637, 'reg_lambda': 0.002916374494678274, 'min_child_samples': 8, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:30,968] Trial 72 finished with value: 0.7050932568149211 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3600, 'learning_rate': 0.09052281876294521, 'num_leaves': 360, 'max_depth': 50, 'subsample': 0.8339484516454281, 'colsample_bytree': 0.7352603942357887, 'reg_alpha': 0.00014312545585707314, 'reg_lambda': 0.006808287511933129, 'min_child_samples': 11, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:31,742] Trial 73 finished with value: 0.7236394069823051 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3300, 'learning_rate': 0.11458954384162853, 'num_leaves': 20, 'max_depth': 47, 'subsample': 0.8414446127775468, 'colsample_bytree': 0.813345120793757, 'reg_alpha': 0.0010100765349386547, 'reg_lambda': 0.0133930584339339, 'min_child_samples': 3, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:32,920] Trial 74 finished with value: 0.7050932568149211 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3500, 'learning_rate': 0.12968673335230566, 'num_leaves': 55, 'max_depth': 45, 'subsample': 0.8754769394739941, 'colsample_bytree': 0.7869012485887659, 'reg_alpha': 0.004638451474588063, 'reg_lambda': 0.11425059883486106, 'min_child_samples': 6, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:34,295] Trial 75 finished with value: 0.7246484935437589 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 3000, 'learning_rate': 0.0795184545233397, 'num_leaves': 85, 'max_depth': 49, 'subsample': 0.7885114159605515, 'colsample_bytree': 0.8531890380122463, 'reg_alpha': 0.04268393390401854, 'reg_lambda': 0.002411052050758601, 'min_child_samples': 8, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:35,573] Trial 76 finished with value: 0.7090004782400765 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 2500, 'learning_rate': 0.07177714642093275, 'num_leaves': 90, 'max_depth': 29, 'subsample': 0.7805083323555857, 'colsample_bytree': 0.8547804437510292, 'reg_alpha': 0.05858901450921315, 'reg_lambda': 0.00030747895401661703, 'min_child_samples': 10, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:36,483] Trial 77 finished with value: 0.6796987087517934 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 3000, 'learning_rate': 0.049481629661719484, 'num_leaves': 135, 'max_depth': 49, 'subsample': 0.8127561239887612, 'colsample_bytree': 0.9248891988904482, 'reg_alpha': 0.037681089754396414, 'reg_lambda': 6.38326637069606e-05, 'min_child_samples': 38, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:36:37,388] Trial 78 finished with value: 0.7119320899091344 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 2200, 'learning_rate': 0.10296159803383102, 'num_leaves': 45, 'max_depth': 43, 'subsample': 0.7367535551537676, 'colsample_bytree': 0.880582038373094, 'reg_alpha': 0.3429062637718161, 'reg_lambda': 0.0006043958318567152, 'min_child_samples': 13, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:34,588] Trial 79 finished with value: 0.7167910090865615 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 2800, 'learning_rate': 0.043335561749886765, 'num_leaves': 320, 'max_depth': 47, 'subsample': 0.7101668490321453, 'colsample_bytree': 0.8291983092037626, 'reg_alpha': 0.020126002748595818, 'reg_lambda': 0.0019535179922631576, 'min_child_samples': 4, 'boosting_type': 'dart'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:35,315] Trial 80 finished with value: 0.6904399808703969 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 3800, 'learning_rate': 0.06640187331922855, 'num_leaves': 125, 'max_depth': 46, 'subsample': 0.7891515266915524, 'colsample_bytree': 0.6039655997966038, 'reg_alpha': 0.09629192409688479, 'reg_lambda': 0.005642016380239352, 'min_child_samples': 43, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:36,774] Trial 81 finished with value: 0.719713055954089 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 400, 'learning_rate': 0.08093450927034958, 'num_leaves': 70, 'max_depth': 50, 'subsample': 0.8478508083472341, 'colsample_bytree': 0.8484571513977432, 'reg_alpha': 0.0028159796041587444, 'reg_lambda': 0.0027592642848777494, 'min_child_samples': 8, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:37,779] Trial 82 finished with value: 0.7216834050693448 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3200, 'learning_rate': 0.08718944878966164, 'num_leaves': 30, 'max_depth': 49, 'subsample': 0.8305329808694463, 'colsample_bytree': 0.9007199237092225, 'reg_alpha': 0.16166706326669297, 'reg_lambda': 0.041476411155855354, 'min_child_samples': 7, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:42,079] Trial 83 finished with value: 0.715844093735055 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3400, 'learning_rate': 0.012743700625480643, 'num_leaves': 80, 'max_depth': 48, 'subsample': 0.7635297281843237, 'colsample_bytree': 0.794041918859609, 'reg_alpha': 0.009409126269382981, 'reg_lambda': 0.016796684514412425, 'min_child_samples': 5, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:43,423] Trial 84 finished with value: 0.71974175035868 and parameters: {'class_weight_option': 'balanced', 'n_estimators': 3100, 'learning_rate': 0.07747191376752445, 'num_leaves': 60, 'max_depth': 45, 'subsample': 0.8022908770563156, 'colsample_bytree': 0.7630450061610164, 'reg_alpha': 0.0004058247109869998, 'reg_lambda': 0.0009358809023097972, 'min_child_samples': 10, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:44,689] Trial 85 finished with value: 0.7206982305117169 and parameters: {'class_weight_option': {'0': 1.1, '1': 1.0, '2': 1.0}, 'n_estimators': 2900, 'learning_rate': 0.05830824831285099, 'num_leaves': 35, 'max_depth': 47, 'subsample': 0.8844840976802246, 'colsample_bytree': 0.8599872212120032, 'reg_alpha': 5.90398405435795e-05, 'reg_lambda': 0.0038881702392030487, 'min_child_samples': 7, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:45,783] Trial 86 finished with value: 0.7158584409373505 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 2600, 'learning_rate': 0.11577578108050846, 'num_leaves': 200, 'max_depth': 50, 'subsample': 0.9835950898793586, 'colsample_bytree': 0.8748243595372897, 'reg_alpha': 1.3313151675653931, 'reg_lambda': 0.009768490363066961, 'min_child_samples': 4, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:46,679] Trial 87 finished with value: 0.6855475848876136 and parameters: {'class_weight_option': None, 'n_estimators': 3700, 'learning_rate': 0.09614499063008983, 'num_leaves': 50, 'max_depth': 44, 'subsample': 0.8196280644143277, 'colsample_bytree': 0.7168301500736923, 'reg_alpha': 0.001607397490999753, 'reg_lambda': 0.08753156449145093, 'min_child_samples': 34, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:47,761] Trial 88 finished with value: 0.7295169775227164 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3300, 'learning_rate': 0.14065738179264858, 'num_leaves': 30, 'max_depth': 48, 'subsample': 0.9339425439913707, 'colsample_bytree': 0.6971586509486012, 'reg_alpha': 0.00017121411210794922, 'reg_lambda': 0.7832699063593876, 'min_child_samples': 6, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n",
      "[I 2025-04-28 21:47:49,504] Trial 89 finished with value: 0.7119416547106647 and parameters: {'class_weight_option': {'0': 1.2, '1': 1.0, '2': 1.0}, 'n_estimators': 3300, 'learning_rate': 0.14098332744172992, 'num_leaves': 240, 'max_depth': 48, 'subsample': 0.9313586805969316, 'colsample_bytree': 0.7009265768103821, 'reg_alpha': 0.0001531480505747124, 'reg_lambda': 3.598565970916253, 'min_child_samples': 6, 'boosting_type': 'gbdt'}. Best is trial 41 with value: 0.730468675274988.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "# Assume all necessary functions (run_complete_pipeline, etc.) are defined above\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # Feature Selection Settings\n",
    "    PERFORM_FEATURE_SELECTION = True # Set to True or False\n",
    "    FS_THRESHOLD = 'mean' # Threshold ('mean', 'median', or float like 1e-5)\n",
    "\n",
    "    # Model Qualification Threshold\n",
    "    MIN_CV_SCORE_THRESHOLD = 0.72 # Minimum average CV score to qualify a model\n",
    "\n",
    "    # Parallelism Setting for Sklearn Models (used in Optuna objective and Ensemble)\n",
    "    # Set to 1 if experiencing issues, otherwise set to desired core count (e.g., os.cpu_count() - 2)\n",
    "    # N_CORES_TO_USE = 1 # Start with 1 for stability, increase carefully\n",
    "    # import os\n",
    "    N_CORES_TO_USE = max(1, os.cpu_count() - 4) # Example: Use all but 2 cores\n",
    "\n",
    "    # --- Run the Pipeline ---\n",
    "    pipeline_start_time = time.time()\n",
    "\n",
    "    success = run_complete_pipeline(\n",
    "        perform_feature_selection=PERFORM_FEATURE_SELECTION,\n",
    "        fs_threshold=FS_THRESHOLD,\n",
    "        min_cv_score_threshold=MIN_CV_SCORE_THRESHOLD,\n",
    "        n_jobs_sklearn=N_CORES_TO_USE # Pass the core count\n",
    "        )\n",
    "\n",
    "    pipeline_end_time = time.time()\n",
    "    pipeline_duration = pipeline_end_time - pipeline_start_time\n",
    "\n",
    "    # --- Final Status Output ---\n",
    "    status_msg = f\"Pipeline execution {'succeeded' if success else 'failed'}.\"\n",
    "    duration_msg = f\"Total time: {pipeline_duration:.2f} seconds ({pipeline_duration / 60:.2f} minutes).\"\n",
    "\n",
    "    print(f\"\\n{'='*30}\\n{status_msg}\")\n",
    "    print(duration_msg)\n",
    "    print(f\"{'='*30}\")\n",
    "\n",
    "    # Log final status if possible\n",
    "    try:\n",
    "        logger.info(status_msg)\n",
    "        logger.info(duration_msg)\n",
    "    except Exception as log_final_e:\n",
    "        # This might happen if the logger itself failed earlier\n",
    "        print(f\"Note: Final status logging failed: {log_final_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2de13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4a758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845a5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superNova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
