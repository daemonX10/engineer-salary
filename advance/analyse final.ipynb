{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab21e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 01:13:40,448 - INFO - Set LOKY_MAX_CPU_COUNT to 20\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,\n",
    "    AdaBoostClassifier, VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# SVC import is removed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA # Keep PCA if used in FE\n",
    "# TruncatedSVD import removed (can be added back if needed)\n",
    "from category_encoders import TargetEncoder # Keep if used (e.g., for job title)\n",
    "# CatBoostEncoder import removed (can be added back if needed)\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb # Added LightGBM\n",
    "import optuna\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import logging\n",
    "import subprocess\n",
    "import math\n",
    "from sklearn.calibration import CalibratedClassifierCV # <-- Added for Step 2/3 plan\n",
    "\n",
    "# Configure logging (Ensure this runs before logger is used)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Define the global logger instance\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "# Optional: Set LOKY env var if needed for Windows parallelism issues with joblib\n",
    "try:\n",
    "    cpu_count = os.cpu_count()\n",
    "    if cpu_count: os.environ[\"LOKY_MAX_CPU_COUNT\"] = str(cpu_count)\n",
    "    logger.info(f\"Set LOKY_MAX_CPU_COUNT to {os.environ.get('LOKY_MAX_CPU_COUNT')}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not set LOKY_MAX_CPU_COUNT: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3850ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Assume logger is defined globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_directory_structure():\n",
    "    \"\"\"Creates the necessary directory structure for the project.\"\"\"\n",
    "    directories = ['models', 'features', 'results', 'submissions', 'logs', 'plots', 'optuna_trials', 'scalers', 'calibrated_models'] # Added calibrated_models\n",
    "    logger.info(\"Creating directory structure...\")\n",
    "    for directory in directories:\n",
    "        try:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "                logger.info(f\"Created directory: {directory}\")\n",
    "            # else: # Optional: log if directory already exists\n",
    "                logger.debug(f\"Directory already exists: {directory}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating directory {directory}: {e}\")\n",
    "            # Re-raise the exception to halt execution if directory creation fails,\n",
    "            # as it's likely critical for the pipeline.\n",
    "            raise\n",
    "    logger.info(\"Directory structure verified/created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5bab0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Returns the current timestamp in YYYYMMDD_HHMMSS format.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6100ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume logger is defined globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def save_feature_importance(model, feature_names, timestamp, model_name):\n",
    "    \"\"\"\n",
    "    Saves feature importances for compatible models (Tree-based, Linear).\n",
    "    Logs a message for models where standard importance isn't directly available.\n",
    "    \"\"\"\n",
    "    if not feature_names:\n",
    "        logger.warning(f\"No feature names provided for {model_name}. Skipping feature importance.\")\n",
    "        return\n",
    "\n",
    "    importances = None\n",
    "    importance_type = None\n",
    "    is_fitted = True # Assume fitted unless checked otherwise\n",
    "\n",
    "    # --- Model Type Specific Handling ---\n",
    "    if isinstance(model, KerasClassifier):\n",
    "        try:\n",
    "            _ = model.model_ # Check if internal model exists\n",
    "            logger.info(f\"Standard feature importance plot not generated for Keras model {model_name}.\")\n",
    "            logger.info(\"Consider using techniques like Permutation Importance or SHAP.\")\n",
    "        except AttributeError:\n",
    "            logger.warning(f\"Keras model {model_name} not fitted. Skip importance.\")\n",
    "        return # Exit for Keras models\n",
    "\n",
    "    elif isinstance(model, (VotingClassifier, StackingClassifier)):\n",
    "        logger.info(f\"Importance plot not generated for ensemble {model_name}.\")\n",
    "        return # Exit for ensembles\n",
    "\n",
    "    elif isinstance(model, MLPClassifier):\n",
    "        logger.info(f\"Standard feature importance not directly available for MLPClassifier {model_name}.\")\n",
    "        return # Exit for MLP\n",
    "\n",
    "    # Check for standard attributes AFTER handling special cases\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        importance_type = 'Importance'\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        if model.coef_.ndim > 1:\n",
    "            importances = np.abs(model.coef_).mean(axis=0)\n",
    "        else:\n",
    "            importances = np.abs(model.coef_)\n",
    "        importance_type = 'Coefficient Magnitude'\n",
    "    elif hasattr(model, 'estimator_') and hasattr(model.estimator_, 'feature_importances_'):\n",
    "        # Handle cases like AdaBoost where the base estimator holds importance\n",
    "        # Ensure estimator_ exists and has the attribute\n",
    "        if getattr(model, 'estimator_', None) and hasattr(model.estimator_, 'feature_importances_'):\n",
    "             logger.info(f\"Using importance from base estimator ({model.estimator_.__class__.__name__}) of {model_name}.\")\n",
    "             importances = model.estimator_.feature_importances_\n",
    "             importance_type = 'Base Estimator Importance'\n",
    "        else:\n",
    "             logger.warning(f\"Base estimator not found or lacks importance for {model_name}.\")\n",
    "             return\n",
    "\n",
    "    # Add check for LightGBM specifically if feature_importances_ isn't present on fitted model sometimes\n",
    "    elif isinstance(model, lgb.LGBMClassifier) and hasattr(model, 'booster_'):\n",
    "         try:\n",
    "             importances = model.booster_.feature_importance(importance_type='gain') # Or 'split'\n",
    "             importance_type = 'LGBM Gain'\n",
    "             logger.info(f\"Using booster_.feature_importance() for {model_name}.\")\n",
    "         except Exception as lgbm_imp_err:\n",
    "              logger.warning(f\"Could not get LGBM importance via booster_: {lgbm_imp_err}\")\n",
    "              return\n",
    "    else:\n",
    "        logger.info(f\"Model {model_name} ({model.__class__.__name__}) lacks standard importance attributes (feature_importances_, coef_, relevant estimator_).\")\n",
    "        return\n",
    "\n",
    "    # --- Process and Save Importances (if found) ---\n",
    "    if importances is None:\n",
    "        logger.warning(f\"Could not retrieve importances for {model_name}.\")\n",
    "        return\n",
    "\n",
    "    if isinstance(importances, list): # Ensure numpy array\n",
    "        importances = np.array(importances)\n",
    "\n",
    "    if importances.ndim > 1:\n",
    "        logger.warning(f\"Importances shape {importances.shape} for {model_name}. Taking mean over axis 0.\")\n",
    "        importances = importances.mean(axis=0)\n",
    "\n",
    "    if len(importances) != len(feature_names):\n",
    "        logger.warning(f\"Importance length ({len(importances)}) vs names ({len(feature_names)}) mismatch for {model_name}.\")\n",
    "        return\n",
    "\n",
    "    # --- Create DataFrame and Plot ---\n",
    "    try:\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "        # Handle potential NaN/Inf in importance values before sorting\n",
    "        importance_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        importance_df.dropna(subset=['Importance'], inplace=True)\n",
    "        if importance_df.empty:\n",
    "             logger.warning(f\"Importance DataFrame became empty after dropping NaN/Inf for {model_name}.\")\n",
    "             return\n",
    "\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_n = min(30, len(importance_df))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df.head(top_n), palette='viridis')\n",
    "        plt.title(f'Top {top_n} Feature Importances - {model_name}')\n",
    "        plt.xlabel(f'Relative {importance_type}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Ensure directories exist before saving\n",
    "        plot_dir = 'plots'\n",
    "        results_dir = 'results'\n",
    "        if not os.path.exists(plot_dir): os.makedirs(plot_dir)\n",
    "        if not os.path.exists(results_dir): os.makedirs(results_dir)\n",
    "\n",
    "        plot_filename = os.path.join(plot_dir, f'{model_name}_feature_importance_{timestamp}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved importance plot: {plot_filename}\")\n",
    "\n",
    "        csv_filename = os.path.join(results_dir, f'{model_name}_feature_importance_{timestamp}.csv')\n",
    "        importance_df.to_csv(csv_filename, index=False)\n",
    "        logger.info(f\"Saved importance csv: {csv_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not save importance plot/CSV for {model_name}: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40c9c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_keras_model(n_features, n_classes, optimizer='adam', learning_rate=0.001,\n",
    "                      hidden_units=[128, 64], dropout_rate=0.3, activation='relu', l2_reg=1e-4):\n",
    "    \"\"\"Builds a Keras MLP model with specified architecture and hyperparameters.\"\"\"\n",
    "\n",
    "    model = keras.Sequential(name=\"keras_mlp_tabular\")\n",
    "    model.add(layers.Input(shape=(n_features,)))\n",
    "\n",
    "    # Optional: Batch Norm before the first layer\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Hidden Layers\n",
    "    for units in hidden_units:\n",
    "        model.add(layers.Dense(\n",
    "            units,\n",
    "            kernel_regularizer=keras.regularizers.l2(l2_reg) # Add L2 regularization\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization()) # Batch Norm after Dense layer\n",
    "        model.add(layers.Activation(activation)) # Activation after Batch Norm\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    # Select Optimizer\n",
    "    if optimizer.lower() == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer.lower() == 'sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        logger.warning(f\"Unsupported optimizer '{optimizer}'. Defaulting to Adam.\")\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Log model summary details\n",
    "    logger.info(f\"Keras model built: Input({n_features}), Hidden({len(hidden_units)} layers, units={hidden_units}), Output({n_classes})\")\n",
    "    logger.info(f\" Activation: {activation}, Dropout: {dropout_rate}, L2 Reg: {l2_reg}, Optimizer: {optimizer}, LR: {learning_rate}\")\n",
    "    # Optional detailed summary:\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    short_model_summary = \"\\n\".join(stringlist)\n",
    "    logger.debug(f\"Keras Model Summary:\\n{short_model_summary}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e9b3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_data(df, all_states, all_feature1, timestamp, is_training=True, feature_columns_to_use=None):\n",
    "    \"\"\"\n",
    "    Preprocesses data using combined FE logic and robust categorical handling.\n",
    "    Warns about constant columns in training data but does not drop them.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting preprocessing (Combined Logic). Is training: {is_training}\")\n",
    "    start_time = time.time()\n",
    "    data = df.copy()\n",
    "    y = None\n",
    "    le = None\n",
    "    target_column = 'salary_category'\n",
    "\n",
    "    # 1. Handle Target Variable\n",
    "    if target_column in data.columns and is_training:\n",
    "        target = data[target_column]\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(target)\n",
    "        logger.info(f\"Target '{target_column}' label encoded.\")\n",
    "        # Save encoder and mapping\n",
    "        if not os.path.exists('features'): os.makedirs('features')\n",
    "        joblib.dump(le, f'features/label_encoder_{timestamp}.joblib')\n",
    "        mapping = {int(v): k for k, v in zip(le.classes_, le.transform(le.classes_))}\n",
    "        mapping_file = f'features/target_mapping_{timestamp}.json'\n",
    "        with open(mapping_file, 'w') as f: json.dump(mapping, f, indent=4)\n",
    "        logger.info(\"Saved label encoder and mapping.\")\n",
    "    elif not is_training:\n",
    "        # Load encoder\n",
    "        try:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "            if encoder_files: le = joblib.load(f'features/{encoder_files[-1]}'); logger.info(f\"Loaded LE: {encoder_files[-1]}\")\n",
    "            else: logger.warning(\"No LE file found!\"); le = None\n",
    "        except Exception as e: logger.error(f\"Failed load LE: {e}\"); le = None\n",
    "    elif is_training: # Training mode but no target\n",
    "        logger.error(f\"Target column '{target_column}' missing in training data!\")\n",
    "        raise ValueError(f\"Target column '{target_column}' not found.\")\n",
    "\n",
    "    # 2. Define Feature Groups\n",
    "    boolean_features = [f for f in ['feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_10', 'feature_11'] if f in data.columns]\n",
    "    numerical_features = [f for f in ['feature_2', 'feature_9', 'feature_12'] if f in data.columns]\n",
    "    job_desc_cols = [col for col in data.columns if col.startswith('job_desc_')]\n",
    "    all_numerical_features = numerical_features + job_desc_cols\n",
    "\n",
    "    # 3. Initial Cleaning\n",
    "    logger.info(\"Initial cleaning: Numerical and Boolean Features...\")\n",
    "    for col in all_numerical_features:\n",
    "        if col in data.columns:\n",
    "            if data[col].dtype == 'object': data[col] = data[col].replace(['', ' ', 'NA', 'None', 'NULL'], np.nan)\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "            median_val = data[col].median(); fill_value = median_val if not pd.isna(median_val) else 0\n",
    "            data[col] = data[col].fillna(fill_value)\n",
    "    for col in boolean_features:\n",
    "        if col in data.columns:\n",
    "            numeric_view = pd.to_numeric(data[col], errors='coerce'); is_boolean_like = numeric_view.dropna().isin([0, 1]).all()\n",
    "            if is_boolean_like: data[col] = numeric_view.fillna(0).astype(int)\n",
    "            else:\n",
    "                num_non_bool = numeric_view.dropna().loc[~numeric_view.dropna().isin([0, 1])].count()\n",
    "                logger.warning(f\"Col '{col}' has non-0/1 vals ({num_non_bool}). Treat as numeric, impute median.\")\n",
    "                median_val = numeric_view.median(); fill_value = median_val if not pd.isna(median_val) else 0\n",
    "                data[col] = numeric_view.fillna(fill_value)\n",
    "\n",
    "    logger.info(\"Starting Feature Engineering...\")\n",
    "    engineered_feature_names = []\n",
    "    target_encoded_title = 'job_title_encoded' # Define expected name\n",
    "\n",
    "    # --- Feature Engineering Steps (Combined logic from analysis) ---\n",
    "    if 'job_title' in data.columns:\n",
    "        data['job_title'] = data['job_title'].fillna('Unknown')\n",
    "        title_flags = ['is_senior', 'is_junior', 'is_developer', 'is_specialist']\n",
    "        data['is_senior'] = data['job_title'].str.lower().str.contains('senior|sr|lead|principal').fillna(False).astype(int)\n",
    "        data['is_junior'] = data['job_title'].str.lower().str.contains('junior|jr|associate|entry').fillna(False).astype(int)\n",
    "        data['is_developer'] = data['job_title'].str.lower().str.contains('develop|programmer|coder|engineer').fillna(False).astype(int)\n",
    "        data['is_specialist'] = data['job_title'].str.lower().str.contains('special|expert|consult').fillna(False).astype(int)\n",
    "        engineered_feature_names.extend(title_flags)\n",
    "        title_counts = data['job_title'].value_counts(); rare_titles = title_counts[title_counts < 10].index\n",
    "        data['job_title_grouped'] = data['job_title'].apply(lambda x: 'Other_Title' if x in rare_titles else x)\n",
    "        title_encoder_col = 'job_title_grouped'; engineered_feature_names.append(target_encoded_title)\n",
    "        if is_training:\n",
    "            job_encoder = TargetEncoder(cols=[title_encoder_col], handle_missing='value', handle_unknown='value')\n",
    "            data[target_encoded_title] = job_encoder.fit_transform(data[[title_encoder_col]], y) # Use encoded y\n",
    "            joblib.dump(job_encoder, f'features/job_title_encoder_{timestamp}.joblib')\n",
    "            logger.info(f\"Fit/saved TE for {title_encoder_col}\")\n",
    "        else:\n",
    "            encoder_path = f'features/job_title_encoder_{timestamp}.joblib'\n",
    "            fallback_files = sorted([f for f in os.listdir('features') if f.startswith('job_title_encoder_')])\n",
    "            loaded = False\n",
    "            \n",
    "            if os.path.exists(encoder_path):\n",
    "                try:\n",
    "                    job_encoder = joblib.load(encoder_path)\n",
    "                    data[target_encoded_title] = job_encoder.transform(data[[title_encoder_col]])\n",
    "                    loaded = True\n",
    "                    logger.info(f\"Loaded TE: {encoder_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed load TE '{encoder_path}': {e}. Try fallback.\")\n",
    "                    \n",
    "            if not loaded and fallback_files:\n",
    "                latest_encoder = fallback_files[-1]\n",
    "                try:\n",
    "                    job_encoder = joblib.load(f'features/{latest_encoder}')\n",
    "                    data[target_encoded_title] = job_encoder.transform(data[[title_encoder_col]])\n",
    "                    loaded = True\n",
    "                    logger.info(f\"Loaded fallback TE: {latest_encoder}\")\n",
    "                except Exception as e_fb:\n",
    "                    logger.error(f\"Fallback TE failed: {e_fb}. Fill 0.5\")\n",
    "            if not loaded: logger.error(\"No TE found. Fill 0.5\"); data[target_encoded_title] = 0.5\n",
    "        data = data.drop(['job_title', 'job_title_grouped'], axis=1, errors='ignore'); logger.info(\"Processed 'job_title'.\")\n",
    "\n",
    "    if 'job_posted_date' in data.columns:\n",
    "        data['job_posted_date'] = data['job_posted_date'].fillna('2000/01')\n",
    "        def extract_year(d): \n",
    "            try: return int(str(d)[:4]) \n",
    "            except: return 2000\n",
    "        def extract_month(d): \n",
    "            try: return int(str(d).split('/')[1]) \n",
    "            except: return 1\n",
    "        data['job_posted_year'] = data['job_posted_date'].apply(extract_year); data['job_posted_month'] = data['job_posted_date'].apply(extract_month); data['job_posted_month'] = data['job_posted_month'].clip(1, 12)\n",
    "        date_features = ['month_sin', 'month_cos', 'job_recency', 'job_posted_year_norm']\n",
    "        data['month_sin'] = np.sin(2 * np.pi * data['job_posted_month'] / 12); data['month_cos'] = np.cos(2 * np.pi * data['job_posted_month'] / 12); data['job_recency'] = data['job_posted_year'] * 12 + data['job_posted_month']\n",
    "        mean_year = 2022; data['job_posted_year_norm'] = data['job_posted_year'] - mean_year\n",
    "        engineered_feature_names.extend(date_features)\n",
    "        data = data.drop(['job_posted_date', 'job_posted_year', 'job_posted_month'], axis=1, errors='ignore'); logger.info(\"Processed 'job_posted_date'.\")\n",
    "\n",
    "    num_transform_features = []\n",
    "    # Feature 9 processing\n",
    "    if 'feature_9' in data.columns:\n",
    "        try: data['feature_9_bin'] = pd.qcut(data['feature_9'].rank(method='first'), q=5, labels=[0, 1, 2, 3, 4], duplicates='drop').astype(int)\n",
    "        except ValueError: logger.warning(\"qcut fail f9, use cut.\")\n",
    "        try: \n",
    "            data['feature_9_bin'] = pd.cut(data['feature_9'], bins=5, labels=[0, 1, 2, 3, 4], include_lowest=True, duplicates='drop').astype(int) \n",
    "        except Exception as e_cut: logger.error(f\"Cut fail f9: {e_cut}. Bin 0.\"); data['feature_9_bin'] = 0\n",
    "        data['feature_9_bin'] = data['feature_9_bin'].fillna(int(data['feature_9_bin'].median())) # Fill potential NaNs from cut\n",
    "        num_transform_features.append('feature_9_bin'); logger.info(\"Added bin f9.\")\n",
    "        if 'feature_2' in data.columns: interaction_name = 'feature_2_9_interaction'; data[interaction_name] = data['feature_2'] * data['feature_9']; num_transform_features.append(interaction_name); logger.info(f\"Added: {interaction_name}\")\n",
    "\n",
    "    # Feature 2 processing\n",
    "    if 'feature_2' in data.columns:\n",
    "        # Log transform (handle potential zeros/negatives if necessary)\n",
    "        data['feature_2_log'] = np.log1p(data['feature_2']) # Add log transform\n",
    "        num_transform_features.append('feature_2_log')\n",
    "\n",
    "        data['feature_2_squared'] = data['feature_2'] ** 2\n",
    "        data['feature_2_sqrt'] = np.sqrt(np.abs(data['feature_2']))\n",
    "        num_transform_features.extend(['feature_2_squared', 'feature_2_sqrt'])\n",
    "        try: \n",
    "            data['feature_2_bin'] = pd.qcut(data['feature_2'].rank(method='first'), q=5, labels=[0, 1, 2, 3, 4], duplicates='drop').astype(int)\n",
    "        except ValueError: \n",
    "            logger.warning(\"qcut fail f2, use cut.\"); \n",
    "            try: \n",
    "                data['feature_2_bin'] = pd.cut(data['feature_2'], bins=5, labels=[0, 1, 2, 3, 4], include_lowest=True, duplicates='drop').astype(int) \n",
    "            except Exception as e_cut: \n",
    "                logger.error(f\"Cut fail f2: {e_cut}. Bin 0.\"); \n",
    "                data['feature_2_bin'] = 0\n",
    "        data['feature_2_bin'] = data['feature_2_bin'].fillna(int(data['feature_2_bin'].median())) # Fill potential NaNs from cut\n",
    "        num_transform_features.append('feature_2_bin')\n",
    "        logger.info(\"Added transforms f2 (sq, sqrt, bin).\")\n",
    "    engineered_feature_names.extend(num_transform_features)\n",
    "\n",
    "    bool_agg_features = []; actual_boolean_cols = [col for col in boolean_features if col in data.columns]\n",
    "    if actual_boolean_cols: data['boolean_sum'] = data[actual_boolean_cols].sum(axis=1); data['boolean_sum_squared'] = data['boolean_sum'] ** 2; bool_agg_features.extend(['boolean_sum', 'boolean_sum_squared']); logger.info(\"Added bool aggs.\")\n",
    "    else: data['boolean_sum'] = 0; data['boolean_sum_squared'] = 0; logger.info(\"No bool features for agg.\")\n",
    "    engineered_feature_names.extend(bool_agg_features)\n",
    "\n",
    "    if 'feature_10' in data.columns and 'feature_8' in data.columns: interaction_name = 'feature_10_8_interaction'; data[interaction_name] = data['feature_10'] * data['feature_8']; engineered_feature_names.append(interaction_name); logger.info(f\"Added: {interaction_name}\")\n",
    "\n",
    "    # --- NEW Interactions based on Analysis ---\n",
    "    new_interactions = []\n",
    "    if 'feature_2' in data.columns:\n",
    "        if target_encoded_title in data.columns:\n",
    "             int_name = f'feat2_{target_encoded_title}'; data[int_name] = data['feature_2'] * data[target_encoded_title]; new_interactions.append(int_name)\n",
    "        if 'boolean_sum' in data.columns:\n",
    "             int_name = 'feat2_boolsum'; data[int_name] = data['feature_2'] * data['boolean_sum']; new_interactions.append(int_name)\n",
    "        if 'job_recency' in data.columns:\n",
    "             int_name = 'feat2_recency'; data[int_name] = data['feature_2'] * data['job_recency']; new_interactions.append(int_name)\n",
    "        # Interaction with top PCA component (assuming pca_0 exists)\n",
    "        if 'job_desc_pca_0' in data.columns:\n",
    "            int_name = 'feat2_pca0'; data[int_name] = data['feature_2'] * data['job_desc_pca_0']; new_interactions.append(int_name)\n",
    "    if target_encoded_title in data.columns and 'job_recency' in data.columns:\n",
    "        int_name = f'{target_encoded_title}_recency'; data[int_name] = data[target_encoded_title] * data['job_recency']; new_interactions.append(int_name)\n",
    "    if new_interactions: logger.info(f\"Added new interactions: {new_interactions}\"); engineered_feature_names.extend(new_interactions)\n",
    "    # --- End New Interactions ---\n",
    "\n",
    "    job_desc_eng_features = []\n",
    "    if job_desc_cols:\n",
    "        desc_agg = ['job_desc_mean', 'job_desc_std', 'job_desc_min', 'job_desc_max', 'job_desc_sum', 'job_desc_q25', 'job_desc_q75', 'job_desc_iqr']\n",
    "        data['job_desc_mean'] = data[job_desc_cols].mean(axis=1); data['job_desc_std'] = data[job_desc_cols].std(axis=1).fillna(0); data['job_desc_min'] = data[job_desc_cols].min(axis=1); data['job_desc_max'] = data[job_desc_cols].max(axis=1); data['job_desc_sum'] = data[job_desc_cols].sum(axis=1); data['job_desc_q25'] = data[job_desc_cols].quantile(0.25, axis=1); data['job_desc_q75'] = data[job_desc_cols].quantile(0.75, axis=1); data['job_desc_iqr'] = data['job_desc_q75'] - data['job_desc_q25']; job_desc_eng_features.extend(desc_agg)\n",
    "        n_pca_components = 15 # Keep default or adjust based on experiments\n",
    "        if len(job_desc_cols) > n_pca_components:\n",
    "            logger.info(f\"Applying PCA (n={n_pca_components}) to job desc...\")\n",
    "            pca_names = [f'job_desc_pca_{i}' for i in range(n_pca_components)]; job_desc_eng_features.extend(pca_names); job_desc_pca_result = None\n",
    "            if is_training: pca = PCA(n_components=n_pca_components, random_state=42); job_desc_pca_result = pca.fit_transform(data[job_desc_cols]); joblib.dump(pca, f'features/job_desc_pca_{timestamp}.joblib'); logger.info(\"Fit/saved PCA.\")\n",
    "            else:\n",
    "                pca_path = f'features/job_desc_pca_{timestamp}.joblib'; fallback_files = sorted([f for f in os.listdir('features') if f.startswith('job_desc_pca_')]); pca_loaded = False; pca = None\n",
    "                if os.path.exists(pca_path): \n",
    "                    try: \n",
    "                        pca = joblib.load(pca_path)\n",
    "                        pca_loaded=True; logger.info(f\"Loaded PCA: {pca_path}\") \n",
    "                    except Exception as e: \n",
    "                        logger.error(f\"Fail load PCA: {e}. Try fallback.\")\n",
    "                if not pca_loaded and fallback_files:\n",
    "                    latest_pca = fallback_files[-1]\n",
    "                    try: \n",
    "                        pca = joblib.load(f'features/{latest_pca}')\n",
    "                        pca_loaded = True\n",
    "                    except Exception as e_fb:\n",
    "                        logger.error(f\"Fallback PCA load failed: {e_fb}.\")\n",
    "                else:\n",
    "                    latest_pca = None\n",
    "                if pca_loaded and pca is not None: \n",
    "                    try: \n",
    "                        job_desc_pca_result = pca.transform(data[job_desc_cols]) \n",
    "                    except Exception as e_trans: \n",
    "                        logger.error(f\"PCA transform fail: {e_trans}. Fill 0.\")\n",
    "                if job_desc_pca_result is None: logger.error(\"PCA result None. Fill 0.\")\n",
    "                job_desc_pca_result = np.zeros((data.shape[0], n_pca_components))\n",
    "            for i in range(min(n_pca_components, job_desc_pca_result.shape[1])): data[pca_names[i]] = job_desc_pca_result[:, i]\n",
    "        else: logger.warning(f\"Skip PCA: Not enough features.\")\n",
    "        data = data.drop(columns=job_desc_cols, errors='ignore'); logger.info(\"Finished job desc features.\")\n",
    "    else: logger.info(\"No job desc features.\")\n",
    "    engineered_feature_names.extend(job_desc_eng_features)\n",
    "\n",
    "    # --- Robust Categorical Handling ---\n",
    "    if 'job_state' in data.columns: data['job_state'] = data['job_state'].fillna('Unknown')\n",
    "    if 'feature_1' in data.columns: data['feature_1'] = data['feature_1'].fillna('Unknown')\n",
    "\n",
    "    manual_ohe_features = []\n",
    "    logger.info(f\"Applying manual OHE for 'job_state' ({len(all_states)} unique).\")\n",
    "    if 'job_state' in data.columns:\n",
    "        for state in all_states: col_name = f'state_{state}'; data[col_name] = (data['job_state'] == state).astype(int); manual_ohe_features.append(col_name)\n",
    "        data = data.drop('job_state', axis=1, errors='ignore')\n",
    "    else: logger.warning(\"'job_state' not found for OHE.\")\n",
    "\n",
    "    logger.info(f\"Applying manual OHE for 'feature_1' ({len(all_feature1)} unique).\")\n",
    "    if 'feature_1' in data.columns:\n",
    "        for feat in all_feature1: col_name = f'feat1_{feat}'; data[col_name] = (data['feature_1'] == feat).astype(int); manual_ohe_features.append(col_name)\n",
    "        data = data.drop('feature_1', axis=1, errors='ignore')\n",
    "    else: logger.warning(\"'feature_1' not found for OHE.\")\n",
    "    engineered_feature_names.extend(manual_ohe_features)\n",
    "    # --- End FE ---\n",
    "\n",
    "    # 5. Final Cleanup and Column Management\n",
    "    logger.info(\"Final cleanup and column alignment...\")\n",
    "    columns_to_exclude = ['obs']\n",
    "    if is_training and target_column in df.columns: columns_to_exclude.append(target_column)\n",
    "    potential_feature_cols = [col for col in data.columns if col not in columns_to_exclude]\n",
    "\n",
    "    inf_cols_handled = []; nan_cols_handled = []\n",
    "    for col in potential_feature_cols:\n",
    "        if pd.api.types.is_numeric_dtype(data[col]):\n",
    "            if np.isinf(data[col]).any(): inf_cols_handled.append(col); data[col] = data[col].replace([np.inf, -np.inf], np.nan)\n",
    "            if data[col].isnull().any(): nan_cols_handled.append(col); data[col] = data[col].fillna(0)\n",
    "    if inf_cols_handled: logger.warning(f\"Replaced Inf: {inf_cols_handled}\")\n",
    "    final_nan_cols = list(set(nan_cols_handled) - set(inf_cols_handled))\n",
    "    if final_nan_cols: logger.info(f\"Filled NaNs with 0: {final_nan_cols}\")\n",
    "    for col in potential_feature_cols:\n",
    "        if data[col].dtype == 'bool': data[col] = data[col].astype(int)\n",
    "\n",
    "    if is_training:\n",
    "        constant_cols_found = []\n",
    "        for col in potential_feature_cols:\n",
    "            nunique_val = data[col].nunique(dropna=False)\n",
    "            if nunique_val <= 1: logger.warning(f\"Col '{col}' constant (nunique={nunique_val}) in train. Engineered: {col in engineered_feature_names}. Keeping col.\") ; constant_cols_found.append(col)\n",
    "            elif nunique_val <= 3 and col in engineered_feature_names: logger.info(f\"Eng col '{col}' low card (nunique={nunique_val}) in train.\")\n",
    "\n",
    "        final_feature_columns = potential_feature_cols\n",
    "        joblib.dump(final_feature_columns, f'features/feature_columns_{timestamp}.joblib')\n",
    "        logger.info(f\"Saved {len(final_feature_columns)} feature names (const cols NOT dropped).\")\n",
    "        X = data[final_feature_columns]\n",
    "        logger.info(f\"Preprocessing train done. Shape: {X.shape}. Time: {time.time() - start_time:.2f}s\")\n",
    "        try: X.head().to_csv(f'features/processed_features_head_{timestamp}.csv', index=False)\n",
    "        except Exception as e: logger.warning(f\"Could not save head: {e}\")\n",
    "        return X, y, final_feature_columns, le\n",
    "    else: # Test Data\n",
    "        if feature_columns_to_use is None:\n",
    "            try:\n",
    "                col_files = sorted([f for f in os.listdir('features') if f.startswith('feature_columns_')])\n",
    "                if col_files:\n",
    "                    latest_col = col_files[-1]\n",
    "                    feature_columns_to_use = joblib.load(f'features/{latest_col}')\n",
    "                    logger.info(f\"Loaded {len(feature_columns_to_use)} cols from: {latest_col}\")\n",
    "                else:\n",
    "                    logger.error(\"CRITICAL: No feature_columns file.\")\n",
    "                    raise FileNotFoundError(\"feature_columns_*.joblib missing.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load feature columns: {e}.\")\n",
    "                raise\n",
    "\n",
    "        X = pd.DataFrame(columns=feature_columns_to_use); missing_cols = []; processed_cols = list(data.columns); extra_cols = list(set(processed_cols) - set(feature_columns_to_use) - set(columns_to_exclude))\n",
    "        for col in feature_columns_to_use:\n",
    "            if col in data.columns: X[col] = data[col]\n",
    "            else: X[col] = 0; missing_cols.append(col)\n",
    "        if missing_cols: logger.warning(f\"Cols missing in test (filled 0): {missing_cols}\")\n",
    "        if extra_cols: logger.warning(f\"Cols extra in test (dropped align): {extra_cols}\")\n",
    "        X = X[feature_columns_to_use]; logger.info(f\"Preprocessing test done. Shape: {X.shape}. Time: {time.time() - start_time:.2f}s\")\n",
    "        return X, y, feature_columns_to_use, le # y is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "36a0de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight  # Added compute_sample_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "# Imports for models used within the function\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              ExtraTreesClassifier, AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import joblib  # For saving models\n",
    "from sklearn.calibration import CalibratedClassifierCV  # For calibration step\n",
    "\n",
    "# Assume build_keras_model function is defined elsewhere\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "# Assume save_feature_importance function is defined\n",
    "BOOSTING_EARLY_STOPPING_PATIENCE = 50\n",
    "def optimize_model(X, y, timestamp, model_type, n_trials=30, n_jobs_optuna=1):\n",
    "    \"\"\"\n",
    "    Optimizes hyperparameters for a given model type using Optuna,\n",
    "    then trains and saves the final model with best parameters.\n",
    "    Includes class_weight='balanced' or equivalent strategies.\n",
    "    Correctly handles Optuna-specific trial parameters during final instantiation.\n",
    "    Attempts calibration after successful model saving.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting {model_type} optimization ({n_trials} trials)...\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    if not isinstance(y, (np.ndarray, pd.Series)):\n",
    "        y = np.array(y)\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)  # Ensure DataFrame for consistent .iloc\n",
    "\n",
    "    n_classes = len(np.unique(y))\n",
    "    n_features = X.shape[1]\n",
    "    y_keras = to_categorical(y, num_classes=n_classes) if model_type == 'keras_mlp' else y\n",
    "\n",
    "    KERAS_EPOCHS = 150  # Reduced Keras epochs slightly\n",
    "    KERAS_PATIENCE = 25  # Increased Keras patience slightly\n",
    "    OPTUNA_TIMEOUT_PER_MODEL = 3600  # Default 1 hour\n",
    "    # Increase timeout for complex models\n",
    "    if model_type in ['xgboost', 'catboost', 'randomforest', 'gradientboosting', 'extratrees', 'lightgbm']:\n",
    "        OPTUNA_TIMEOUT_PER_MODEL = 7200  # Increase to 2 hours\n",
    "    logger.info(f\"Optuna timeout for {model_type}: {OPTUNA_TIMEOUT_PER_MODEL}s.\")\n",
    "\n",
    "    # --- Optuna Objective Function ---\n",
    "    def objective(trial):\n",
    "        model = None\n",
    "        fit_params = {}\n",
    "        use_gpu = False\n",
    "        is_keras = False\n",
    "\n",
    "        # --- Model Definitions for Optuna Trial (Includes custom weights/params) ---\n",
    "        if model_type == 'xgboost':\n",
    "            tree_method = trial.suggest_categorical('tree_method', ['hist', 'gpu_hist'])\n",
    "            param = {\n",
    "                'objective': 'multi:softprob', # Keep for probabilities\n",
    "                'num_class': n_classes,\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 5000, step=100), # Wider range\n",
    "                'max_depth': trial.suggest_int('max_depth', 4, 26, step=1), # Wider range, finer step\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.007, 0.15, log=True), # Slightly lower min\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0), # Allow full subsample\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0), # Wider range\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 20), # Wider range (regularization)\n",
    "                'gamma': trial.suggest_float('gamma', 1e-7, 1.0, log=True), # Wider upper bound (regularization)\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-7, 20.0, log=True), # Wider range (regularization)\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-7, 20.0, log=True), # Wider range (regularization)\n",
    "                'random_state': 42,\n",
    "                'booster': 'gbtree',\n",
    "                'tree_method': tree_method,\n",
    "                # 'n_jobs': 1 # Handled by tree_method check\n",
    "            }\n",
    "            if tree_method == 'gpu_hist':\n",
    "                param['gpu_id'] = 0\n",
    "                # param.pop('n_jobs', None) # n_jobs not used with gpu_hist\n",
    "            else:\n",
    "                param['n_jobs'] = 1 # Use 1 core for CPU hist for stability within Optuna\n",
    "                param.pop('gpu_id', None)\n",
    "\n",
    "            model = XGBClassifier(**param)\n",
    "            # Early stopping will be added in the CV loop fit call\n",
    "            fit_params = {} # Reset, ES handled in loop\n",
    "\n",
    "        elif model_type == 'catboost':\n",
    "            task_type = trial.suggest_categorical('task_type', ['CPU', 'GPU'])\n",
    "            # More nuanced class weight options - focus on boosting High (0) and Medium (2) slightly\n",
    "            class_weight_options = [\n",
    "                None,\n",
    "                'Balanced',\n",
    "                # Note: Optuna often saves dict keys as strings, ensure final fit handles int conversion if needed\n",
    "                {0: 1.2, 1: 1.0, 2: 1.1}, # Boost High slightly more\n",
    "                {0: 1.3, 1: 1.0, 2: 1.2}, # Boost High more, Medium slightly\n",
    "                {0: 1.1, 1: 1.0, 2: 1.2}, # Boost Medium slightly more\n",
    "            ]\n",
    "            chosen_class_weight_config = trial.suggest_categorical('class_weight_config', class_weight_options)\n",
    "\n",
    "            param = {\n",
    "                'iterations': trial.suggest_int('iterations', 300, 5500, step=100), # Wider range\n",
    "                'depth': trial.suggest_int('depth', 4, 24), # Wider range\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.007, 0.15, log=True), # Slightly lower min\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.5, 30.0, log=True), # Wider range (regularization)\n",
    "                'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0, log=True), # Exploration range\n",
    "                'border_count': trial.suggest_categorical('border_count', [32, 64, 128, 254]), # Added 32\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 0.9), # Wider range\n",
    "                'loss_function': 'MultiClass',\n",
    "                'eval_metric': 'Accuracy', # Using Accuracy as metric, can change to MultiClass\n",
    "                'random_seed': 42,\n",
    "                'thread_count': -1, # Use all available CPU cores if task_type='CPU'\n",
    "                'verbose': False,\n",
    "                'task_type': task_type,\n",
    "                # 'auto_class_weights': None, # Set based on choice below\n",
    "                # 'class_weights': None # Set based on choice below\n",
    "            }\n",
    "            # Apply class weight strategy\n",
    "            if isinstance(chosen_class_weight_config, dict):\n",
    "                param['class_weights'] = chosen_class_weight_config\n",
    "                trial.set_user_attr(\"class_weight_info\", chosen_class_weight_config) # Log the dict\n",
    "            elif chosen_class_weight_config == 'Balanced':\n",
    "                param['auto_class_weights'] = 'Balanced'\n",
    "                trial.set_user_attr(\"class_weight_info\", 'Balanced')\n",
    "            else: # None case\n",
    "                trial.set_user_attr(\"class_weight_info\", 'None')\n",
    "\n",
    "\n",
    "            if task_type == 'GPU':\n",
    "                param['devices'] = '0'\n",
    "                param.pop('thread_count', None) # Not needed for GPU\n",
    "\n",
    "            model = CatBoostClassifier(**param)\n",
    "            fit_params = {'early_stopping_rounds': BOOSTING_EARLY_STOPPING_PATIENCE, 'verbose': False}\n",
    "\n",
    "        elif model_type == 'randomforest':\n",
    "            # More class weight dictionary options\n",
    "            class_weight_choices = [\n",
    "                'balanced',\n",
    "                'balanced_subsample',\n",
    "                # Note: Optuna often saves dict keys as strings, ensure final fit handles int conversion if needed\n",
    "                {0: 1.1, 1: 1.0, 2: 1.1},\n",
    "                {0: 1.2, 1: 1.0, 2: 1.2},\n",
    "                {0: 1.3, 1: 1.0, 2: 1.1},\n",
    "            ]\n",
    "            class_weight = trial.suggest_categorical('class_weight', class_weight_choices)\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 5500, step=100), # Wider range\n",
    "                'max_depth': trial.suggest_int('max_depth', 10, 100, step=2), # Allow deeper trees, rely on leaf constraints\n",
    "                # 'max_depth': trial.suggest_categorical('max_depth', [10, 20, 30, 40, 50, 60, None]), # Alternative: includes None\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 50), # Wider range (regularization)\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 30), # Wider range (regularization)\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.6, 0.8]), # Added float options\n",
    "                'bootstrap': True, # Usually best for RF\n",
    "                'class_weight': class_weight, # Apply choice\n",
    "                'random_state': 42,\n",
    "                'n_jobs': n_jobs_optuna,\n",
    "                'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.05) # Add slight pruning\n",
    "            }\n",
    "            model = RandomForestClassifier(**param)\n",
    "            fit_params = {} # RF doesn't have special fit params here\n",
    "\n",
    "        elif model_type == 'extratrees':\n",
    "            # More class weight dictionary options\n",
    "            class_weight_choices = [\n",
    "                'balanced',\n",
    "                'balanced_subsample',\n",
    "                {0: 1.1, 1: 1.0, 2: 1.1},\n",
    "                {0: 1.2, 1: 1.0, 2: 1.2},\n",
    "                {0: 1.3, 1: 1.0, 2: 1.1},\n",
    "            ]\n",
    "            class_weight = trial.suggest_categorical('class_weight', class_weight_choices)\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 5500, step=500), # Wider range\n",
    "                'max_depth': trial.suggest_int('max_depth', 10, 80, step=2), # Allow deeper\n",
    "                # 'max_depth': trial.suggest_categorical('max_depth', [10, 20, 30, 40, 50, 60, 70, None]), # Alternative\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 40), # Wider range\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 30), # Wider range\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.6, 0.8]), # Added floats\n",
    "                'bootstrap': trial.suggest_categorical('bootstrap', [False, True]), # Tune bootstrap for ET\n",
    "                'class_weight': class_weight, # Apply choice\n",
    "                'random_state': 42,\n",
    "                'n_jobs': n_jobs_optuna,\n",
    "                'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "                'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.05) # Add slight pruning\n",
    "            }\n",
    "            model = ExtraTreesClassifier(**param)\n",
    "            fit_params = {}\n",
    "\n",
    "        elif model_type == 'gradientboosting':\n",
    "            param = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 200, 3500, step=100), # Wider range\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.007, 0.15, log=True), # Slightly lower min\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 14), # Wider range\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 5, 50), # Wider range (regularization)\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 3, 40), # Wider range (regularization)\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0), # Allow 1.0\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.7, 0.9]), # Added floats\n",
    "                'random_state': 42,\n",
    "                'loss': 'log_loss', # Keep log_loss for predict_proba\n",
    "                'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.2), # Wider range\n",
    "                # 'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.1) # Add cost-complexity pruning\n",
    "            }\n",
    "            model = GradientBoostingClassifier(**param)\n",
    "            # Sample weights applied in the CV loop fit call below\n",
    "            fit_params = {}\n",
    "\n",
    "        elif model_type == 'adaboost':\n",
    "            base_depth = trial.suggest_int('base_estimator_max_depth', 1, 6) # Allow slightly deeper base trees\n",
    "            # More class weight options\n",
    "            class_weights_options = [\n",
    "                'balanced',\n",
    "                {0: 1.2, 1: 1.0, 2: 1.1},\n",
    "                {0: 1.3, 1: 1.0, 2: 1.2},\n",
    "                {0: 1.1, 1: 1.0, 2: 1.3},\n",
    "            ]\n",
    "            weight_choice = trial.suggest_categorical('class_weight_choice', class_weights_options)\n",
    "            param_ada = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 5000, step=50), # Wider range\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 2.0, log=True), # Wider range\n",
    "                'algorithm':'SAMME', # Try both\n",
    "                'random_state': 42\n",
    "            }\n",
    "            # Apply class weight choice to the base estimator\n",
    "            base_est = DecisionTreeClassifier(max_depth=base_depth, random_state=42, class_weight=weight_choice)\n",
    "            model = AdaBoostClassifier(estimator=base_est, **param_ada)\n",
    "            # Log info for final model reconstruction\n",
    "            trial.set_user_attr(\"base_estimator_max_depth\", base_depth)\n",
    "            trial.set_user_attr(\"class_weight_info\", weight_choice if isinstance(weight_choice, dict) else 'balanced' if weight_choice=='balanced' else 'None')\n",
    "            trial.set_user_attr(\"algorithm\", param_ada['algorithm']) # Log algorithm choice\n",
    "            fit_params = {}\n",
    "\n",
    "        elif model_type == 'lightgbm':\n",
    "            # More class weight options\n",
    "            class_weight_options = [\n",
    "                None,\n",
    "                'balanced',\n",
    "                {0: 1.2, 1: 1.0, 2: 1.1}, # Boost High slightly more\n",
    "                {0: 1.3, 1: 1.0, 2: 1.2}, # Boost High more, Medium slightly\n",
    "                {0: 1.1, 1: 1.0, 2: 1.3}, # Boost Medium more\n",
    "            ]\n",
    "            class_weight = trial.suggest_categorical('class_weight_option', class_weight_options)\n",
    "            param = {\n",
    "                'objective': 'multiclass',\n",
    "                'num_class': n_classes,\n",
    "                'metric': 'multi_logloss', # Standard metric for multi-class probabilities\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 300, 5500, step=100), # Wider range\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.007, 0.15, log=True), # Slightly lower min\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 500, step=5), # Wider range (key param)\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 50), # Wider range, can be -1 if num_leaves is constrained\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0), # Allow 1.0\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0), # Allow 1.0\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 20.0, log=True), # Wider range (regularization)\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 20.0, log=True), # Wider range (regularization)\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 3, 60), # Wider range (regularization)\n",
    "                'class_weight': class_weight, # Apply choice\n",
    "                'random_state': 42,\n",
    "                'n_jobs': n_jobs_optuna,\n",
    "                'verbose': -1,\n",
    "                'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']) # Keep both\n",
    "                # Consider adding 'min_split_gain'\n",
    "                # 'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.1)\n",
    "            }\n",
    "            model = lgb.LGBMClassifier(**param)\n",
    "            # Use specific early stopping callback for LGBM\n",
    "            fit_params = {'callbacks': [lgb.early_stopping(BOOSTING_EARLY_STOPPING_PATIENCE, verbose=False)]}\n",
    "            \n",
    "\n",
    "        else:\n",
    "            logger.error(f\"Unsupported model type: {model_type}\")\n",
    "            raise ValueError(f\"Unsupported: {model_type}\")\n",
    "\n",
    "        # --- Cross-validation ---\n",
    "        scores = []\n",
    "        is_dataframe = isinstance(X, pd.DataFrame)\n",
    "        try:\n",
    "            for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "                # --- Use .iloc consistently for pandas objects ---\n",
    "                if is_dataframe: # If X is a DataFrame, assume y is a Series\n",
    "                    X_train_fold = X.iloc[train_idx]\n",
    "                    X_valid_fold = X.iloc[valid_idx]\n",
    "                    # Select training labels based on position\n",
    "                    y_train_fold = y_keras[train_idx] if is_keras else y.iloc[train_idx] # ***MODIFIED***\n",
    "                    # Select validation labels based on position\n",
    "                    y_valid_fold_orig = y.iloc[valid_idx] # ***MODIFIED***\n",
    "                else: # If X is a numpy array, assume y is also numpy array\n",
    "                    X_train_fold = X[train_idx]\n",
    "                    X_valid_fold = X[valid_idx]\n",
    "                    y_train_fold = y_keras[train_idx] if is_keras else y[train_idx]\n",
    "                    y_valid_fold_orig = y[valid_idx]\n",
    "                current_fit_params = fit_params.copy()\n",
    "\n",
    "                # --- Handle Sample Weights for Models That Need It in Fit ---\n",
    "                fold_sample_weight = None\n",
    "                if model_type in ['gradientboosting']:  \n",
    "                    # Calculate balanced weights\n",
    "                    sample_weight = compute_sample_weight('balanced', y=y_train_fold)\n",
    "                    # Apply custom emphasis based on strategy (e.g., boost High/Medium)\n",
    "                    emphasis_weights = {0: 1.1, 1: 1.0, 2: 1.1}  # Example emphasis\n",
    "                    for cls_idx, weight_multiplier in emphasis_weights.items():\n",
    "                         # Ensure y_train_fold is numpy for boolean indexing if it was a Series\n",
    "                         y_train_fold_np = y_train_fold.values if isinstance(y_train_fold, pd.Series) else y_train_fold\n",
    "                         sample_weight[y_train_fold_np == cls_idx] *= weight_multiplier\n",
    "                    fold_sample_weight = sample_weight\n",
    "                    current_fit_params['sample_weight'] = fold_sample_weight\n",
    "                    logger.debug(f\"Trial {trial.number} Fold {fold+1}: Applied sample weights for {model_type}\")\n",
    "\n",
    "                try:\n",
    "                    # Pass eval_set for models that use it with callbacks/early stopping\n",
    "                    eval_set = [(X_valid_fold, y_valid_fold_orig)]\n",
    "                    \n",
    "                    # --- XGBoost Specific Fit Call ---\n",
    "                    if model_type == 'xgboost':\n",
    "                         model.fit(X_train_fold, y_train_fold,\n",
    "                                   eval_set=eval_set,\n",
    "                                   # Pass directly\n",
    "                                   verbose=False) # Pass other relevant args directly if needed\n",
    "                    # --- LightGBM Specific Fit Call (already seemed correct) ---\n",
    "                    elif model_type == 'lightgbm':\n",
    "                         # Note: LGBM uses callbacks for early stopping, passed via fit_params\n",
    "                         current_fit_params['eval_set'] = eval_set\n",
    "                         current_fit_params['eval_metric'] = 'multi_logloss' # Or match objective metric\n",
    "                         model.fit(X_train_fold, y_train_fold, **current_fit_params)\n",
    "                    # --- CatBoost Specific Fit Call (already seemed correct) ---\n",
    "                    elif model_type == 'catboost':\n",
    "                         current_fit_params['eval_set'] = eval_set\n",
    "                         # Early stopping rounds already part of CatBoost init/params\n",
    "                         model.fit(X_train_fold, y_train_fold, **current_fit_params)\n",
    "                    # --- Default Fit Call for other models ---\n",
    "                    else:\n",
    "                         # Pass sample_weight if applicable (e.g., for GB)\n",
    "                         model.fit(X_train_fold, y_train_fold, **current_fit_params)\n",
    "                    \n",
    "                    # Predict and score\n",
    "                    y_pred = model.predict(X_valid_fold)\n",
    "                    if is_keras and y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "                        y_pred = np.argmax(y_pred, axis=1)\n",
    "                    score = accuracy_score(y_valid_fold_orig, y_pred)\n",
    "                    scores.append(score)\n",
    "                    logger.debug(f\"Trial {trial.number} Fold {fold+1} Score: {score:.5f}\")\n",
    "\n",
    "                except ValueError as ve:\n",
    "                    logger.warning(f\"CV fold {fold+1} VAL ERROR {model_type} trial {trial.number}: {ve}\")\n",
    "                    return 0.0\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"CV fold {fold+1} EXCEPTION {model_type} trial {trial.number}: {e}\", exc_info=True)\n",
    "                    scores = []\n",
    "                    break  # Log full traceback\n",
    "        except Exception as outer_e:\n",
    "            logger.error(f\"Outer CV error {model_type} trial {trial.number}: {outer_e}\", exc_info=True)\n",
    "            return 0.0\n",
    "        if not scores:\n",
    "            logger.error(f\"Cross-validation failed completely for {model_type} trial {trial.number}\")\n",
    "            return 0.0\n",
    "        mean_score = np.mean(scores)\n",
    "        logger.debug(f\"Trial {trial.number} ({model_type}) completed. Avg CV Score: {mean_score:.5f}\")\n",
    "        return mean_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Run Optuna Study ---\n",
    "    study_name = f\"{model_type}_opt_{timestamp}\"\n",
    "    storage_name = f\"sqlite:///optuna_trials/{study_name}.db\"\n",
    "    study = optuna.create_study(direction='maximize', study_name=study_name, storage=storage_name, \n",
    "                              load_if_exists=True, pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "    completed_trials = len([t for t in study.trials if t.state==optuna.trial.TrialState.COMPLETE])\n",
    "    trials_to_run = n_trials-completed_trials\n",
    "    \n",
    "    if trials_to_run > 0:\n",
    "        logger.info(f\"Setting Optuna timeout {OPTUNA_TIMEOUT_PER_MODEL}s.\")\n",
    "        try:\n",
    "            study.optimize(objective, n_trials=trials_to_run, timeout=OPTUNA_TIMEOUT_PER_MODEL, n_jobs=1)\n",
    "        except Exception as opt_e:\n",
    "            logger.error(f\"Optuna optimize fail {model_type}: {opt_e}\", exc_info=True)\n",
    "            return None, -1, {}\n",
    "    else:\n",
    "        logger.info(f\"Study {study_name} has {completed_trials} trials. Skip optimize.\")\n",
    "\n",
    "    # --- Retrieve Results ---\n",
    "    try:\n",
    "        if not any(t.state == optuna.trial.TrialState.COMPLETE for t in study.trials):\n",
    "            logger.error(f\"Optuna study {model_type} no successful trials.\")\n",
    "            return None, -1, {}\n",
    "        best_trial = study.best_trial\n",
    "        best_params = best_trial.params\n",
    "        best_cv_score = best_trial.value\n",
    "    except ValueError:\n",
    "        logger.error(f\"Optuna study {model_type} no best trial.\")\n",
    "        return None, -1, {}\n",
    "    except Exception as res_e:\n",
    "        logger.error(f\"Error get Optuna results {model_type}: {res_e}\", exc_info=True)\n",
    "        return None, -1, {}\n",
    "    logger.info(f\"Opt complete {model_type}. Best CV score: {best_cv_score:.5f}. Best params: {best_params}\")\n",
    "\n",
    "    # --- Save Study Summary ---\n",
    "    try:\n",
    "        summary_file = f'optuna_trials/{model_type}_study_summary_{timestamp}.txt'\n",
    "        params_json = best_params.copy()\n",
    "        if model_type=='adaboost' and \"base_estimator_max_depth\" in best_trial.user_attrs:\n",
    "            params_json['base_estimator_max_depth'] = best_trial.user_attrs[\"base_estimator_max_depth\"]\n",
    "            params_json['class_weight_info'] = best_trial.user_attrs.get(\"class_weight_info\", \"N/A\")\n",
    "        if model_type=='xgboost' and 'tree_method' in best_params:\n",
    "            params_json['tree_method'] = best_params['tree_method']\n",
    "        if model_type=='catboost' and 'task_type' in best_params:\n",
    "            params_json['task_type'] = best_params['task_type']\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(f\"Optuna Summary: {model_type}\\nTS: {timestamp}\\nBest Trial: {best_trial.number}\\nScore: {best_cv_score:.5f}\\n\\nParams:\\n\")\n",
    "            json.dump(params_json, f, indent=4)\n",
    "        logger.info(f\"Saved Optuna summary: {summary_file}\")\n",
    "    except Exception as file_e:\n",
    "        logger.warning(f\"Could not save Optuna summary {model_type}: {file_e}\")\n",
    "\n",
    "    # --- Train final model ---\n",
    "    final_model = None\n",
    "    final_fit_params = {}  # Reset for final fit\n",
    "    try:\n",
    "        logger.info(f\"Instantiating final {model_type} model...\")\n",
    "        # Clean best_params from Optuna-specific args before final instantiation\n",
    "        params_for_final = best_params.copy()\n",
    "        optuna_internal_params = ['class_weight_option', 'class_weight_choice', 'class_weight_idx', \n",
    "                                 'class_weight_strategy', 'use_smote', 'smote_k', \n",
    "                                 'use_focal_loss', 'focal_gamma']  # Params used only in objective logic\n",
    "        for p in optuna_internal_params:\n",
    "            params_for_final.pop(p, None)\n",
    "\n",
    "        # Inside optimize_model, after Optuna, in elif model_type == 'adaboost':\n",
    "        if model_type == 'adaboost':\n",
    "            # Clean best_params from Optuna-specific args before final instantiation\n",
    "            params_for_final = best_params.copy()\n",
    "            # List internal params used only during Optuna trials\n",
    "            optuna_internal_params = ['class_weight_choice', 'base_estimator_max_depth']\n",
    "            for p in optuna_internal_params:\n",
    "                # --- THESE LINES REMOVE THE BAD PARAMETERS ---\n",
    "                params_for_final.pop(p, None)\n",
    "                # --- END OF REMOVAL ---\n",
    "\n",
    "            # Retrieve correct values from Optuna trial attributes\n",
    "            best_d = best_trial.user_attrs.get('base_estimator_max_depth', 1)\n",
    "            weight_info_raw = best_trial.user_attrs.get(\"class_weight_info\", 'balanced')\n",
    "            \n",
    "            # --- *** ADDED: Convert dictionary keys if needed *** ---\n",
    "            weight_info_processed = weight_info_raw\n",
    "            if isinstance(weight_info_raw, dict):\n",
    "                try:\n",
    "                    # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                    weight_info_processed = {int(k): v for k, v in weight_info_raw.items()}\n",
    "                    logger.info(f\"Converted AdaBoost class_weight keys to int: {weight_info_processed}\")\n",
    "                except ValueError as e:\n",
    "                     logger.error(f\"Error converting AdaBoost class_weight keys: {e}. Using raw: {weight_info_raw}\")\n",
    "                     weight_info_processed = weight_info_raw # Fallback to raw if conversion fails\n",
    "            # --- *** END KEY CONVERSION *** ---\n",
    "            \n",
    "            logger.info(f\"Reconstruct AdaBoost DT(max_depth={best_d}, class_weight={weight_info_processed}) using SAMME\")\n",
    "            # Create the base estimator correctly\n",
    "            base_est_inst = DecisionTreeClassifier(max_depth=best_d, random_state=42, class_weight=weight_info_processed)\n",
    "\n",
    "            final_p_ada = params_for_final # Use the cleaned dictionary for AdaBoost itself\n",
    "            final_p_ada['algorithm'] = 'SAMME'\n",
    "            # --- FINAL MODEL TRAINING WILL STILL HAPPEN USING base_est_inst and final_p_ada ---\n",
    "            final_model = AdaBoostClassifier(estimator=base_est_inst, **final_p_ada)\n",
    "\n",
    "        elif model_type == 'xgboost':\n",
    "            final_params_xgb = params_for_final.copy()\n",
    "            final_params_xgb['objective'] = 'multi:softprob'\n",
    "            final_params_xgb['num_class'] = n_classes\n",
    "            final_params_xgb['n_jobs'] = 1\n",
    "            logger.info(\"XGBoost final model - balancing via sample_weight in fit.\")\n",
    "            final_model = XGBClassifier(**final_params_xgb)\n",
    "            # Prepare sample weights for fit step\n",
    "            sample_weights_xgb = compute_sample_weight('balanced', y=y)  # Start with balanced\n",
    "            emphasis_weights = {0: 2, 1: 1.0, 2: 2}  # Emphasize High/Medium\n",
    "            for cls_idx, weight_multiplier in emphasis_weights.items():\n",
    "                sample_weights_xgb[y == cls_idx] *= weight_multiplier\n",
    "            final_fit_params['sample_weight'] = sample_weights_xgb\n",
    "\n",
    "        elif model_type == 'catboost':\n",
    "            final_params_cat = params_for_final.copy()\n",
    "            final_params_cat['loss_function'] = 'MultiClass'\n",
    "            final_params_cat['verbose'] = False\n",
    "            # Re-apply class weight strategy based on best trial's choice\n",
    "            chosen_weight = best_params.get('class_weight_option')\n",
    "            if isinstance(chosen_weight, dict):\n",
    "                final_params_cat['class_weights'] = chosen_weight\n",
    "                logger.info(f\"CatBoost using custom weights: {chosen_weight}\")\n",
    "            elif chosen_weight == 'Balanced':\n",
    "                final_params_cat['auto_class_weights'] = 'Balanced'\n",
    "                logger.info(\"CatBoost using auto_class_weights=Balanced\")\n",
    "            else:\n",
    "                logger.info(\"CatBoost using default balancing or no weights.\")\n",
    "            final_model = CatBoostClassifier(**final_params_cat)\n",
    "\n",
    "        elif model_type == 'randomforest':\n",
    "            final_params_rf = params_for_final.copy()\n",
    "            final_params_rf['n_jobs'] = n_jobs_optuna\n",
    "            \n",
    "            # --- *** ADDED: Process class_weight dictionary keys *** ---\n",
    "            class_weight_raw = best_params.get('class_weight', 'balanced')\n",
    "            class_weight_processed = class_weight_raw\n",
    "            if isinstance(class_weight_raw, dict):\n",
    "                 try:\n",
    "                     # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                     class_weight_processed = {int(k): v for k, v in class_weight_raw.items()}\n",
    "                     logger.info(f\"Converted RF class_weight keys to int: {class_weight_processed}\")\n",
    "                 except ValueError as e:\n",
    "                      logger.error(f\"Error converting RF class_weight keys: {e}. Using raw: {class_weight_raw}\")\n",
    "                      class_weight_processed = class_weight_raw # Fallback\n",
    "            # --- *** END KEY CONVERSION *** ---\n",
    "            \n",
    "            final_params_rf['class_weight'] = best_params.get('class_weight', 'balanced')  # Use optimized or default balanced\n",
    "            logger.info(f\"RF final model using class_weight={final_params_rf['class_weight']}\")\n",
    "            final_model = RandomForestClassifier(**final_params_rf)\n",
    "\n",
    "        elif model_type == 'extratrees':\n",
    "            final_params_et = params_for_final.copy()\n",
    "            final_params_et['n_jobs'] = n_jobs_optuna\n",
    "            \n",
    "            # --- *** ADDED: Process class_weight dictionary keys *** ---\n",
    "            class_weight_raw = best_params.get('class_weight', 'balanced')\n",
    "            class_weight_processed = class_weight_raw\n",
    "            if isinstance(class_weight_raw, dict):\n",
    "                 try:\n",
    "                     # Convert string keys ('0', '1', ...) to integers (0, 1, ...)\n",
    "                     class_weight_processed = {int(k): v for k, v in class_weight_raw.items()}\n",
    "                     logger.info(f\"Converted ET class_weight keys to int: {class_weight_processed}\")\n",
    "                 except ValueError as e:\n",
    "                      logger.error(f\"Error converting ET class_weight keys: {e}. Using raw: {class_weight_raw}\")\n",
    "                      class_weight_processed = class_weight_raw # Fallback\n",
    "            # --- *** END KEY CONVERSION *** ---\n",
    "            \n",
    "            final_params_et['class_weight'] = best_params.get('class_weight', 'balanced')  # Use optimized or default balanced\n",
    "            logger.info(f\"ET final model using class_weight={final_params_et['class_weight']}\")\n",
    "            final_model = ExtraTreesClassifier(**final_params_et)\n",
    "\n",
    "        elif model_type == 'gradientboosting':\n",
    "            final_params_gb = params_for_final.copy()\n",
    "            logger.info(\"GradientBoosting final model - applying sample_weight in fit\")\n",
    "            final_model = GradientBoostingClassifier(**final_params_gb)\n",
    "            sample_weights_gb = compute_sample_weight('balanced', y=y)\n",
    "            emphasis_weights = {0: 1.6, 1: 1.0, 2: 1.5}\n",
    "            for cls_idx, mult in emphasis_weights.items():\n",
    "                sample_weights_gb[y == cls_idx] *= mult\n",
    "            final_fit_params['sample_weight'] = sample_weights_gb\n",
    "\n",
    "        # Inside optimize_model, after Optuna, in elif model_type == 'knn':\n",
    "\n",
    "        elif model_type == 'lightgbm':\n",
    "            final_params_lgbm = params_for_final.copy()\n",
    "            final_params_lgbm['objective'] = 'multiclass'\n",
    "            final_params_lgbm['num_class'] = n_classes\n",
    "            final_params_lgbm['n_jobs'] = n_jobs_optuna\n",
    "            \n",
    "            # --- *** ADDED: Refined Key Conversion for LGBM *** ---\n",
    "            class_weight_value_to_use = 'balanced' # Default\n",
    "            # Use 'class_weight_option' key from Optuna params for LGBM\n",
    "            if 'class_weight_option' in best_params:\n",
    "                class_weight_raw = best_params['class_weight_option'] # Get raw value from Optuna result\n",
    "                class_weight_processed = class_weight_raw\n",
    "\n",
    "                if isinstance(class_weight_raw, dict):\n",
    "                    logger.info(f\"Raw class_weight dict found for LGBM: {class_weight_raw}\")\n",
    "                    try:\n",
    "                        if all(isinstance(k, int) for k in class_weight_raw.keys()):\n",
    "                            logger.info(\"LGBM class_weight keys appear to be integers already.\")\n",
    "                            class_weight_processed = class_weight_raw\n",
    "                        else:\n",
    "                            logger.info(\"Attempting conversion of LGBM class_weight keys to int...\")\n",
    "                            class_weight_processed = {int(k): v for k, v in class_weight_raw.items()}\n",
    "                            logger.info(f\"Successfully converted LGBM class_weight keys to int: {class_weight_processed}\")\n",
    "                    except Exception as e_gen:\n",
    "                         logger.error(f\"Error processing LGBM class_weight dict: {e_gen}. Using 'balanced'.\")\n",
    "                         class_weight_processed = 'balanced'\n",
    "                class_weight_value_to_use = class_weight_processed\n",
    "            else:\n",
    "                 logger.info(\"No 'class_weight_option' found in best_params for LGBM, using default 'balanced'.\")\n",
    "                 class_weight_value_to_use = 'balanced'\n",
    "            \n",
    "            final_params_lgbm['class_weight'] = class_weight_value_to_use\n",
    "            logger.info(f\"LGBM final model using class_weight={final_params_lgbm['class_weight']}\")\n",
    "            final_model = lgb.LGBMClassifier(**final_params_lgbm)\n",
    "\n",
    "        # --- Fit the final model ---\n",
    "        if final_model is not None:\n",
    "            logger.info(f\"Fitting final {model_type} model...\")\n",
    "            start_fit_time = time.time()\n",
    "            model_fitted_successfully = False\n",
    "            try:\n",
    "                # Fit using specific params if they exist (like sample_weight)\n",
    "                if final_fit_params:\n",
    "                    logger.info(f\"Fitting {model_type} with additional fit parameters: {list(final_fit_params.keys())}\")\n",
    "                    final_model.fit(X, y, **final_fit_params)  # Pass original y and weights dict\n",
    "                else:\n",
    "                    final_model.fit(X, y)  # Fit standard models\n",
    "\n",
    "                fit_duration = time.time() - start_fit_time\n",
    "                logger.info(f\"Final {model_type} fitted in {fit_duration:.2f}s.\")\n",
    "                model_fitted_successfully = True\n",
    "\n",
    "            except Exception as fit_e:\n",
    "                logger.error(f\"Error during final fit for {model_type}: {fit_e}\", exc_info=True)\n",
    "                # Keep going to return score/params, but model will be None\n",
    "\n",
    "            # --- Save model and importance only if fit succeeded ---\n",
    "            if model_fitted_successfully:\n",
    "                model_path = f'models/{model_type}_{timestamp}.joblib'\n",
    "                logger.info(f\"Saving final {model_type} model...\")\n",
    "                try:\n",
    "                    if isinstance(final_model, KerasClassifier):\n",
    "                        tf_model_save_path = f'models/{model_type}_tfmodel_{timestamp}'\n",
    "                        try:\n",
    "                            final_model.model_.save(tf_model_save_path)\n",
    "                            logger.info(f\"Saved Keras TF model: {tf_model_save_path}\")\n",
    "                        except Exception as k_save_err:\n",
    "                            logger.warning(f\"Keras TF save fail ({k_save_err}), try joblib...\")\n",
    "                            joblib.dump(final_model, model_path)\n",
    "                            logger.info(f\"Saved Keras wrapper: {model_path}\")\n",
    "                    else:\n",
    "                        joblib.dump(final_model, model_path)\n",
    "                        logger.info(f\"Saved final {model_type} via joblib: {model_path}\")\n",
    "                except Exception as save_err:\n",
    "                    logger.error(f\"Failed save model {model_type}: {save_err}\", exc_info=True)\n",
    "\n",
    "                # --- Attempt Calibration AFTER saving base model ---\n",
    "                if model_type not in ['knn', 'mlp']:  # Models less suitable or needing sample_weight for calibration fit\n",
    "                    try:\n",
    "                        logger.info(f\"Attempting calibration for {model_type}...\")\n",
    "                        # Use 'estimator' argument, not 'base_estimator'\n",
    "                        calibrated_model = CalibratedClassifierCV(\n",
    "                            estimator=final_model,\n",
    "                            cv=3,\n",
    "                            method='isotonic',\n",
    "                            n_jobs=n_jobs_optuna,\n",
    "                            ensemble=False\n",
    "                        )\n",
    "                        calibrated_model.fit(X, y)  # Calibrate on the full training data\n",
    "                        calibrated_path = f'calibrated_models/{model_type}_calibrated_{timestamp}.joblib'\n",
    "                        if not os.path.exists('calibrated_models'):\n",
    "                            os.makedirs('calibrated_models')\n",
    "                        joblib.dump(calibrated_model, calibrated_path)\n",
    "                        logger.info(f\"Saved calibrated model: {calibrated_path}\")\n",
    "                    except Exception as cal_err:\n",
    "                        logger.warning(f\"Calibration failed for {model_type}: {cal_err}\", exc_info=False)\n",
    "\n",
    "                # --- Save Importance ---\n",
    "                feat_names = list(X.columns) if isinstance(X, pd.DataFrame) else None\n",
    "                if feat_names:\n",
    "                    logger.info(f\"Saving importance {model_type}...\")\n",
    "                    save_feature_importance(final_model, feat_names, timestamp, model_type)\n",
    "                else:\n",
    "                    logger.warning(f\"No feat names for importance {model_type}.\")\n",
    "\n",
    "            else:  # Fit failed\n",
    "                final_model = None  # Ensure model is None if fit failed\n",
    "\n",
    "        else:  # Instantiation failed\n",
    "            logger.error(f\"Could not instantiate final model {model_type}.\")\n",
    "            return None, best_cv_score, best_params\n",
    "\n",
    "    except Exception as final_e:\n",
    "        logger.error(f\"Failed final instantiate/fit/save process {model_type}: {final_e}\", exc_info=True)\n",
    "        # Return score/params from Optuna, but model is None\n",
    "        return None, best_cv_score, best_params\n",
    "\n",
    "    # Return potentially None model if fit/save failed, but score/params if Optuna succeeded\n",
    "    return final_model, best_cv_score, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8951a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import logging\n",
    "import os\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "# Meta-learner imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb # For new default meta-learner\n",
    "# Calibration import (needed if implementing Step 3)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scikeras.wrappers import KerasClassifier # For type checking\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_ensemble(qualified_models_with_scores, X_train_ensemble, y_train_ensemble, timestamp, n_jobs_ensemble=1):\n",
    "    \"\"\"\n",
    "    Creates Voting and Stacking ensembles from a list of qualified models.\n",
    "    Default Stacking meta-learner is now LightGBM with class weights.\n",
    "    Includes comments for adding model calibration.\n",
    "\n",
    "    Args:\n",
    "        qualified_models_with_scores (list): List of tuples: (name, fitted_model, cv_score).\n",
    "                                             Assumes models were potentially trained with class weights.\n",
    "        X_train_ensemble (pd.DataFrame or np.ndarray): Training features for fitting ensembles.\n",
    "        y_train_ensemble (pd.Series or np.ndarray): Training target for fitting ensembles.\n",
    "        timestamp (str): Timestamp string for saving files.\n",
    "        n_jobs_ensemble (int): Number of parallel jobs for ensemble fitting.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fitted_voting_classifier, fitted_stacking_classifier, best_individual_model_object)\n",
    "               Models can be None if creation failed or skipped.\n",
    "    \"\"\"\n",
    "    logger.info(\"Attempting ensemble creation...\")\n",
    "\n",
    "    if not qualified_models_with_scores:\n",
    "        logger.error(\"No qualified models provided for ensemble creation.\")\n",
    "        return None, None, None\n",
    "\n",
    "    sorted_models = sorted(qualified_models_with_scores, key=lambda x: x[2], reverse=True)\n",
    "    logger.info(f\"Qualified models for ensembling (Name, CV Score): {[(m[0], f'{m[2]:.5f}') for m in sorted_models]}\")\n",
    "    N_ens = len(sorted_models)\n",
    "\n",
    "    if N_ens < 2:\n",
    "        logger.warning(f\"Less than 2 qualified models ({N_ens}). Skipping ensembles.\")\n",
    "        if N_ens == 1: n, m, s = sorted_models[0]; logger.info(f\"Returning single best model: {n} (CV: {s:.5f})\"); return None, None, m\n",
    "        else: return None, None, None\n",
    "\n",
    "    # Filter usable models (as before)\n",
    "    estimators_valid_for_ensemble = []\n",
    "    keras_models_excluded = []\n",
    "    for name, model, score in sorted_models:\n",
    "        is_keras_wrapper = isinstance(model, KerasClassifier); model_saved_path = f'models/{name}_{timestamp}.joblib'; tf_model_path = f'models/{name}_tfmodel_{timestamp}'\n",
    "        if is_keras_wrapper and not os.path.exists(tf_model_path) and not os.path.exists(model_saved_path): logger.warning(f\"Keras model {name} save files missing. Exclude.\"); keras_models_excluded.append(name)\n",
    "        else: estimators_valid_for_ensemble.append((name, model))\n",
    "\n",
    "    if len(estimators_valid_for_ensemble) < 2:\n",
    "        logger.warning(f\"<2 models usable for ensemble. Skipping.\"); non_keras_models = [(n, m, s) for n, m, s in sorted_models if n not in keras_models_excluded]\n",
    "        if non_keras_models: best_n, best_m, best_s = non_keras_models[0]; logger.info(f\"Return best non-excluded: {best_n} ({best_s:.5f})\"); return None, None, best_m\n",
    "        elif sorted_models: best_n, best_m, best_s = sorted_models[0]; logger.info(f\"Return original best: {best_n} ({best_s:.5f})\"); return None, None, best_m\n",
    "        else: return None, None, None\n",
    "\n",
    "    logger.info(f\"Using {len(estimators_valid_for_ensemble)} models for ensemble: {[n for n,m in estimators_valid_for_ensemble]}\")\n",
    "    est_ens = estimators_valid_for_ensemble\n",
    "\n",
    "    # --- Optional Calibration Step (Implement if needed based on Step 3 plan) ---\n",
    "    # Example: Calibrate base models before putting them in est_ens_calibrated\n",
    "    est_ens_calibrated = []\n",
    "    logger.info(\"Calibrating base models for ensemble...\")\n",
    "    for name, model in est_ens:\n",
    "        try:\n",
    "            # Use isotonic calibration, fit on the same training data used for ensemble fitting\n",
    "            # CV within CalibratedClassifierCV helps prevent overfitting during calibration itself\n",
    "            calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv=3, n_jobs=n_jobs_ensemble, ensemble=False) # Fit base estimator on each fold\n",
    "            calibrated_model.fit(X_train_ensemble, y_train_ensemble)\n",
    "            calibrated_model_path = f'calibrated_models/{name}_calibrated_{timestamp}.joblib'\n",
    "            joblib.dump(calibrated_model, calibrated_model_path)\n",
    "            est_ens_calibrated.append((name + \"_calibrated\", calibrated_model)) # Use new name and calibrated model\n",
    "            logger.info(f\"Calibrated model {name} and saved to {calibrated_model_path}\")\n",
    "        except Exception as cal_err:\n",
    "            logger.error(f\"Failed to calibrate model {name}: {cal_err}. Skipping calibration for this model.\", exc_info=True)\n",
    "            est_ens_calibrated.append((name, model)) # Use original model if calibration fails\n",
    "    est_ens_to_use = est_ens_calibrated # Use the calibrated list for subsequent steps\n",
    "    logger.info(f\"Using {len(est_ens_to_use)} models (calibrated where possible) for final ensemble.\")\n",
    "    # --- End Optional Calibration Step ---\n",
    "\n",
    "    # Use original (potentially uncalibrated) estimators for now\n",
    "    est_ens_to_use = est_ens\n",
    "\n",
    "    # --- Voting Classifier ---\n",
    "    vote_clf = None\n",
    "    can_soft = all(hasattr(m, 'predict_proba') for _, m in est_ens_to_use)\n",
    "    weights_used = None\n",
    "\n",
    "    if can_soft:\n",
    "        logger.info(\"Attempting Soft Voting...\")\n",
    "        # Weighting based on original CV scores (even if models are calibrated)\n",
    "        scores_map = {name: score for name, model, score in qualified_models_with_scores}\n",
    "        # Use original names to get scores, handle potentially calibrated names\n",
    "        scores = [scores_map.get(name.replace('_calibrated','')) for name, model in est_ens_to_use if name.replace('_calibrated','') in scores_map]\n",
    "\n",
    "        if scores and len(scores) == len(est_ens_to_use): # Ensure weights align\n",
    "            min_s = min(scores); shift_s = [s - min_s + 1e-6 for s in scores]; tot_s = sum(shift_s)\n",
    "            weights_used = [s / tot_s for s in shift_s] if tot_s > 0 else None\n",
    "            logger.info(f\"Weights:{list(np.round(weights_used,3)) if weights_used else 'Uniform'}\")\n",
    "        else:\n",
    "            logger.warning(\"Could not align scores for weighting. Using uniform weights.\")\n",
    "            weights_used = None\n",
    "\n",
    "        try:\n",
    "            vote_clf = VotingClassifier(estimators=est_ens_to_use, voting='soft', weights=weights_used, n_jobs=n_jobs_ensemble)\n",
    "            vote_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            vote_path = f'models/voting_ensemble_soft_{timestamp}.joblib'\n",
    "            joblib.dump(vote_clf, vote_path)\n",
    "            logger.info(f\"Saved Soft Voting Ensemble: {vote_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Soft Voting: {e}\", exc_info=True); vote_clf = None; can_soft = False\n",
    "    else:\n",
    "        logger.warning(\"Not all base models support predict_proba for Soft Voting.\")\n",
    "\n",
    "    if not can_soft: # Fallback or initial choice\n",
    "        logger.warning(\"Attempting Hard Voting...\")\n",
    "        weights_used = None # No weights for hard voting\n",
    "        try:\n",
    "            vote_clf = VotingClassifier(estimators=est_ens_to_use, voting='hard', n_jobs=n_jobs_ensemble)\n",
    "            vote_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            vote_path = f'models/voting_ensemble_hard_{timestamp}.joblib'\n",
    "            joblib.dump(vote_clf, vote_path)\n",
    "            logger.info(f\"Saved Hard Voting Ensemble: {vote_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Hard Voting: {e}\", exc_info=True); vote_clf = None\n",
    "\n",
    "    # --- Stacking Classifier ---\n",
    "    stack_clf = None\n",
    "    # Check predict_proba on the models actually used in the ensemble list\n",
    "    can_stack = all(hasattr(m, 'predict_proba') for _, m in est_ens_to_use)\n",
    "    meta_learner = None\n",
    "\n",
    "    if can_stack:\n",
    "        logger.info(\"Attempting Stacking...\")\n",
    "        try:\n",
    "            # *** Use LightGBM as meta-learner with class weights ***\n",
    "            meta_learner = lgb.LGBMClassifier(\n",
    "                n_estimators=150, # Reasonably more estimators for meta\n",
    "                learning_rate=0.05, # Slightly lower LR\n",
    "                num_leaves=20,      # Limit complexity\n",
    "                # max_depth=5,       # Optional: limit depth further\n",
    "                class_weight='balanced', # Crucial for imbalanced meta-predictions\n",
    "                random_state=42,\n",
    "                n_jobs=1 # Meta learner should run on single core\n",
    "            )\n",
    "            # Alternative: Simpler Logistic Regression\n",
    "            # meta_learner = LogisticRegression(random_state=42, class_weight='balanced', solver='liblinear', C=1.0, n_jobs=1)\n",
    "\n",
    "            logger.info(f\"Using {meta_learner.__class__.__name__} as stacking meta-learner.\")\n",
    "            stack_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "            stack_clf = StackingClassifier(\n",
    "                estimators=est_ens_to_use, # Use potentially calibrated models\n",
    "                final_estimator=meta_learner,\n",
    "                cv=stack_cv,\n",
    "                stack_method='predict_proba',\n",
    "                n_jobs=1,\n",
    "                passthrough=False\n",
    "            )\n",
    "            stack_clf.fit(X_train_ensemble, y_train_ensemble)\n",
    "            stack_path = f'models/stacking_ensemble_{timestamp}.joblib'\n",
    "            joblib.dump(stack_clf, stack_path)\n",
    "            logger.info(f\"Saved Stacking Ensemble (Meta: {meta_learner.__class__.__name__}): {stack_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed Stacking: {e}\", exc_info=True)\n",
    "            stack_clf = None\n",
    "    else:\n",
    "        logger.warning(\"Cannot create Stacking Ensemble (requires predict_proba).\")\n",
    "\n",
    "    # Determine best individual model from the original qualified list\n",
    "    best_ind_q_model = None; best_n = \"N/A\"; best_s = -1.0\n",
    "    if sorted_models:\n",
    "        best_n, best_m, best_s = sorted_models[0]\n",
    "        best_ind_q_model = best_m\n",
    "        logger.info(f\"Best individual qualified model identified: {best_n} (CV Score: {best_s:.5f})\")\n",
    "\n",
    "    # Save summary (remains the same logic)\n",
    "    try:\n",
    "        summary_path = f'results/ensemble_creation_summary_{timestamp}.txt'\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(\"Ensemble Summary\\n=============\\nQualified Models:\\n\")\n",
    "            for n, _, s in sorted_models: f.write(f\"- {n}: CV {s:.5f} {'(Excl.)' if n in keras_models_excluded else '(Incl.)'}\\n\")\n",
    "            f.write(f\"\\nEnsembles ({len(est_ens_to_use)} models):\\n\") # Reflect models used\n",
    "            vote_t = 'Soft' if vote_clf and vote_clf.voting == 'soft' else ('Hard' if vote_clf and vote_clf.voting == 'hard' else 'N/A')\n",
    "            f.write(f\"- Voting ({vote_t}): {'Saved' if vote_clf else 'Failed/Skipped'}.\\n\")\n",
    "            meta_name = meta_learner.__class__.__name__ if meta_learner and stack_clf else 'N/A'\n",
    "            f.write(f\"- Stacking (Meta:{meta_name}): {'Saved' if stack_clf else 'Failed/Skipped'}.\\n\")\n",
    "            if keras_models_excluded: f.write(f\"\\nKeras Excluded: {', '.join(keras_models_excluded)}\\n\")\n",
    "            if best_ind_q_model: f.write(f\"\\nBest individual model overall (from qualified list): {best_n}\\n\")\n",
    "        logger.info(f\"Saved ensemble summary: {summary_path}\")\n",
    "    except Exception as file_e: logger.warning(f\"Could not save ensemble summary: {file_e}\")\n",
    "\n",
    "    return vote_clf, stack_clf, best_ind_q_model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6c12018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scikeras.wrappers import KerasClassifier # Keep for isinstance check\n",
    "import joblib # Keep if fallback LE loading is needed\n",
    "import os # For checking plot/results dirs\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def evaluate_model(model, X_eval, y_eval, model_name, timestamp, le):\n",
    "    \"\"\"Evaluates a trained model on a given dataset (e.g., validation or hold-out).\"\"\"\n",
    "    if model is None:\n",
    "        logger.warning(f\"Skip eval {model_name}: model None.\")\n",
    "        return None, None # Return None for accuracy and report dictionary\n",
    "\n",
    "    # Handle LabelEncoder loading if missing\n",
    "    if le is None:\n",
    "        logger.warning(f\"Skip eval {model_name}: LabelEncoder (le) is None. Attempting fallback.\")\n",
    "        try:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "            if encoder_files:\n",
    "                le = joblib.load(f'features/{encoder_files[-1]}')\n",
    "                logger.info(f\"Loaded fallback LE for evaluation: {encoder_files[-1]}\")\n",
    "            else:\n",
    "                logger.error(\"Evaluation cannot proceed without LabelEncoder.\")\n",
    "                return None, None\n",
    "        except Exception as load_err:\n",
    "             logger.error(f\"Evaluation failed: Could not load fallback LE: {load_err}\")\n",
    "             return None, None\n",
    "\n",
    "    logger.info(f\"Evaluating model '{model_name}'...\")\n",
    "    # Ensure y_eval is a numpy array for consistency\n",
    "    if isinstance(y_eval, pd.Series):\n",
    "        y_eval = y_eval.values\n",
    "\n",
    "    is_keras_wrapper = isinstance(model, KerasClassifier)\n",
    "\n",
    "    try:\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_eval)\n",
    "        \n",
    "        # --- ADDED: Handle CatBoost prediction type ---\n",
    "        # Check if the model is CatBoost AND predictions are not integers\n",
    "        is_catboost = 'CatBoostClassifier' in str(type(model)) # Simple check\n",
    "        if is_catboost and not np.issubdtype(y_pred.dtype, np.integer):\n",
    "            logger.warning(f\"CatBoost predictions seem non-integer ({y_pred.dtype}): {y_pred[:5]}. Attempting mapping to encoded labels.\")\n",
    "            # Create a mapping from original string labels -> encoded integers\n",
    "            label_to_encoded = {label: i for i, label in enumerate(le.classes_)}\n",
    "            try:\n",
    "                # Attempt direct mapping (if CatBoost predicts original labels like 'low')\n",
    "                y_pred_mapped = np.array([label_to_encoded.get(str(p), -1) for p in y_pred.flatten()])\n",
    "                if np.any(y_pred_mapped == -1): # Check if direct mapping failed\n",
    "                    logger.warning(\"Direct label mapping failed, trying string-to-int mapping (assuming '0', '1', '2').\")\n",
    "                    # Attempt string-to-int mapping (if CatBoost predicts '0', '1', '2')\n",
    "                    y_pred_mapped = np.array([int(p) for p in y_pred.flatten()])\n",
    "                y_pred = y_pred_mapped # Use the mapped integer predictions\n",
    "                logger.info(f\"Successfully mapped CatBoost predictions to integers: {y_pred[:5]}\")\n",
    "            except Exception as map_err:\n",
    "                logger.error(f\"Failed to map CatBoost predictions to integers: {map_err}. Evaluation/Prediction may fail.\")\n",
    "                # Let the original y_pred pass through, error will likely occur later\n",
    "        # --- END CatBoost Handling ---\n",
    "\n",
    "        # Ensure predictions are class indices (for Keras primarily, now redundant for CatBoost if mapped)\n",
    "        if isinstance(model, KerasClassifier) and y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "            # ... (argmax logic) ...\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        elif not np.issubdtype(y_pred.dtype, np.integer):\n",
    "             logger.error(f\"Predictions for {model_name} are not integers after processing: {y_pred.dtype}. Evaluation may fail.\")\n",
    "             # Optionally try to force conversion, but it's risky\n",
    "             # y_pred = y_pred.astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_eval, y_pred)\n",
    "\n",
    "        # Generate classification report and confusion matrix\n",
    "        try:\n",
    "            y_eval_labels = le.inverse_transform(y_eval)\n",
    "            y_pred_labels = le.inverse_transform(y_pred)\n",
    "            target_names = le.classes_ # Get class names in correct order\n",
    "        except Exception as le_error:\n",
    "            logger.warning(f\"LabelEncoder inverse_transform failed for '{model_name}': {le_error}. Using numeric labels for report.\")\n",
    "            y_eval_labels = y_eval\n",
    "            y_pred_labels = y_pred\n",
    "            target_names = [str(i) for i in sorted(np.unique(y_eval))]\n",
    "\n",
    "        # Classification Report\n",
    "        report_str = classification_report(y_eval_labels, y_pred_labels, target_names=target_names, zero_division=0)\n",
    "        report_dict = classification_report(y_eval_labels, y_pred_labels, target_names=target_names, output_dict=True, zero_division=0)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_eval_labels, y_pred_labels, labels=target_names)\n",
    "\n",
    "        logger.info(f\"Evaluation Results for '{model_name}':\")\n",
    "        logger.info(f\"  Accuracy: {accuracy:.5f}\")\n",
    "\n",
    "        # Ensure results directory exists\n",
    "        results_dir = 'results'\n",
    "        if not os.path.exists(results_dir): os.makedirs(results_dir)\n",
    "\n",
    "        # Save evaluation results to a file\n",
    "        eval_filename = os.path.join(results_dir, f'{model_name}_evaluation_{timestamp}.txt')\n",
    "        with open(eval_filename, 'w') as f:\n",
    "            f.write(f\"Model Evaluation Summary\\n=========================\\n\")\n",
    "            f.write(f\"Model Name: {model_name}\\nTimestamp: {timestamp}\\nAccuracy: {accuracy:.5f}\\n\\n\")\n",
    "            f.write(\"Classification Report:\\n\"); f.write(report_str)\n",
    "            f.write(\"\\n\\nConfusion Matrix:\\n\"); f.write(np.array2string(cm))\n",
    "        logger.info(f\"Saved evaluation summary: {eval_filename}\")\n",
    "\n",
    "        # Ensure plots directory exists\n",
    "        plot_dir = 'plots'\n",
    "        if not os.path.exists(plot_dir): os.makedirs(plot_dir)\n",
    "\n",
    "        # Save confusion matrix plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "        plt.xlabel('Predicted Label'); plt.ylabel('True Label')\n",
    "        plt.title(f'Confusion Matrix - {model_name} (Accuracy: {accuracy:.3f})')\n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(plot_dir, f'{model_name}_confusion_matrix_{timestamp}.png')\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close() # Close the plot\n",
    "        logger.info(f\"Saved confusion matrix plot: {plot_filename}\")\n",
    "\n",
    "        return accuracy, report_dict\n",
    "\n",
    "    except AttributeError as ae:\n",
    "         if 'predict' in str(ae): logger.error(f\"Evaluation error '{model_name}': Model not fitted? AttrErr: {ae}\", exc_info=True)\n",
    "         else: logger.error(f\"AttributeError during eval '{model_name}': {ae}\", exc_info=True)\n",
    "         return None, None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during eval '{model_name}': {e}\", exc_info=True)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab564437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import joblib\n",
    "import os\n",
    "from scikeras.wrappers import KerasClassifier # Keep for isinstance check\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "def make_test_predictions(model, X_test, test_obs, timestamp, model_name, le):\n",
    "    \"\"\"Generates predictions on the test set and saves results.\"\"\"\n",
    "    logger.info(f\"Generating test predictions using {model_name}...\")\n",
    "\n",
    "    if model is None:\n",
    "        logger.error(f\"Predict fail {model_name}: model None.\")\n",
    "        return None\n",
    "\n",
    "    # Handle missing LabelEncoder (with fallback)\n",
    "    if le is None:\n",
    "        logger.warning(f\"LE None for {model_name}. Try fallback...\")\n",
    "        try:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if f.startswith('label_encoder_')])\n",
    "            if encoder_files:\n",
    "                le=joblib.load(f'features/{encoder_files[-1]}')\n",
    "                logger.info(f\"Loaded fallback LE: {encoder_files[-1]}.\")\n",
    "            else:\n",
    "                logger.error(f\"Predict fail: LE None, no fallback.\")\n",
    "                return None\n",
    "        except Exception as load_e:\n",
    "            logger.error(f\"LE None, fallback load fail:{load_e}. No predict.\")\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Make predictions (potentially encoded integers or strings from CatBoost)\n",
    "        y_pred_raw = model.predict(X_test)\n",
    "\n",
    "        # --- ADDED: Handle CatBoost prediction type ---\n",
    "        y_pred_enc = y_pred_raw # Default assumption\n",
    "        is_catboost = 'CatBoostClassifier' in str(type(model))\n",
    "        if is_catboost and not np.issubdtype(y_pred_raw.dtype, np.integer):\n",
    "            logger.warning(f\"CatBoost test predictions non-integer ({y_pred_raw.dtype}): {y_pred_raw[:5]}. Attempting mapping.\")\n",
    "            label_to_encoded = {label: i for i, label in enumerate(le.classes_)}\n",
    "            try:\n",
    "                y_pred_mapped = np.array([label_to_encoded.get(str(p), -1) for p in y_pred_raw.flatten()])\n",
    "                if np.any(y_pred_mapped == -1):\n",
    "                    logger.warning(\"Direct label mapping failed, trying string-to-int mapping.\")\n",
    "                    y_pred_mapped = np.array([int(p) for p in y_pred_raw.flatten()])\n",
    "                y_pred_enc = y_pred_mapped # Use mapped integer predictions\n",
    "                logger.info(f\"Mapped CatBoost test predictions to integers: {y_pred_enc[:5]}\")\n",
    "            except Exception as map_err:\n",
    "                logger.error(f\"Failed map CatBoost test predictions: {map_err}. Prediction may fail.\")\n",
    "                # Use original predictions; inverse_transform will likely fail\n",
    "                y_pred_enc = y_pred_raw\n",
    "        # --- END CatBoost Handling ---\n",
    "\n",
    "        # Ensure predictions are class indices (for Keras)\n",
    "        if isinstance(model, KerasClassifier) and y_pred_enc.ndim > 1 and y_pred_enc.shape[1] > 1:\n",
    "            y_pred_enc = np.argmax(y_pred_enc, axis=1)\n",
    "        elif not np.issubdtype(y_pred_enc.dtype, np.integer):\n",
    "             logger.error(f\"Test predictions for {model_name} not integers after processing: {y_pred_enc.dtype}. Inverse transform likely to fail.\")\n",
    "             # Cannot proceed reliably if predictions aren't integer encoded labels here\n",
    "\n",
    "        # Inverse transform to get original salary category labels\n",
    "        predicted_labels = le.inverse_transform(y_pred_enc) # This now expects integer y_pred_enc\n",
    "\n",
    "\n",
    "        # Create submission DataFrame\n",
    "        submission_df = pd.DataFrame({'obs': test_obs, 'salary_category': predicted_labels})\n",
    "\n",
    "        # Sanitize model name for filename\n",
    "        safe_model_name = model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\").replace(\" \", \"_\")\n",
    "\n",
    "        # Ensure directories exist\n",
    "        submission_dir = 'submissions'\n",
    "        results_dir = 'results'\n",
    "        if not os.path.exists(submission_dir): os.makedirs(submission_dir)\n",
    "        if not os.path.exists(results_dir): os.makedirs(results_dir)\n",
    "\n",
    "        submission_path = os.path.join(submission_dir, f'solution_{safe_model_name}_{timestamp}.csv')\n",
    "\n",
    "        # Save submission file\n",
    "        submission_df.to_csv(submission_path, index=False)\n",
    "        logger.info(f\"Saved submission: {submission_path}\")\n",
    "\n",
    "        # Log value counts of predictions for analysis\n",
    "        pred_value_counts = submission_df['salary_category'].value_counts().to_dict()\n",
    "        logger.info(f\"Test prediction distribution for '{model_name}': {pred_value_counts}\")\n",
    "\n",
    "        # Save prediction summary\n",
    "        summary_filename = os.path.join(results_dir, f'{safe_model_name}_test_prediction_summary_{timestamp}.txt')\n",
    "        with open(summary_filename, 'w') as f:\n",
    "            f.write(f\"Test Prediction Summary\\n======================\\n\")\n",
    "            f.write(f\"Model Name: {model_name}\\n\")\n",
    "            try: f.write(f\"Model Class: {model.__class__.__name__}\\n\")\n",
    "            except: f.write(f\"Model Class: N/A\\n\")\n",
    "            f.write(f\"Timestamp: {timestamp}\\nTotal Predictions: {len(predicted_labels)}\\n\\nDistribution:\\n\")\n",
    "            total_preds = len(predicted_labels)\n",
    "            if total_preds > 0:\n",
    "                for label, count in sorted(pred_value_counts.items()):\n",
    "                    percentage = (count / total_preds) * 100\n",
    "                    f.write(f\"- {label}: {count} ({percentage:.2f}%)\\n\")\n",
    "            else:\n",
    "                f.write(\"- No predictions were generated.\\n\")\n",
    "        logger.info(f\"Saved test prediction summary: {summary_filename}\")\n",
    "\n",
    "        return submission_df\n",
    "\n",
    "    except AttributeError as ae:\n",
    "        logger.error(f\"AttributeError during predict/inverse_transform '{model_name}': {ae}.\", exc_info=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during test prediction '{model_name}': {e}\", exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc317202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier # Keep for FS\n",
    "# Imports for models used within the pipeline are assumed to be at the top of the file\n",
    "# Imports for utility functions are assumed to be at the top of the file\n",
    "import os\n",
    "\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "# Assume utility functions (create_directory_structure, get_timestamp) are defined\n",
    "# Assume preprocessing function (preprocess_data) is defined\n",
    "# Assume model optimization function (optimize_model) is defined\n",
    "# Assume ensemble function (create_ensemble) is defined\n",
    "# Assume evaluation function (evaluate_model) is defined\n",
    "# Assume prediction function (make_test_predictions) is defined\n",
    "\n",
    "def run_complete_pipeline(perform_feature_selection=False, min_cv_score_threshold=0.72, fs_threshold='mean', n_jobs_sklearn=1):\n",
    "    \"\"\"\n",
    "    Run the complete model training pipeline with combined FE logic,\n",
    "    updated model list, and class weight balancing strategy integrated\n",
    "    into optimize_model.\n",
    "    \"\"\"\n",
    "    timestamp = get_timestamp()\n",
    "    main_log_file = None\n",
    "    file_handler = None\n",
    "    pipeline_success = False # Track overall success\n",
    "\n",
    "    try:\n",
    "        # 1. Setup\n",
    "        print(\"--- Starting Complete Pipeline Run (Class Weights, LGBM added) ---\")\n",
    "        create_directory_structure()\n",
    "        main_log_file = f'logs/pipeline_run_{timestamp}.log'\n",
    "        file_handler = logging.FileHandler(main_log_file)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        if file_handler.baseFilename not in [h.baseFilename for h in logger.handlers if isinstance(h, logging.FileHandler)]:\n",
    "             logger.addHandler(file_handler)\n",
    "\n",
    "        logger.info(f\"--- Starting Complete Pipeline Run --- Timestamp: {timestamp} ---\")\n",
    "        logger.info(f\"Pipeline Config: Combined FE, Scaling=True, FeatSelect={perform_feature_selection} (Thresh={fs_threshold}), CV Thresh={min_cv_score_threshold}, n_jobs={n_jobs_sklearn} used, Class Weights Enabled, Const Cols Kept\")\n",
    "        logger.info(f\"Logging detailed output to: {main_log_file}\")\n",
    "\n",
    "        # 2. Load Data\n",
    "        logger.info(\"Loading data...\")\n",
    "        try: train_df = pd.read_csv('train.csv'); test_df = pd.read_csv('test.csv'); logger.info(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "        except FileNotFoundError as e: logger.error(f\"Data load error: {e}.\"); raise\n",
    "        if 'salary_category' not in train_df.columns: logger.error(\"Target missing.\"); raise ValueError(\"Missing target\")\n",
    "        if train_df.empty or test_df.empty: logger.error(\"Data empty.\"); raise ValueError(\"Empty data\")\n",
    "        if 'obs' not in test_df.columns: logger.error(\"Column 'obs' missing in test.csv.\"); raise ValueError(\"Missing obs\")\n",
    "        all_states = set(train_df['job_state'].dropna().unique()).union(set(test_df['job_state'].dropna().unique()))\n",
    "        all_feature1 = set(train_df['feature_1'].dropna().unique()).union(set(test_df['feature_1'].dropna().unique()))\n",
    "        logger.info(f\"Found {len(all_states)} unique states and {len(all_feature1)} unique feature_1 values.\")\n",
    "\n",
    "        # 3. Preprocess Training Data\n",
    "        logger.info(\"Preprocessing training data (using combined logic)...\")\n",
    "        X_train_orig, y_train_orig, feature_cols_initial, label_encoder = preprocess_data(train_df, all_states, all_feature1, timestamp, is_training=True)\n",
    "        if X_train_orig is None or y_train_orig is None or label_encoder is None or feature_cols_initial is None: logger.error(\"Train preprocess failed.\"); raise RuntimeError(\"Preprocessing train failed\")\n",
    "        logger.info(f\"Train preprocess done. Initial Feats: {X_train_orig.shape[1]}\"); y_train_orig = pd.Series(y_train_orig)\n",
    "\n",
    "        # 4. Train/Validation Split\n",
    "        logger.info(\"Splitting data (80/20)...\")\n",
    "        X_train_full, X_val, y_train_full, y_val = train_test_split(X_train_orig, y_train_orig, test_size=0.20, random_state=42, stratify=y_train_orig)\n",
    "        logger.info(f\"Train (Pre-scale): {X_train_full.shape}, Val (Pre-scale): {X_val.shape}\"); y_train_full = pd.Series(y_train_full, index=X_train_full.index); y_val = pd.Series(y_val, index=X_val.index)\n",
    "\n",
    "        # 5. SCALING STEP\n",
    "        logger.info(\"Applying StandardScaler...\")\n",
    "        scaler = StandardScaler(); X_train_full_scaled = scaler.fit_transform(X_train_full[feature_cols_initial]); X_val_scaled = scaler.transform(X_val[feature_cols_initial])\n",
    "        X_train_full_scaled = pd.DataFrame(X_train_full_scaled, index=X_train_full.index, columns=feature_cols_initial)\n",
    "        X_val_scaled = pd.DataFrame(X_val_scaled, index=X_val.index, columns=feature_cols_initial)\n",
    "        scaler_path = f'scalers/scaler_{timestamp}.joblib'; joblib.dump(scaler, scaler_path); logger.info(f\"Scaler saved: {scaler_path}\")\n",
    "\n",
    "        # 6. Preprocess & Scale Test Data\n",
    "        logger.info(\"Preprocessing test data (using combined logic)...\")\n",
    "        X_test_orig, _, _, _ = preprocess_data(test_df, all_states, all_feature1, timestamp, is_training=False, feature_columns_to_use=feature_cols_initial)\n",
    "        if X_test_orig is None: logger.error(\"Test preprocess failed.\"); raise RuntimeError(\"Preprocessing test failed\")\n",
    "        try: X_test_aligned = X_test_orig[feature_cols_initial]; logger.info(\"Test columns aligned.\")\n",
    "        except KeyError as ke: logger.error(f\"Test col mismatch after preprocess: {ke}.\"); raise RuntimeError(f\"Test column mismatch: {ke}\")\n",
    "        logger.info(\"Scaling test data...\"); X_test_scaled = scaler.transform(X_test_aligned)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test_aligned.index, columns=feature_cols_initial)\n",
    "        logger.info(f\"Test preprocess & scale done. Shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        # Define Data Partitions\n",
    "        X_opt_train = X_train_full_scaled.copy(); y_opt_train = y_train_full.copy(); X_holdout_val = X_val_scaled.copy(); y_holdout_val = y_val.copy(); X_final_test = X_test_scaled.copy()\n",
    "        current_feature_cols = list(feature_cols_initial)\n",
    "\n",
    "        # 7. Optional Feature Selection\n",
    "        if perform_feature_selection:\n",
    "            logger.info(f\"Performing feature selection (Threshold: {fs_threshold})...\")\n",
    "            try:\n",
    "                selector_model = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=n_jobs_sklearn, class_weight='balanced', max_depth=20)\n",
    "                logger.info(\"Fitting RF for feature selection...\"); selector_model.fit(X_opt_train, y_opt_train)\n",
    "                selector = SelectFromModel(selector_model, threshold=fs_threshold, prefit=True)\n",
    "                selected_mask = selector.get_support(); selected_features = X_opt_train.columns[selected_mask]\n",
    "                num_orig = X_opt_train.shape[1]; num_sel = len(selected_features)\n",
    "                if num_sel == 0: logger.error(\"FS removed ALL features!\"); raise RuntimeError(\"FS removed all features.\")\n",
    "                elif num_sel < num_orig:\n",
    "                    num_removed = num_orig - num_sel; logger.info(f\"Feat selection removed {num_removed} features. Selected {num_sel}.\")\n",
    "                    current_feature_cols = list(selected_features)\n",
    "                    X_opt_train = X_opt_train[current_feature_cols]; X_holdout_val = X_holdout_val[current_feature_cols]; X_final_test = X_final_test[current_feature_cols]\n",
    "                    logger.info(f\"Selection applied to train/val/test partitions.\")\n",
    "                    joblib.dump(current_feature_cols, f'features/selected_feature_columns_{timestamp}.joblib')\n",
    "                else: logger.info(f\"Feature selection removed no features with threshold '{fs_threshold}'.\"); perform_feature_selection = False\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error feature selection: {e}. Use all scaled.\", exc_info=True); perform_feature_selection = False\n",
    "                current_feature_cols = list(feature_cols_initial); X_opt_train=X_train_full_scaled[current_feature_cols]; X_holdout_val=X_val_scaled[current_feature_cols]; X_final_test=X_test_scaled[current_feature_cols]\n",
    "        else:\n",
    "            logger.info(\"Skipping feature selection.\")\n",
    "            X_opt_train = X_opt_train[current_feature_cols]; X_holdout_val = X_holdout_val[current_feature_cols]; X_final_test = X_final_test[current_feature_cols]\n",
    "\n",
    "        logger.info(f\"Data shapes post-scaling/selection: Train={X_opt_train.shape}, Val={X_holdout_val.shape}, Test={X_final_test.shape}\")\n",
    "        logger.info(f\"Number of features used in modeling: {len(current_feature_cols)}\")\n",
    "\n",
    "        # 8. Optimize, Train Base Models & Make Individual Predictions\n",
    "        # --- UPDATED LIST (No SVC, Added LightGBM) ---\n",
    "        models_to_optimize = [\n",
    "            ('adaboost', 200),\n",
    "            ('randomforest', 200), ('extratrees', 500),\n",
    "            ('gradientboosting', 200), \n",
    "            ('catboost', 300), ('xgboost', 500), ('lightgbm', 500)\n",
    "        ]\n",
    "        qualified_models_with_scores = [] # Stores (name, fitted_model, cv_score)\n",
    "        optimized_params_all = {} # Stores best params found by Optuna\n",
    "\n",
    "        logger.info(f\"--- Optimizing models (Class Weights Enabled Where Applicable) ---\")\n",
    "        logger.info(f\"Minimum CV score threshold for qualification: {min_cv_score_threshold}\")\n",
    "        logger.info(f\"Optimization Order: {[m[0] for m in models_to_optimize]}\")\n",
    "\n",
    "        for model_name, n_trials in models_to_optimize:\n",
    "            indiv_sub_df = None\n",
    "            try:\n",
    "                logger.info(f\"--- Optimizing {model_name} ({n_trials} trials) ---\")\n",
    "                # Optimize_model now handles final training with class_weight where applicable\n",
    "                final_model, best_cv_score, best_params = optimize_model(\n",
    "                    X_opt_train, y_opt_train, timestamp, model_name,\n",
    "                    n_trials=n_trials, n_jobs_optuna=n_jobs_sklearn\n",
    "                )\n",
    "\n",
    "                # Log details for qualification debugging\n",
    "                log_score = best_cv_score if best_cv_score is not None else -1.0\n",
    "                comparison_result = best_cv_score >= min_cv_score_threshold if best_cv_score is not None else False\n",
    "                logger.info(f\"Qualification Check for {model_name}: \"\n",
    "                            f\"final_model exists? {final_model is not None}, \"\n",
    "                            f\"best_cv_score={log_score:.7f}, \"\n",
    "                            f\"threshold={min_cv_score_threshold}, \"\n",
    "                            f\"comparison result: {comparison_result}\")\n",
    "\n",
    "                # Qualification Check\n",
    "                if final_model is not None and best_cv_score is not None and best_cv_score >= min_cv_score_threshold:\n",
    "                    logger.info(f\"+++ QUALIFIED: {model_name} (CV Score: {best_cv_score:.5f})\")\n",
    "                    # Evaluate on Holdout\n",
    "                    holdout_acc, _ = evaluate_model(final_model, X_holdout_val, y_holdout_val, f\"{model_name}_qualified_holdout_eval\", timestamp, label_encoder)\n",
    "                    if holdout_acc is not None:\n",
    "                        logger.info(f\"Hold-out Acc ({model_name}): {holdout_acc:.5f}\")\n",
    "                        # Store model with HOLD-OUT score for potential ensemble weighting later\n",
    "                        qualified_models_with_scores.append((model_name, final_model, holdout_acc)) # Store HOLD-OUT acc\n",
    "                    else:\n",
    "                        logger.warning(f\"Hold-out Eval failed for {model_name}. Cannot use its score for weighting.\")\n",
    "                        # Still add model? Maybe add with CV score as fallback weight? Or exclude?\n",
    "                        # Let's add with CV score for now, but this case needs consideration.\n",
    "                        qualified_models_with_scores.append((model_name, final_model, best_cv_score))\n",
    "\n",
    "                    if best_params: optimized_params_all[model_name] = best_params\n",
    "\n",
    "                    # Generate Individual Predictions\n",
    "                    logger.info(f\"--- Generating individual predictions for {model_name} ---\")\n",
    "                    indiv_sub_df = make_test_predictions(final_model, X_final_test, test_df['obs'], timestamp, f\"{model_name}_qual_individual_pred\", label_encoder)\n",
    "                    if indiv_sub_df is None: logger.error(f\"Failed individual predictions for {model_name}.\")\n",
    "                    else: logger.info(f\"Individual prediction file saved for {model_name}.\")\n",
    "\n",
    "                elif best_cv_score is not None: # Didn't meet threshold or final fit failed\n",
    "                     logger.info(f\"--- NOT QUALIFIED: {model_name} (CV Score: {best_cv_score:.5f} {' - Final model fit/save failed' if final_model is None else ''}) ---\")\n",
    "                     if best_params: optimized_params_all[model_name] = best_params # Still save params\n",
    "                else: # Optuna itself failed or returned None score\n",
    "                    logger.warning(f\"Optimization failed or returned invalid score for {model_name}. Skip.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in main optimization loop for {model_name}: {e}\", exc_info=True)\n",
    "\n",
    "        # --- Post-Optimization Check ---\n",
    "        logger.info(\"--- Model Optimization Phase Complete ---\")\n",
    "        if not qualified_models_with_scores:\n",
    "            logger.error(f\"CRITICAL: NO models met CV threshold {min_cv_score_threshold}. Aborting.\")\n",
    "            if file_handler: logger.removeHandler(file_handler); file_handler.close()\n",
    "            return False\n",
    "        logger.info(f\"--- {len(qualified_models_with_scores)} models qualified. ---\")\n",
    "        # Log based on HOLD-OUT score now stored\n",
    "        logger.info(f\"Qualified Models (Name, Holdout Acc/CV Score): {[(m[0], f'{m[2]:.5f}') for m in qualified_models_with_scores]}\")\n",
    "\n",
    "        # 9. Create Ensembles & Select FINAL Best Model\n",
    "        final_model = None; final_model_name = \"N/A\"; vote_ens = None; stack_ens = None; best_ind_q_model = None\n",
    "\n",
    "        if len(qualified_models_with_scores) == 1:\n",
    "            # Use holdout score here too\n",
    "            final_model_name, final_model, final_holdout_score = qualified_models_with_scores[0]\n",
    "            logger.warning(f\"Only 1 qualified: {final_model_name} (Holdout Acc: {final_holdout_score:.5f}). Select it.\")\n",
    "            best_ind_q_model = final_model\n",
    "        elif len(qualified_models_with_scores) > 1:\n",
    "            logger.info(f\"--- Creating and Evaluating Ensembles (using balanced models if applicable) ---\")\n",
    "            # Pass potentially balanced models to create_ensemble\n",
    "            # create_ensemble now uses LGBM meta-learner by default\n",
    "            vote_ens, stack_ens, best_ind_q_model_obj = create_ensemble(\n",
    "                qualified_models_with_scores, X_opt_train, y_opt_train, timestamp, n_jobs_ensemble=n_jobs_sklearn\n",
    "            )\n",
    "\n",
    "            # Evaluate candidate ensembles and best individual on HOLD-OUT set\n",
    "            logger.info(\"--- Evaluating candidate final models on HOLD-OUT validation set ---\")\n",
    "            candidates = {} # Store {name: (holdout_accuracy, model_object)}\n",
    "            best_ind_name_from_ensemble = None # Track name from ensemble function\n",
    "\n",
    "            if vote_ens:\n",
    "                vote_model_name = f\"voting_ensemble_{vote_ens.voting}_qualified\"\n",
    "                logger.info(f\"--- Eval {vote_model_name} ---\")\n",
    "                val_acc, _ = evaluate_model(vote_ens, X_holdout_val, y_holdout_val, f\"{vote_model_name}_holdout_eval\", timestamp, label_encoder)\n",
    "                if val_acc is not None: candidates[vote_model_name] = (val_acc, vote_ens); logger.info(f\"Hold-out Acc ({vote_model_name}): {val_acc:.5f}\")\n",
    "                else: logger.warning(f\"Eval fail: {vote_model_name}\")\n",
    "\n",
    "            if stack_ens:\n",
    "                stack_meta_name = stack_ens.final_estimator_.__class__.__name__\n",
    "                stack_model_name = f\"stacking_ensemble_{stack_meta_name}_qualified\"\n",
    "                logger.info(f\"--- Eval {stack_model_name} ---\")\n",
    "                val_acc, _ = evaluate_model(stack_ens, X_holdout_val, y_holdout_val, f\"{stack_model_name}_holdout_eval\", timestamp, label_encoder)\n",
    "                if val_acc is not None: candidates[stack_model_name] = (val_acc, stack_ens); logger.info(f\"Hold-out Acc ({stack_model_name}): {val_acc:.5f}\")\n",
    "                else: logger.warning(f\"Eval fail: {stack_model_name}\")\n",
    "\n",
    "            # Find the best individual model's info again to evaluate it\n",
    "            if qualified_models_with_scores:\n",
    "                 # Sort by stored score (holdout acc or fallback CV score)\n",
    "                 best_ind_info = max(qualified_models_with_scores, key=lambda item: item[2])\n",
    "                 best_ind_name_from_list = best_ind_info[0]\n",
    "                 best_ind_q_model = best_ind_info[1] # The actual best model object\n",
    "                 best_ind_stored_score = best_ind_info[2]\n",
    "                 logger.info(f\"--- Eval Best Individual ({best_ind_name_from_list}, Stored Score: {best_ind_stored_score:.5f}) ---\")\n",
    "                 eval_name = f\"{best_ind_name_from_list}_best_qual_holdout_eval\"\n",
    "                 val_acc, _ = evaluate_model(best_ind_q_model, X_holdout_val, y_holdout_val, eval_name, timestamp, label_encoder)\n",
    "                 if val_acc is not None:\n",
    "                     cand_name = f\"{best_ind_name_from_list}_best_qualified\"\n",
    "                     candidates[cand_name] = (val_acc, best_ind_q_model)\n",
    "                     logger.info(f\"Hold-out Acc ({best_ind_name_from_list}): {val_acc:.5f}\")\n",
    "                 else: logger.warning(f\"Hold-out Eval failed for {best_ind_name_from_list}\")\n",
    "\n",
    "            # Select FINAL model based on highest HOLD-OUT accuracy among candidates\n",
    "            if candidates:\n",
    "                final_model_name = max(candidates, key=lambda k: candidates[k][0])\n",
    "                final_val_score, final_model = candidates[final_model_name]\n",
    "                logger.info(f\"--- FINAL MODEL SELECTION ---\")\n",
    "                logger.info(f\"Selected '{final_model_name}' as FINAL model (Hold-Out Acc: {final_val_score:.5f})\")\n",
    "            else:\n",
    "                logger.error(\"Hold-out evaluation failed for all candidates.\")\n",
    "                # Fallback: Use best individual based on stored score (holdout or CV)\n",
    "                if best_ind_q_model and best_ind_name_from_list:\n",
    "                    final_model = best_ind_q_model\n",
    "                    final_model_name = f\"{best_ind_name_from_list}_best_qualified_fallback\"\n",
    "                    logger.warning(f\"FALLBACK: Using best individual model '{final_model_name}' based on its stored score.\")\n",
    "                else:\n",
    "                    logger.error(\"Could not determine final model even as fallback. Aborting.\")\n",
    "                    raise RuntimeError(\"Final model selection failed.\")\n",
    "\n",
    "        # Check if a final model was successfully selected\n",
    "        if not final_model:\n",
    "            logger.error(\"No final model could be selected. Aborting.\")\n",
    "            raise RuntimeError(\"Final model selection failed.\")\n",
    "\n",
    "        # 10. Make FINAL Test Predictions\n",
    "        logger.info(f\"--- Generating FINAL predictions using: {final_model_name} ---\")\n",
    "        final_sub_df = make_test_predictions(final_model, X_final_test, test_df['obs'], timestamp, f\"{final_model_name}_FINAL\", label_encoder)\n",
    "        if final_sub_df is None:\n",
    "            logger.error(f\"Failed FINAL submission with {final_model_name}.\")\n",
    "            # Consider if pipeline should fail here\n",
    "            pipeline_success = False # Mark as failed if submission fails\n",
    "        else:\n",
    "            logger.info(f\"FINAL submission file generated with {final_model_name}.\")\n",
    "            pipeline_success = True # Mark successful only if prediction works\n",
    "\n",
    "        # 11. Final Summary\n",
    "        logger.info(\"--- Pipeline Run Summary ---\")\n",
    "        logger.info(f\"Timestamp: {timestamp}\")\n",
    "        logger.info(f\"Config: Combined FE, Scaling=True, FeatSelect={perform_feature_selection} (Thresh={fs_threshold}), CV Thresh={min_cv_score_threshold}, n_jobs={n_jobs_sklearn}, Class Weights Enabled, Const Cols Kept\")\n",
    "        logger.info(f\"Final # Features: {len(current_feature_cols)}\")\n",
    "        logger.info(\"Models Optimized: \" + \", \".join([m[0] for m in models_to_optimize]))\n",
    "        qual_details = [(m[0], f\"{m[2]:.5f}\") for m in qualified_models_with_scores] if qualified_models_with_scores else [\"None\"]\n",
    "        logger.info(\"Models Qualified (Name, Holdout Acc/CV Score): \" + \", \".join([f\"{n}({s})\" for n, s in qual_details]))\n",
    "        logger.info(f\"Ensembles Created: Voting={'Yes' if vote_ens else 'No'}, Stacking={'Yes' if stack_ens else 'No'} (Meta: {stack_ens.final_estimator_.__class__.__name__ if stack_ens else 'N/A'})\")\n",
    "        logger.info(f\"Final model selected: {final_model_name}\")\n",
    "        logger.info(\"Individual predictions saved for qualified models.\")\n",
    "        if final_sub_df is not None:\n",
    "            safe_final_n = final_model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\").replace(\" \", \"_\")\n",
    "            final_sub_path = f\"submissions/solution_{safe_final_n}_FINAL_{timestamp}.csv\"\n",
    "            logger.info(f\"Final submission file: {final_sub_path}\")\n",
    "        else:\n",
    "            logger.warning(\"No FINAL submission file was generated.\")\n",
    "        logger.info(f\"Logs in: {main_log_file}\")\n",
    "        logger.info(f\"--- Pipeline {'Completed Successfully' if pipeline_success else 'Completed with Errors'} ---\")\n",
    "\n",
    "        # Close log handler\n",
    "        if file_handler: logger.removeHandler(file_handler); file_handler.close()\n",
    "        return pipeline_success\n",
    "\n",
    "    # --- Main Exception Handling ---\n",
    "    except Exception as e:\n",
    "        logger.error(f\"--- Pipeline Failed Critically --- Error Type: {type(e).__name__} ---\")\n",
    "        logger.error(f\"Error Message: {e}\", exc_info=True)\n",
    "        # Ensure log handler is closed\n",
    "        if file_handler and file_handler in logger.handlers:\n",
    "            logger.removeHandler(file_handler)\n",
    "            file_handler.close()\n",
    "        return False # Indicate critical failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9d824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 01:13:40,669 - INFO - Creating directory structure...\n",
      "2025-04-26 01:13:40,669 - INFO - Created directory: models\n",
      "2025-04-26 01:13:40,669 - INFO - Created directory: features\n",
      "2025-04-26 01:13:40,669 - INFO - Created directory: results\n",
      "2025-04-26 01:13:40,669 - INFO - Created directory: submissions\n",
      "2025-04-26 01:13:40,669 - INFO - Created directory: logs\n",
      "2025-04-26 01:13:40,669 - INFO - Created directory: plots\n",
      "2025-04-26 01:13:40,669 - INFO - Created directory: optuna_trials\n",
      "2025-04-26 01:13:40,669 - INFO - Created directory: scalers\n",
      "2025-04-26 01:13:40,669 - INFO - Created directory: calibrated_models\n",
      "2025-04-26 01:13:40,669 - INFO - Directory structure verified/created.\n",
      "2025-04-26 01:13:40,681 - INFO - --- Starting Complete Pipeline Run --- Timestamp: 20250426_011340 ---\n",
      "2025-04-26 01:13:40,682 - INFO - Pipeline Config: Combined FE, Scaling=True, FeatSelect=True (Thresh=mean), CV Thresh=0.72, n_jobs=16 used, Class Weights Enabled, Const Cols Kept\n",
      "2025-04-26 01:13:40,682 - INFO - Logging detailed output to: logs/pipeline_run_20250426_011340.log\n",
      "2025-04-26 01:13:40,683 - INFO - Loading data...\n",
      "2025-04-26 01:13:40,748 - INFO - Train shape: (1280, 317), Test shape: (854, 316)\n",
      "2025-04-26 01:13:40,748 - INFO - Found 39 unique states and 5 unique feature_1 values.\n",
      "2025-04-26 01:13:40,748 - INFO - Preprocessing training data (using combined logic)...\n",
      "2025-04-26 01:13:40,765 - INFO - Starting preprocessing (Combined Logic). Is training: True\n",
      "2025-04-26 01:13:40,765 - INFO - Target 'salary_category' label encoded.\n",
      "2025-04-26 01:13:40,765 - INFO - Saved label encoder and mapping.\n",
      "2025-04-26 01:13:40,765 - INFO - Initial cleaning: Numerical and Boolean Features...\n",
      "2025-04-26 01:13:40,831 - WARNING - Col 'feature_10' has non-0/1 vals (834). Treat as numeric, impute median.\n",
      "2025-04-26 01:13:40,831 - INFO - Starting Feature Engineering...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Complete Pipeline Run (Class Weights, LGBM added) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 01:13:40,850 - INFO - Fit/saved TE for job_title_grouped\n",
      "2025-04-26 01:13:40,864 - INFO - Processed 'job_title'.\n",
      "2025-04-26 01:13:40,871 - INFO - Processed 'job_posted_date'.\n",
      "2025-04-26 01:13:40,875 - INFO - Added bin f9.\n",
      "2025-04-26 01:13:40,875 - INFO - Added: feature_2_9_interaction\n",
      "2025-04-26 01:13:40,875 - INFO - Added transforms f2 (sq, sqrt, bin).\n",
      "2025-04-26 01:13:40,881 - INFO - Added bool aggs.\n",
      "2025-04-26 01:13:40,882 - INFO - Added: feature_10_8_interaction\n",
      "2025-04-26 01:13:40,883 - INFO - Added new interactions: ['feat2_job_title_encoded', 'feat2_boolsum', 'feat2_recency', 'job_title_encoded_recency']\n",
      "2025-04-26 01:13:40,932 - INFO - Applying PCA (n=15) to job desc...\n",
      "2025-04-26 01:13:40,966 - INFO - Fit/saved PCA.\n",
      "2025-04-26 01:13:40,966 - INFO - Finished job desc features.\n",
      "2025-04-26 01:13:40,966 - INFO - Applying manual OHE for 'job_state' (39 unique).\n",
      "2025-04-26 01:13:40,984 - INFO - Applying manual OHE for 'feature_1' (5 unique).\n",
      "2025-04-26 01:13:40,984 - INFO - Final cleanup and column alignment...\n",
      "2025-04-26 01:13:41,000 - WARNING - Col 'is_senior' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-26 01:13:41,000 - WARNING - Col 'is_junior' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-26 01:13:41,000 - WARNING - Col 'is_developer' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-26 01:13:41,000 - WARNING - Col 'is_specialist' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'feature_9_bin' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_GA' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_MN' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_LA' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_NY' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - WARNING - Col 'state_WY' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_OK' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_SC' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_AZ' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_CT' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - WARNING - Col 'state_RI' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_MI' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_MA' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_OH' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_IA' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_NJ' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_SD' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_AL' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,000 - INFO - Eng col 'state_AR' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,014 - INFO - Eng col 'state_NC' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,015 - INFO - Eng col 'state_WA' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,015 - INFO - Eng col 'state_MO' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,016 - INFO - Eng col 'state_FL' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_IN' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_CA' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_KY' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_DC' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_TN' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_OR' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_VA' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - WARNING - Col 'state_KS' constant (nunique=1) in train. Engineered: True. Keeping col.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_TX' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_PA' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_NV' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_IL' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_AK' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_UT' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_NM' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,017 - INFO - Eng col 'state_CO' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,026 - INFO - Eng col 'state_MD' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,026 - INFO - Eng col 'feat1_C' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,026 - INFO - Eng col 'feat1_D' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,026 - INFO - Eng col 'feat1_B' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,028 - INFO - Eng col 'feat1_E' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,028 - INFO - Eng col 'feat1_A' low card (nunique=2) in train.\n",
      "2025-04-26 01:13:41,028 - INFO - Saved 100 feature names (const cols NOT dropped).\n",
      "2025-04-26 01:13:41,032 - INFO - Preprocessing train done. Shape: (1280, 100). Time: 0.27s\n",
      "2025-04-26 01:13:41,033 - INFO - Train preprocess done. Initial Feats: 100\n",
      "2025-04-26 01:13:41,033 - INFO - Splitting data (80/20)...\n",
      "2025-04-26 01:13:41,033 - INFO - Train (Pre-scale): (1024, 100), Val (Pre-scale): (256, 100)\n",
      "2025-04-26 01:13:41,033 - INFO - Applying StandardScaler...\n",
      "2025-04-26 01:13:41,049 - INFO - Scaler saved: scalers/scaler_20250426_011340.joblib\n",
      "2025-04-26 01:13:41,049 - INFO - Preprocessing test data (using combined logic)...\n",
      "2025-04-26 01:13:41,049 - INFO - Starting preprocessing (Combined Logic). Is training: False\n",
      "2025-04-26 01:13:41,049 - INFO - Loaded LE: label_encoder_20250426_011340.joblib\n",
      "2025-04-26 01:13:41,049 - INFO - Initial cleaning: Numerical and Boolean Features...\n",
      "2025-04-26 01:13:41,121 - WARNING - Col 'feature_10' has non-0/1 vals (540). Treat as numeric, impute median.\n",
      "2025-04-26 01:13:41,121 - INFO - Starting Feature Engineering...\n",
      "2025-04-26 01:13:41,149 - INFO - Loaded TE: features/job_title_encoder_20250426_011340.joblib\n",
      "2025-04-26 01:13:41,150 - INFO - Processed 'job_title'.\n",
      "2025-04-26 01:13:41,158 - INFO - Processed 'job_posted_date'.\n",
      "2025-04-26 01:13:41,160 - INFO - Added bin f9.\n",
      "2025-04-26 01:13:41,160 - INFO - Added: feature_2_9_interaction\n",
      "2025-04-26 01:13:41,165 - INFO - Added transforms f2 (sq, sqrt, bin).\n",
      "2025-04-26 01:13:41,167 - INFO - Added bool aggs.\n",
      "2025-04-26 01:13:41,167 - INFO - Added: feature_10_8_interaction\n",
      "2025-04-26 01:13:41,167 - INFO - Added new interactions: ['feat2_job_title_encoded', 'feat2_boolsum', 'feat2_recency', 'job_title_encoded_recency']\n",
      "2025-04-26 01:13:41,200 - INFO - Applying PCA (n=15) to job desc...\n",
      "2025-04-26 01:13:41,216 - INFO - Loaded PCA: features/job_desc_pca_20250426_011340.joblib\n",
      "2025-04-26 01:13:41,232 - INFO - Finished job desc features.\n",
      "2025-04-26 01:13:41,238 - INFO - Applying manual OHE for 'job_state' (39 unique).\n",
      "2025-04-26 01:13:41,247 - INFO - Applying manual OHE for 'feature_1' (5 unique).\n",
      "2025-04-26 01:13:41,249 - INFO - Final cleanup and column alignment...\n",
      "2025-04-26 01:13:41,266 - INFO - Preprocessing test done. Shape: (854, 100). Time: 0.22s\n",
      "2025-04-26 01:13:41,266 - INFO - Test columns aligned.\n",
      "2025-04-26 01:13:41,266 - INFO - Scaling test data...\n",
      "2025-04-26 01:13:41,266 - INFO - Test preprocess & scale done. Shape: (854, 100)\n",
      "2025-04-26 01:13:41,266 - INFO - Performing feature selection (Threshold: mean)...\n",
      "2025-04-26 01:13:41,266 - INFO - Fitting RF for feature selection...\n",
      "2025-04-26 01:13:41,479 - INFO - Feat selection removed 60 features. Selected 40.\n",
      "2025-04-26 01:13:41,481 - INFO - Selection applied to train/val/test partitions.\n",
      "2025-04-26 01:13:41,481 - INFO - Data shapes post-scaling/selection: Train=(1024, 40), Val=(256, 40), Test=(854, 40)\n",
      "2025-04-26 01:13:41,481 - INFO - Number of features used in modeling: 40\n",
      "2025-04-26 01:13:41,481 - INFO - --- Optimizing models (Class Weights Enabled Where Applicable) ---\n",
      "2025-04-26 01:13:41,481 - INFO - Minimum CV score threshold for qualification: 0.72\n",
      "2025-04-26 01:13:41,481 - INFO - Optimization Order: ['adaboost', 'randomforest', 'extratrees', 'gradientboosting', 'catboost', 'xgboost', 'lightgbm']\n",
      "2025-04-26 01:13:41,481 - INFO - --- Optimizing adaboost (200 trials) ---\n",
      "2025-04-26 01:13:41,481 - INFO - Starting adaboost optimization (200 trials)...\n",
      "2025-04-26 01:13:41,481 - INFO - Optuna timeout for adaboost: 3600s.\n",
      "[I 2025-04-26 01:13:42,213] A new study created in RDB with name: adaboost_opt_20250426_011340\n",
      "2025-04-26 01:13:42,216 - INFO - Setting Optuna timeout 3600s.\n",
      "[I 2025-04-26 01:17:24,630] Trial 0 finished with value: 0.7079961740793879 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': 'balanced', 'n_estimators': 3500, 'learning_rate': 0.033589904727342634}. Best is trial 0 with value: 0.7079961740793879.\n",
      "[I 2025-04-26 01:17:34,703] Trial 1 finished with value: 0.6943280726924916 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.1}, 'n_estimators': 3400, 'learning_rate': 0.16303785195911552}. Best is trial 0 with value: 0.7079961740793879.\n",
      "[I 2025-04-26 01:20:27,778] Trial 2 finished with value: 0.7265327594452415 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 2800, 'learning_rate': 0.8912222034876036}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:20:28,379] Trial 3 finished with value: 0.6386896221903395 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.3}, 'n_estimators': 2250, 'learning_rate': 0.8634371189815653}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:20:31,797] Trial 4 finished with value: 0.5830320420851267 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': 'balanced', 'n_estimators': 950, 'learning_rate': 0.5857518037784043}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:20:47,138] Trial 5 finished with value: 0.6953323768531803 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.3}, 'n_estimators': 250, 'learning_rate': 0.0107742702578548}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:21:38,741] Trial 6 finished with value: 0.5829985652797705 and parameters: {'base_estimator_max_depth': 1, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.3}, 'n_estimators': 3950, 'learning_rate': 0.011165535406008871}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:21:47,435] Trial 7 finished with value: 0.716805356288857 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': 'balanced', 'n_estimators': 2900, 'learning_rate': 0.4746991637924428}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:25:21,636] Trial 8 finished with value: 0.7069966523194643 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.1}, 'n_estimators': 3350, 'learning_rate': 0.08828071851547106}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:26:50,265] Trial 9 finished with value: 0.7031229076996652 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': 'balanced', 'n_estimators': 2050, 'learning_rate': 1.763050603248119}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:26:54,592] Trial 10 finished with value: 0.6914203730272598 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 4800, 'learning_rate': 0.21605734715461447}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:27:48,429] Trial 11 finished with value: 0.7080153036824486 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1550, 'learning_rate': 0.5116518543108826}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:30:27,776] Trial 12 finished with value: 0.7031037780966045 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 3000, 'learning_rate': 1.7903501127578578}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:32:38,752] Trial 13 finished with value: 0.7079961740793879 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 2550, 'learning_rate': 0.3172789639547821}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:32:41,966] Trial 14 finished with value: 0.6445145863223338 and parameters: {'base_estimator_max_depth': 2, 'class_weight_choice': 'balanced', 'n_estimators': 4250, 'learning_rate': 0.07720782251599945}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:34:24,672] Trial 15 finished with value: 0.7245815399330464 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1700, 'learning_rate': 0.8856167286195735}. Best is trial 2 with value: 0.7265327594452415.\n",
      "[I 2025-04-26 01:36:14,836] Trial 16 finished with value: 0.7285126733620277 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1600, 'learning_rate': 1.0130691325431216}. Best is trial 16 with value: 0.7285126733620277.\n",
      "[I 2025-04-26 01:40:03,109] Trial 17 finished with value: 0.7294930655188905 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 950, 'learning_rate': 1.109900763994356}. Best is trial 17 with value: 0.7294930655188905.\n",
      "[I 2025-04-26 01:41:56,737] Trial 18 finished with value: 0.7089765662362506 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 700, 'learning_rate': 0.32043825518705515}. Best is trial 17 with value: 0.7294930655188905.\n",
      "[I 2025-04-26 01:43:18,286] Trial 19 finished with value: 0.7011573409851746 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1200, 'learning_rate': 1.963374575649679}. Best is trial 17 with value: 0.7294930655188905.\n",
      "[I 2025-04-26 01:43:32,255] Trial 20 finished with value: 0.7060210425633667 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.1}, 'n_estimators': 250, 'learning_rate': 1.176906741077412}. Best is trial 17 with value: 0.7294930655188905.\n",
      "[I 2025-04-26 01:45:32,468] Trial 21 finished with value: 0.7304495456719273 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1650, 'learning_rate': 0.9283951753128962}. Best is trial 21 with value: 0.7304495456719273.\n",
      "[I 2025-04-26 01:47:10,271] Trial 22 finished with value: 0.7226446676231468 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1500, 'learning_rate': 1.1636422323831157}. Best is trial 21 with value: 0.7304495456719273.\n",
      "[I 2025-04-26 01:47:54,473] Trial 23 finished with value: 0.7128646580583453 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 700, 'learning_rate': 0.33716629902154593}. Best is trial 21 with value: 0.7304495456719273.\n",
      "[I 2025-04-26 01:50:05,702] Trial 24 finished with value: 0.7265327594452415 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1900, 'learning_rate': 0.6230713385008706}. Best is trial 21 with value: 0.7304495456719273.\n",
      "[I 2025-04-26 01:51:18,780] Trial 25 finished with value: 0.7324198947871832 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1200, 'learning_rate': 1.0854024064278645}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 01:52:15,516] Trial 26 finished with value: 0.7089622190339551 and parameters: {'base_estimator_max_depth': 4, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1150, 'learning_rate': 1.3776943035650528}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 01:52:52,630] Trial 27 finished with value: 0.7206934481109517 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 550, 'learning_rate': 0.6581269729965386}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 01:54:04,901] Trial 28 finished with value: 0.7031085604973697 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.1, '1': 1.0, '2': 1.3}, 'n_estimators': 1200, 'learning_rate': 0.03479276792417019}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 01:56:31,498] Trial 29 finished with value: 0.7089813486370158 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.1}, 'n_estimators': 2350, 'learning_rate': 0.20577866537828093}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 01:57:37,565] Trial 30 finished with value: 0.7050549976087996 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 950, 'learning_rate': 0.10420768735813242}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 01:59:19,659] Trial 31 finished with value: 0.7216594930655189 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1450, 'learning_rate': 1.2610829517791955}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 02:01:30,377] Trial 32 finished with value: 0.728493543758967 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 1800, 'learning_rate': 0.8491447887102049}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 02:04:01,667] Trial 33 finished with value: 0.7177427068388331 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 2050, 'learning_rate': 0.4191881496547233}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 02:04:07,966] Trial 34 finished with value: 0.6923672883787662 and parameters: {'base_estimator_max_depth': 5, 'class_weight_choice': {'0': 1.3, '1': 1.0, '2': 1.2}, 'n_estimators': 100, 'learning_rate': 1.0210595756304028}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 02:04:43,451] Trial 35 finished with value: 0.732415112386418 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.1}, 'n_estimators': 500, 'learning_rate': 0.7212705317964083}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 02:04:52,733] Trial 36 finished with value: 0.6953084648493544 and parameters: {'base_estimator_max_depth': 3, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.1}, 'n_estimators': 500, 'learning_rate': 0.6700462040291312}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 02:05:50,823] Trial 37 finished with value: 0.7255619320899092 and parameters: {'base_estimator_max_depth': 6, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.1}, 'n_estimators': 850, 'learning_rate': 0.758894210134936}. Best is trial 25 with value: 0.7324198947871832.\n",
      "[I 2025-04-26 02:06:02,471] Trial 38 finished with value: 0.6738354854136777 and parameters: {'base_estimator_max_depth': 2, 'class_weight_choice': {'0': 1.2, '1': 1.0, '2': 1.1}, 'n_estimators': 450, 'learning_rate': 1.4152924558864413}. Best is trial 25 with value: 0.7324198947871832.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "# Assume all necessary functions (run_complete_pipeline, etc.) are defined above\n",
    "# Assume logger is configured globally\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # Feature Selection Settings\n",
    "    PERFORM_FEATURE_SELECTION = True # Set to True or False\n",
    "    FS_THRESHOLD = 'mean' # Threshold ('mean', 'median', or float like 1e-5)\n",
    "\n",
    "    # Model Qualification Threshold\n",
    "    MIN_CV_SCORE_THRESHOLD = 0.72 # Minimum average CV score to qualify a model\n",
    "\n",
    "    # Parallelism Setting for Sklearn Models (used in Optuna objective and Ensemble)\n",
    "    # Set to 1 if experiencing issues, otherwise set to desired core count (e.g., os.cpu_count() - 2)\n",
    "    # N_CORES_TO_USE = 1 # Start with 1 for stability, increase carefully\n",
    "    # import os\n",
    "    N_CORES_TO_USE = max(1, os.cpu_count() - 4) # Example: Use all but 2 cores\n",
    "\n",
    "    # --- Run the Pipeline ---\n",
    "    pipeline_start_time = time.time()\n",
    "\n",
    "    success = run_complete_pipeline(\n",
    "        perform_feature_selection=PERFORM_FEATURE_SELECTION,\n",
    "        fs_threshold=FS_THRESHOLD,\n",
    "        min_cv_score_threshold=MIN_CV_SCORE_THRESHOLD,\n",
    "        n_jobs_sklearn=N_CORES_TO_USE # Pass the core count\n",
    "        )\n",
    "\n",
    "    pipeline_end_time = time.time()\n",
    "    pipeline_duration = pipeline_end_time - pipeline_start_time\n",
    "\n",
    "    # --- Final Status Output ---\n",
    "    status_msg = f\"Pipeline execution {'succeeded' if success else 'failed'}.\"\n",
    "    duration_msg = f\"Total time: {pipeline_duration:.2f} seconds ({pipeline_duration / 60:.2f} minutes).\"\n",
    "\n",
    "    print(f\"\\n{'='*30}\\n{status_msg}\")\n",
    "    print(duration_msg)\n",
    "    print(f\"{'='*30}\")\n",
    "\n",
    "    # Log final status if possible\n",
    "    try:\n",
    "        logger.info(status_msg)\n",
    "        logger.info(duration_msg)\n",
    "    except Exception as log_final_e:\n",
    "        # This might happen if the logger itself failed earlier\n",
    "        print(f\"Note: Final status logging failed: {log_final_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4a758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845a5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superNova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
