{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec121310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting max CPU count to: 20\n"
     ]
    }
   ],
   "source": [
    "# Fix for wmic error in Windows\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = str(os.cpu_count())\n",
    "print(f\"Setting max CPU count to: {os.environ['LOKY_MAX_CPU_COUNT']}\")\n",
    "\n",
    "# For older joblib versions, you might also need:\n",
    "os.environ[\"JOBLIB_TEMP_FOLDER\"] = os.path.join(os.path.expanduser(\"~\"), \"temp_joblib\")\n",
    "if not os.path.exists(os.environ[\"JOBLIB_TEMP_FOLDER\"]):\n",
    "    os.makedirs(os.environ[\"JOBLIB_TEMP_FOLDER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc19e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn matplotlib seaborn optuna xgboost lightgbm catboost imbalanced-learn category_encoders joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90830b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run timestamp: 20250423_135441\n",
      "\n",
      "=== Starting Training Pipeline ===\n",
      "\n",
      "Train data shape: (1280, 317)\n",
      "Test data shape: (854, 316)\n",
      "\n",
      "=== Applying Feature Engineering ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\damod\\anaconda3\\envs\\nova\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"c:\\Users\\damod\\anaconda3\\envs\\nova\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\damod\\anaconda3\\envs\\nova\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\damod\\anaconda3\\envs\\nova\\Lib\\subprocess.py\", line 1551, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n",
      "[I 2025-04-23 13:54:42,609] A new study created in memory with name: no-name-8cd4c567-006b-4f27-bcb3-fd863f5be372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training data shape: (1280, 207)\n",
      "Final test data shape: (854, 207)\n",
      "\n",
      "=== Handling Class Imbalance ===\n",
      "\n",
      "Class distribution after resampling: [470 464 467]\n",
      "\n",
      "=== Optimizing Base Models ===\n",
      "\n",
      "\n",
      "Optimizing XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 13:54:50,822] Trial 0 finished with value: 0.7851499745805796 and parameters: {'n_estimators': 740, 'max_depth': 7, 'learning_rate': 0.025618211059620664, 'subsample': 0.5680064509333211, 'colsample_bytree': 0.9559255099275796, 'min_child_weight': 9, 'reg_alpha': 3.0752800848207787e-07, 'reg_lambda': 0.5625395573328916, 'gamma': 0.05430582884404317}. Best is trial 0 with value: 0.7851499745805796.\n",
      "[I 2025-04-23 13:54:53,528] Trial 1 finished with value: 0.7508973055414335 and parameters: {'n_estimators': 140, 'max_depth': 6, 'learning_rate': 0.011443443891037635, 'subsample': 0.8158059311399543, 'colsample_bytree': 0.7333592521164647, 'min_child_weight': 7, 'reg_alpha': 0.4657323358105676, 'reg_lambda': 0.3253777020594294, 'gamma': 3.257279575643104e-07}. Best is trial 0 with value: 0.7851499745805796.\n",
      "[I 2025-04-23 13:55:04,163] Trial 2 finished with value: 0.7987214031520081 and parameters: {'n_estimators': 764, 'max_depth': 11, 'learning_rate': 0.02510626975005081, 'subsample': 0.8112314981693511, 'colsample_bytree': 0.8794631435216078, 'min_child_weight': 3, 'reg_alpha': 1.333036844986283e-07, 'reg_lambda': 3.633456877810539e-05, 'gamma': 2.3738443500626156e-05}. Best is trial 2 with value: 0.7987214031520081.\n",
      "[I 2025-04-23 13:55:10,510] Trial 3 finished with value: 0.7915734621250636 and parameters: {'n_estimators': 794, 'max_depth': 4, 'learning_rate': 0.014773705315325677, 'subsample': 0.7611871306738567, 'colsample_bytree': 0.871721692790865, 'min_child_weight': 3, 'reg_alpha': 0.0004959639599295692, 'reg_lambda': 0.005905073756115348, 'gamma': 9.23934308487586e-07}. Best is trial 2 with value: 0.7987214031520081.\n",
      "[I 2025-04-23 13:55:13,263] Trial 4 finished with value: 0.7758795119471277 and parameters: {'n_estimators': 530, 'max_depth': 4, 'learning_rate': 0.016437421062075096, 'subsample': 0.9587905048518719, 'colsample_bytree': 0.5778632266282564, 'min_child_weight': 4, 'reg_alpha': 0.6969576554693138, 'reg_lambda': 0.013200719221802904, 'gamma': 6.63516623234612e-06}. Best is trial 2 with value: 0.7987214031520081.\n",
      "[I 2025-04-23 13:55:17,578] Trial 5 finished with value: 0.7937264870360956 and parameters: {'n_estimators': 407, 'max_depth': 9, 'learning_rate': 0.03173190645317101, 'subsample': 0.6750285141617451, 'colsample_bytree': 0.8846299229227728, 'min_child_weight': 6, 'reg_alpha': 3.395141289101089e-05, 'reg_lambda': 2.3188336998066248e-05, 'gamma': 5.378063988373453e-08}. Best is trial 2 with value: 0.7987214031520081.\n",
      "[I 2025-04-23 13:55:19,528] Trial 6 finished with value: 0.7794407727503814 and parameters: {'n_estimators': 804, 'max_depth': 5, 'learning_rate': 0.29161369778321933, 'subsample': 0.8084822647306407, 'colsample_bytree': 0.6325748287716559, 'min_child_weight': 6, 'reg_alpha': 2.086050490319015e-07, 'reg_lambda': 0.00023553331162998273, 'gamma': 0.0016253007921633767}. Best is trial 2 with value: 0.7987214031520081.\n",
      "[I 2025-04-23 13:55:22,210] Trial 7 finished with value: 0.7737290289781392 and parameters: {'n_estimators': 949, 'max_depth': 11, 'learning_rate': 0.17598282509628735, 'subsample': 0.5789327611390522, 'colsample_bytree': 0.7100808181089786, 'min_child_weight': 5, 'reg_alpha': 7.832889394824677e-06, 'reg_lambda': 0.0038117669628902383, 'gamma': 0.03305177395293823}. Best is trial 2 with value: 0.7987214031520081.\n",
      "[I 2025-04-23 13:55:25,665] Trial 8 finished with value: 0.7801525165226233 and parameters: {'n_estimators': 543, 'max_depth': 6, 'learning_rate': 0.03739656589440949, 'subsample': 0.7391961654036994, 'colsample_bytree': 0.9275629386726324, 'min_child_weight': 10, 'reg_alpha': 0.00010211743080642032, 'reg_lambda': 0.01061058080699482, 'gamma': 3.2516744104186936e-08}. Best is trial 2 with value: 0.7987214031520081.\n",
      "[I 2025-04-23 13:55:27,751] Trial 9 finished with value: 0.7815785460091511 and parameters: {'n_estimators': 401, 'max_depth': 6, 'learning_rate': 0.09591927355648439, 'subsample': 0.8018290439562142, 'colsample_bytree': 0.7506962840073675, 'min_child_weight': 7, 'reg_alpha': 1.3545622550351327e-07, 'reg_lambda': 1.0110968155342164e-06, 'gamma': 3.600025591853591e-05}. Best is trial 2 with value: 0.7987214031520081.\n",
      "[I 2025-04-23 13:55:34,134] Trial 10 finished with value: 0.7972852058973057 and parameters: {'n_estimators': 989, 'max_depth': 12, 'learning_rate': 0.07570280239613404, 'subsample': 0.95818144081292, 'colsample_bytree': 0.8304616217973826, 'min_child_weight': 1, 'reg_alpha': 1.3365252435479312e-08, 'reg_lambda': 1.7168129056944152e-08, 'gamma': 0.0007515958107958708}. Best is trial 2 with value: 0.7987214031520081.\n",
      "[I 2025-04-23 13:55:40,602] Trial 11 finished with value: 0.7965760040671073 and parameters: {'n_estimators': 930, 'max_depth': 12, 'learning_rate': 0.0724575968814358, 'subsample': 0.9898287562956831, 'colsample_bytree': 0.8160923996175165, 'min_child_weight': 1, 'reg_alpha': 1.3874802103816724e-08, 'reg_lambda': 1.0624755470098386e-08, 'gamma': 0.001131460040270507}. Best is trial 2 with value: 0.7987214031520081.\n",
      "[I 2025-04-23 13:55:50,215] Trial 12 finished with value: 0.8001423487544486 and parameters: {'n_estimators': 996, 'max_depth': 10, 'learning_rate': 0.053603899732478796, 'subsample': 0.9091284639309398, 'colsample_bytree': 0.9923975438674093, 'min_child_weight': 1, 'reg_alpha': 1.050223076171006e-08, 'reg_lambda': 2.848001019681448e-08, 'gamma': 0.0006822374245167409}. Best is trial 12 with value: 0.8001423487544486.\n",
      "[I 2025-04-23 13:55:53,834] Trial 13 finished with value: 0.7908591764107779 and parameters: {'n_estimators': 674, 'max_depth': 9, 'learning_rate': 0.04800823135235899, 'subsample': 0.9209713403282686, 'colsample_bytree': 0.9987210263032996, 'min_child_weight': 3, 'reg_alpha': 1.7116666819096276e-06, 'reg_lambda': 6.634486857767305e-07, 'gamma': 0.7086186241336336}. Best is trial 12 with value: 0.8001423487544486.\n",
      "[I 2025-04-23 13:56:07,672] Trial 14 finished with value: 0.7951499745805795 and parameters: {'n_estimators': 846, 'max_depth': 10, 'learning_rate': 0.023108605162933435, 'subsample': 0.8790679642473037, 'colsample_bytree': 0.9827171904877414, 'min_child_weight': 2, 'reg_alpha': 0.025947813637827204, 'reg_lambda': 4.488616888167123e-07, 'gamma': 5.617751915161435e-05}. Best is trial 12 with value: 0.8001423487544486.\n",
      "[I 2025-04-23 13:56:11,341] Trial 15 finished with value: 0.7951448906964921 and parameters: {'n_estimators': 670, 'max_depth': 9, 'learning_rate': 0.13801953719030763, 'subsample': 0.8782120291885305, 'colsample_bytree': 0.8996842761511776, 'min_child_weight': 3, 'reg_alpha': 1.2845427799096588e-06, 'reg_lambda': 2.5760517785051154e-05, 'gamma': 5.414896908243836e-06}. Best is trial 12 with value: 0.8001423487544486.\n",
      "[I 2025-04-23 13:56:19,917] Trial 16 finished with value: 0.7994280630401626 and parameters: {'n_estimators': 886, 'max_depth': 10, 'learning_rate': 0.04676081852859709, 'subsample': 0.7145052159013653, 'colsample_bytree': 0.8129907804543502, 'min_child_weight': 1, 'reg_alpha': 0.0015069290793031434, 'reg_lambda': 1.346946682382422e-07, 'gamma': 0.008778474435397251}. Best is trial 12 with value: 0.8001423487544486.\n",
      "[I 2025-04-23 13:56:26,302] Trial 17 finished with value: 0.8051423487544485 and parameters: {'n_estimators': 876, 'max_depth': 8, 'learning_rate': 0.049049720439948166, 'subsample': 0.6589916387185537, 'colsample_bytree': 0.5028213811681286, 'min_child_weight': 1, 'reg_alpha': 0.002010371062285465, 'reg_lambda': 8.827417432355349e-08, 'gamma': 0.013240320958844238}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:56:29,414] Trial 18 finished with value: 0.7965861718352822 and parameters: {'n_estimators': 992, 'max_depth': 8, 'learning_rate': 0.0645393306051293, 'subsample': 0.6426865092428471, 'colsample_bytree': 0.5179003344663399, 'min_child_weight': 2, 'reg_alpha': 0.004049774160151359, 'reg_lambda': 4.752115596198986e-06, 'gamma': 0.5511067433223612}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:56:30,888] Trial 19 finished with value: 0.7930147432638536 and parameters: {'n_estimators': 125, 'max_depth': 8, 'learning_rate': 0.10612174299446923, 'subsample': 0.6055286556250018, 'colsample_bytree': 0.6572967508119756, 'min_child_weight': 4, 'reg_alpha': 0.027935465363697187, 'reg_lambda': 6.075433909263834e-08, 'gamma': 0.00021598755810181813}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:56:31,993] Trial 20 finished with value: 0.7730223690899847 and parameters: {'n_estimators': 279, 'max_depth': 3, 'learning_rate': 0.03732273211393223, 'subsample': 0.6770031911856362, 'colsample_bytree': 0.5159203412095733, 'min_child_weight': 2, 'reg_alpha': 0.00015032874823057223, 'reg_lambda': 3.922203973011586e-06, 'gamma': 0.008093011042989649}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:56:41,991] Trial 21 finished with value: 0.8044102694458566 and parameters: {'n_estimators': 897, 'max_depth': 10, 'learning_rate': 0.051002198710831406, 'subsample': 0.5024859667975882, 'colsample_bytree': 0.7965964821172269, 'min_child_weight': 1, 'reg_alpha': 0.0011508082078180652, 'reg_lambda': 9.917574256711719e-08, 'gamma': 0.010979580210579586}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:56:47,229] Trial 22 finished with value: 0.7887264870360956 and parameters: {'n_estimators': 884, 'max_depth': 10, 'learning_rate': 0.05315405263108708, 'subsample': 0.5011297018485755, 'colsample_bytree': 0.5801021313199304, 'min_child_weight': 1, 'reg_alpha': 0.01383359820497664, 'reg_lambda': 5.621915625607831e-08, 'gamma': 0.09220081720690113}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:56:51,596] Trial 23 finished with value: 0.7937061514997457 and parameters: {'n_estimators': 669, 'max_depth': 11, 'learning_rate': 0.08957885068963585, 'subsample': 0.501217637912676, 'colsample_bytree': 0.7702790213321332, 'min_child_weight': 2, 'reg_alpha': 0.0007603128301450294, 'reg_lambda': 1.8911481984211983e-07, 'gamma': 0.0065466759963428095}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:56:58,148] Trial 24 finished with value: 0.8008439247585155 and parameters: {'n_estimators': 889, 'max_depth': 8, 'learning_rate': 0.05885147135307204, 'subsample': 0.5417102908172892, 'colsample_bytree': 0.6843271639148207, 'min_child_weight': 1, 'reg_alpha': 2.0665037006181278e-05, 'reg_lambda': 3.3128541155307734e-08, 'gamma': 0.0002786368973386513}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:00,779] Trial 25 finished with value: 0.7915836298932384 and parameters: {'n_estimators': 613, 'max_depth': 7, 'learning_rate': 0.13414233281214955, 'subsample': 0.5485479812625098, 'colsample_bytree': 0.6846088832862384, 'min_child_weight': 4, 'reg_alpha': 4.040580895563497e-05, 'reg_lambda': 2.833219582555545e-06, 'gamma': 0.0002061851335517131}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:05,930] Trial 26 finished with value: 0.7994280630401628 and parameters: {'n_estimators': 876, 'max_depth': 8, 'learning_rate': 0.04099836492263279, 'subsample': 0.6142622002379116, 'colsample_bytree': 0.6017892432475932, 'min_child_weight': 2, 'reg_alpha': 0.004713576166596816, 'reg_lambda': 0.00026134321626199413, 'gamma': 0.12299203887381187}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:11,968] Trial 27 finished with value: 0.8015556685307577 and parameters: {'n_estimators': 729, 'max_depth': 7, 'learning_rate': 0.0628659291637204, 'subsample': 0.5365426801682887, 'colsample_bytree': 0.7795478095750689, 'min_child_weight': 1, 'reg_alpha': 0.09811774395585973, 'reg_lambda': 2.209233102378398e-07, 'gamma': 0.00332367587505573}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:17,698] Trial 28 finished with value: 0.7937086934417896 and parameters: {'n_estimators': 822, 'max_depth': 7, 'learning_rate': 0.03275545729498961, 'subsample': 0.5292038876245718, 'colsample_bytree': 0.7789547886572037, 'min_child_weight': 5, 'reg_alpha': 0.040922162367459755, 'reg_lambda': 2.12378158585836e-07, 'gamma': 0.00500288496792265}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:23,313] Trial 29 finished with value: 0.7794356888662938 and parameters: {'n_estimators': 730, 'max_depth': 9, 'learning_rate': 0.02768076510405587, 'subsample': 0.589169022775356, 'colsample_bytree': 0.8485241603386325, 'min_child_weight': 8, 'reg_alpha': 0.22471105095250496, 'reg_lambda': 1.3735956363288755e-06, 'gamma': 0.046443231199663704}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:26,358] Trial 30 finished with value: 0.7994229791560752 and parameters: {'n_estimators': 731, 'max_depth': 5, 'learning_rate': 0.12328509805811225, 'subsample': 0.6285370049569428, 'colsample_bytree': 0.7960290064935077, 'min_child_weight': 2, 'reg_alpha': 0.08625513878718788, 'reg_lambda': 1.0893802202112603e-05, 'gamma': 0.02329436582785386}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:32,044] Trial 31 finished with value: 0.798002033553635 and parameters: {'n_estimators': 921, 'max_depth': 7, 'learning_rate': 0.062218494875766604, 'subsample': 0.5503180329385529, 'colsample_bytree': 0.6882107165281486, 'min_child_weight': 1, 'reg_alpha': 0.00033774722089117244, 'reg_lambda': 6.813224303923577e-08, 'gamma': 0.002624991965555204}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:37,777] Trial 32 finished with value: 0.79443314692425 and parameters: {'n_estimators': 840, 'max_depth': 8, 'learning_rate': 0.07115862398094577, 'subsample': 0.5331803076867947, 'colsample_bytree': 0.7371780939756307, 'min_child_weight': 1, 'reg_alpha': 0.00379048585071075, 'reg_lambda': 3.9655236116096503e-07, 'gamma': 0.00026150099545081164}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:41,422] Trial 33 finished with value: 0.7880071174377224 and parameters: {'n_estimators': 736, 'max_depth': 7, 'learning_rate': 0.08788730060462463, 'subsample': 0.5556586218374888, 'colsample_bytree': 0.7029448888015642, 'min_child_weight': 3, 'reg_alpha': 6.026260350602923e-06, 'reg_lambda': 2.8927180584218866e-08, 'gamma': 0.01659807217612289}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:45,461] Trial 34 finished with value: 0.7994255210981189 and parameters: {'n_estimators': 928, 'max_depth': 6, 'learning_rate': 0.04528096376318666, 'subsample': 0.5817508653568688, 'colsample_bytree': 0.5461331816864501, 'min_child_weight': 2, 'reg_alpha': 0.18278559263780203, 'reg_lambda': 1.0341839470771548e-08, 'gamma': 0.2203208560255397}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:57:56,369] Trial 35 finished with value: 0.8051372648703611 and parameters: {'n_estimators': 780, 'max_depth': 8, 'learning_rate': 0.02196737772423391, 'subsample': 0.6530839868679433, 'colsample_bytree': 0.6440584797422397, 'min_child_weight': 1, 'reg_alpha': 1.961246926016445e-05, 'reg_lambda': 1.1167804320462982e-07, 'gamma': 0.0033718641927698054}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:58:05,283] Trial 36 finished with value: 0.8001423487544486 and parameters: {'n_estimators': 606, 'max_depth': 9, 'learning_rate': 0.017005968920956672, 'subsample': 0.6745507534795042, 'colsample_bytree': 0.6397608848967774, 'min_child_weight': 3, 'reg_alpha': 0.0010745489223498645, 'reg_lambda': 1.3137816105803595e-07, 'gamma': 0.0026880413089446762}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:58:12,019] Trial 37 finished with value: 0.7894382308083376 and parameters: {'n_estimators': 776, 'max_depth': 5, 'learning_rate': 0.011627928545096203, 'subsample': 0.6518153543082517, 'colsample_bytree': 0.8535314995657192, 'min_child_weight': 4, 'reg_alpha': 0.0002447920755551673, 'reg_lambda': 0.9213662804110154, 'gamma': 0.1973980629837981}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:58:19,830] Trial 38 finished with value: 0.7965734621250634 and parameters: {'n_estimators': 773, 'max_depth': 7, 'learning_rate': 0.023388437891455272, 'subsample': 0.7157207060028701, 'colsample_bytree': 0.5975773634694722, 'min_child_weight': 2, 'reg_alpha': 0.006824170953089704, 'reg_lambda': 0.1396542545746903, 'gamma': 9.474941541593802e-06}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:58:24,936] Trial 39 finished with value: 0.7858642602948652 and parameters: {'n_estimators': 623, 'max_depth': 9, 'learning_rate': 0.01945231537297945, 'subsample': 0.7751103575292684, 'colsample_bytree': 0.5463957968433389, 'min_child_weight': 9, 'reg_alpha': 0.0018014556645346794, 'reg_lambda': 8.551509056068918e-05, 'gamma': 0.017646769892473862}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:58:41,968] Trial 40 finished with value: 0.8051372648703611 and parameters: {'n_estimators': 464, 'max_depth': 11, 'learning_rate': 0.01319396338246777, 'subsample': 0.7154303676513091, 'colsample_bytree': 0.7612367910977936, 'min_child_weight': 1, 'reg_alpha': 5.656995782125304e-05, 'reg_lambda': 0.0010480688044462198, 'gamma': 2.7145557151277336e-07}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:58:58,377] Trial 41 finished with value: 0.8051372648703609 and parameters: {'n_estimators': 458, 'max_depth': 11, 'learning_rate': 0.012232209460797177, 'subsample': 0.7046816588651628, 'colsample_bytree': 0.7252787526745863, 'min_child_weight': 1, 'reg_alpha': 1.4407751857441422e-05, 'reg_lambda': 0.002120575910182128, 'gamma': 1.8684139787679883e-07}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:59:17,181] Trial 42 finished with value: 0.8022826639552619 and parameters: {'n_estimators': 442, 'max_depth': 11, 'learning_rate': 0.010210430587671, 'subsample': 0.6977446377477672, 'colsample_bytree': 0.7272231975747546, 'min_child_weight': 1, 'reg_alpha': 1.13790861462393e-05, 'reg_lambda': 0.000839112731279747, 'gamma': 6.119042554636012e-07}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:59:30,958] Trial 43 finished with value: 0.8015709201830198 and parameters: {'n_estimators': 480, 'max_depth': 12, 'learning_rate': 0.012871448112343318, 'subsample': 0.746040592414559, 'colsample_bytree': 0.7484467884615462, 'min_child_weight': 2, 'reg_alpha': 1.3927488730169544e-06, 'reg_lambda': 0.00208778002723034, 'gamma': 1.1603196973695705e-07}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:59:39,610] Trial 44 finished with value: 0.798723945094052 and parameters: {'n_estimators': 346, 'max_depth': 11, 'learning_rate': 0.014512107495628323, 'subsample': 0.7815931420952474, 'colsample_bytree': 0.6447298702022608, 'min_child_weight': 3, 'reg_alpha': 8.052792012752625e-05, 'reg_lambda': 0.0008644320094075448, 'gamma': 1.6219037593670507e-08}. Best is trial 17 with value: 0.8051423487544485.\n",
      "[I 2025-04-23 13:59:51,710] Trial 45 finished with value: 0.8051474326385358 and parameters: {'n_estimators': 501, 'max_depth': 10, 'learning_rate': 0.018749402591536447, 'subsample': 0.7162082327758323, 'colsample_bytree': 0.7151681219871792, 'min_child_weight': 1, 'reg_alpha': 2.711808432344372e-06, 'reg_lambda': 0.0478131139360345, 'gamma': 1.892745571057327e-07}. Best is trial 45 with value: 0.8051474326385358.\n",
      "[I 2025-04-23 14:00:04,098] Trial 46 finished with value: 0.7965760040671073 and parameters: {'n_estimators': 504, 'max_depth': 11, 'learning_rate': 0.01928269829782169, 'subsample': 0.7302048380147769, 'colsample_bytree': 0.715354275014103, 'min_child_weight': 1, 'reg_alpha': 5.26400414120987e-07, 'reg_lambda': 0.12574307468881196, 'gamma': 1.991093577076613e-06}. Best is trial 45 with value: 0.8051474326385358.\n",
      "[I 2025-04-23 14:00:08,463] Trial 47 finished with value: 0.7780249110320285 and parameters: {'n_estimators': 315, 'max_depth': 12, 'learning_rate': 0.013877972979245257, 'subsample': 0.661495373796411, 'colsample_bytree': 0.6206852287908935, 'min_child_weight': 7, 'reg_alpha': 4.6173103785573705e-06, 'reg_lambda': 0.030481833359701265, 'gamma': 2.282523835996724e-07}. Best is trial 45 with value: 0.8051474326385358.\n",
      "[I 2025-04-23 14:00:13,886] Trial 48 finished with value: 0.7965785460091509 and parameters: {'n_estimators': 195, 'max_depth': 10, 'learning_rate': 0.018080422845510567, 'subsample': 0.6946250877978604, 'colsample_bytree': 0.667335390006366, 'min_child_weight': 2, 'reg_alpha': 6.251799264091297e-05, 'reg_lambda': 0.04392625943318659, 'gamma': 3.9196234006658795e-08}. Best is trial 45 with value: 0.8051474326385358.\n",
      "[I 2025-04-23 14:00:25,255] Trial 49 finished with value: 0.7951525165226233 and parameters: {'n_estimators': 407, 'max_depth': 11, 'learning_rate': 0.010036429748396681, 'subsample': 0.8310206423752892, 'colsample_bytree': 0.7591002626315394, 'min_child_weight': 3, 'reg_alpha': 1.3622899176416021e-05, 'reg_lambda': 0.005001911402861626, 'gamma': 1.015837206913741e-07}. Best is trial 45 with value: 0.8051474326385358.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Params: {'n_estimators': 501, 'max_depth': 10, 'learning_rate': 0.018749402591536447, 'subsample': 0.7162082327758323, 'colsample_bytree': 0.7151681219871792, 'min_child_weight': 1, 'reg_alpha': 2.711808432344372e-06, 'reg_lambda': 0.0478131139360345, 'gamma': 1.892745571057327e-07}\n",
      "Best XGBoost CV Accuracy: 0.8051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:00:37,252] A new study created in memory with name: no-name-69ccf530-5011-4329-9068-5106fc672b8d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost CV Accuracy: 0.7973 ± 0.0257\n",
      "\n",
      "Optimizing CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-23 14:01:01,071] Trial 0 failed with parameters: {'iterations': 428, 'depth': 9, 'learning_rate': 0.010457773760883068, 'l2_leaf_reg': 0.0013078910949593458, 'random_strength': 4.140447401164749e-08, 'border_count': 129, 'bagging_temperature': 3.7361131394880873} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-04-23 14:01:01,071] Trial 0 failed with value np.float64(nan).\n",
      "[I 2025-04-23 14:07:11,713] Trial 1 finished with value: 0.7822953736654805 and parameters: {'iterations': 867, 'depth': 9, 'learning_rate': 0.045106093960591495, 'l2_leaf_reg': 0.0017678706992571046, 'random_strength': 1.0837481483261338e-06, 'border_count': 223, 'bagging_temperature': 8.462377171995024}. Best is trial 1 with value: 0.7822953736654805.\n",
      "[I 2025-04-23 14:08:25,636] Trial 2 finished with value: 0.8030071174377225 and parameters: {'iterations': 496, 'depth': 6, 'learning_rate': 0.03096979388426965, 'l2_leaf_reg': 2.2312798895384105e-06, 'random_strength': 3.0358967471684954e-08, 'border_count': 207, 'bagging_temperature': 0.19535084390722157}. Best is trial 2 with value: 0.8030071174377225.\n",
      "[I 2025-04-23 14:08:42,038] Trial 3 finished with value: 0.8065632943568888 and parameters: {'iterations': 876, 'depth': 6, 'learning_rate': 0.1375538369183547, 'l2_leaf_reg': 0.0006701966140011641, 'random_strength': 1.9495939551894314e-06, 'border_count': 59, 'bagging_temperature': 5.141117355666601}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:20:23,297] Trial 4 finished with value: 0.7651703101169293 and parameters: {'iterations': 656, 'depth': 10, 'learning_rate': 0.017268772684202204, 'l2_leaf_reg': 0.0008836530447526557, 'random_strength': 0.00013422509951832964, 'border_count': 248, 'bagging_temperature': 8.205676943291252}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:26:51,608] Trial 5 finished with value: 0.7822826639552618 and parameters: {'iterations': 701, 'depth': 10, 'learning_rate': 0.14613724952115625, 'l2_leaf_reg': 6.76479391849229e-08, 'random_strength': 1.7960651335227897, 'border_count': 178, 'bagging_temperature': 2.0616677552828735}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:30:03,241] Trial 6 finished with value: 0.7787239450940519 and parameters: {'iterations': 614, 'depth': 8, 'learning_rate': 0.027958608874379304, 'l2_leaf_reg': 2.2320501882967707e-05, 'random_strength': 0.00011728176074807561, 'border_count': 244, 'bagging_temperature': 8.550232539734145}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:30:21,303] Trial 7 finished with value: 0.7494534824606 and parameters: {'iterations': 374, 'depth': 7, 'learning_rate': 0.01406818110569849, 'l2_leaf_reg': 0.03021040467091972, 'random_strength': 0.341178619438883, 'border_count': 134, 'bagging_temperature': 9.096174458124535}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:30:35,788] Trial 8 finished with value: 0.7551626842907981 and parameters: {'iterations': 125, 'depth': 8, 'learning_rate': 0.016963336297751767, 'l2_leaf_reg': 2.0458731652968056e-06, 'random_strength': 0.0038721595434351402, 'border_count': 140, 'bagging_temperature': 2.3690775617483073}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:34:26,023] Trial 9 finished with value: 0.7880096593797662 and parameters: {'iterations': 890, 'depth': 10, 'learning_rate': 0.1257750948940923, 'l2_leaf_reg': 0.0157802099690722, 'random_strength': 0.3376424725032553, 'border_count': 56, 'bagging_temperature': 2.2379662745851245}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:36:04,406] Trial 10 finished with value: 0.7808769700050838 and parameters: {'iterations': 351, 'depth': 9, 'learning_rate': 0.1275140060488808, 'l2_leaf_reg': 0.002557237733305852, 'random_strength': 1.5020146463151324e-06, 'border_count': 206, 'bagging_temperature': 3.5774838845851464}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:36:08,108] Trial 11 finished with value: 0.7901448906964921 and parameters: {'iterations': 958, 'depth': 4, 'learning_rate': 0.19813978869654125, 'l2_leaf_reg': 5.400850687843127, 'random_strength': 1.614605033446112e-08, 'border_count': 33, 'bagging_temperature': 6.012834663282222}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:36:11,703] Trial 12 finished with value: 0.7951601423487544 and parameters: {'iterations': 441, 'depth': 5, 'learning_rate': 0.06368565846177213, 'l2_leaf_reg': 6.376416955321921e-06, 'random_strength': 3.743031841522088e-08, 'border_count': 93, 'bagging_temperature': 0.1411255059567234}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:36:24,725] Trial 13 finished with value: 0.8015607524148448 and parameters: {'iterations': 773, 'depth': 6, 'learning_rate': 0.06115626815386801, 'l2_leaf_reg': 1.1560550541643634e-08, 'random_strength': 8.168399787797705e-07, 'border_count': 88, 'bagging_temperature': 5.7591495371047206}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:36:38,241] Trial 14 finished with value: 0.7837163192679206 and parameters: {'iterations': 498, 'depth': 6, 'learning_rate': 0.03523807971964004, 'l2_leaf_reg': 3.184132394036625e-05, 'random_strength': 9.883137018921576e-06, 'border_count': 181, 'bagging_temperature': 4.517820193416714}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:36:41,908] Trial 15 finished with value: 0.7844306049822064 and parameters: {'iterations': 239, 'depth': 6, 'learning_rate': 0.2639849039788329, 'l2_leaf_reg': 4.4637317353354427e-07, 'random_strength': 8.631952288631267e-08, 'border_count': 100, 'bagging_temperature': 6.702621894611704}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:36:48,438] Trial 16 finished with value: 0.802290289781393 and parameters: {'iterations': 555, 'depth': 5, 'learning_rate': 0.08269605678274436, 'l2_leaf_reg': 0.9710970173579918, 'random_strength': 1.9962866517331975e-05, 'border_count': 172, 'bagging_temperature': 0.03136238932404983}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:37:13,994] Trial 17 finished with value: 0.7851499745805796 and parameters: {'iterations': 800, 'depth': 7, 'learning_rate': 0.02901305578281659, 'l2_leaf_reg': 0.0001088479757730809, 'random_strength': 0.007818971032409491, 'border_count': 116, 'bagging_temperature': 4.3648282428974445}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:37:19,671] Trial 18 finished with value: 0.7915582104728012 and parameters: {'iterations': 984, 'depth': 4, 'learning_rate': 0.08190912434554547, 'l2_leaf_reg': 5.744821998388928e-07, 'random_strength': 2.645151575369006e-07, 'border_count': 66, 'bagging_temperature': 3.251577208403592}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:37:22,809] Trial 19 finished with value: 0.7194661921708185 and parameters: {'iterations': 270, 'depth': 5, 'learning_rate': 0.02421191404608944, 'l2_leaf_reg': 0.08995556468666302, 'random_strength': 5.570982870441879e-06, 'border_count': 160, 'bagging_temperature': 7.058670002358216}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:37:37,444] Trial 20 finished with value: 0.8044178952719877 and parameters: {'iterations': 569, 'depth': 6, 'learning_rate': 0.03837255101613142, 'l2_leaf_reg': 0.00018523091890779807, 'random_strength': 1.3933614097402905e-08, 'border_count': 206, 'bagging_temperature': 1.189494457418597}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:38:30,011] Trial 21 finished with value: 0.8058413828164719 and parameters: {'iterations': 719, 'depth': 8, 'learning_rate': 0.2895592425940963, 'l2_leaf_reg': 0.00021416946307102472, 'random_strength': 0.002440347817661132, 'border_count': 119, 'bagging_temperature': 1.3886734070088966}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:38:59,647] Trial 22 finished with value: 0.8051296390442297 and parameters: {'iterations': 734, 'depth': 8, 'learning_rate': 0.2526270202160035, 'l2_leaf_reg': 0.00021565410226535105, 'random_strength': 0.00155389563933026, 'border_count': 63, 'bagging_temperature': 1.2934973135802803}. Best is trial 3 with value: 0.8065632943568888.\n",
      "[I 2025-04-23 14:39:28,213] Trial 23 finished with value: 0.8072877478393492 and parameters: {'iterations': 735, 'depth': 8, 'learning_rate': 0.29464650051264085, 'l2_leaf_reg': 0.005294275361357343, 'random_strength': 0.0026544813331072724, 'border_count': 64, 'bagging_temperature': 1.2897019053978136}. Best is trial 23 with value: 0.8072877478393492.\n",
      "[I 2025-04-23 14:40:12,940] Trial 24 finished with value: 0.7887290289781392 and parameters: {'iterations': 835, 'depth': 9, 'learning_rate': 0.184090330106098, 'l2_leaf_reg': 0.0062678899342904154, 'random_strength': 0.02079259932844412, 'border_count': 32, 'bagging_temperature': 3.4649409302756027}. Best is trial 23 with value: 0.8072877478393492.\n",
      "[I 2025-04-23 14:41:18,555] Trial 25 finished with value: 0.7880122013218098 and parameters: {'iterations': 918, 'depth': 8, 'learning_rate': 0.2898098568504809, 'l2_leaf_reg': 0.08676044632891744, 'random_strength': 0.0007436979897936174, 'border_count': 119, 'bagging_temperature': 5.249958571252816}. Best is trial 23 with value: 0.8072877478393492.\n",
      "[I 2025-04-23 14:41:34,761] Trial 26 finished with value: 0.8101347229283172 and parameters: {'iterations': 688, 'depth': 7, 'learning_rate': 0.2023273046153269, 'l2_leaf_reg': 0.0008238408401774364, 'random_strength': 0.0001068584781664134, 'border_count': 74, 'bagging_temperature': 1.0697987397659339}. Best is trial 26 with value: 0.8101347229283172.\n",
      "[I 2025-04-23 14:41:51,155] Trial 27 finished with value: 0.7922852058973054 and parameters: {'iterations': 649, 'depth': 7, 'learning_rate': 0.18583850642703045, 'l2_leaf_reg': 0.4541079864667853, 'random_strength': 4.6493846082821734e-05, 'border_count': 76, 'bagging_temperature': 4.390873764759634}. Best is trial 26 with value: 0.8101347229283172.\n",
      "[I 2025-04-23 14:42:05,169] Trial 28 finished with value: 0.7937163192679206 and parameters: {'iterations': 790, 'depth': 7, 'learning_rate': 0.10650039289298635, 'l2_leaf_reg': 0.0071065384274272645, 'random_strength': 0.03344105628051454, 'border_count': 43, 'bagging_temperature': 2.771409648638847}. Best is trial 26 with value: 0.8101347229283172.\n",
      "[I 2025-04-23 14:42:26,247] Trial 29 finished with value: 0.8108439247585155 and parameters: {'iterations': 844, 'depth': 7, 'learning_rate': 0.2188504393461133, 'l2_leaf_reg': 0.0007447295295401203, 'random_strength': 0.00027291656783447714, 'border_count': 77, 'bagging_temperature': 1.009698991167879}. Best is trial 29 with value: 0.8108439247585155.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CatBoost Params: {'iterations': 844, 'depth': 7, 'learning_rate': 0.2188504393461133, 'l2_leaf_reg': 0.0007447295295401203, 'random_strength': 0.00027291656783447714, 'border_count': 77, 'bagging_temperature': 1.009698991167879}\n",
      "Best CatBoost CV Accuracy: 0.8108\n",
      "CatBoost CV Accuracy: 0.8080 ± 0.0173\n",
      "\n",
      "=== Building Ensemble Model ===\n",
      "\n",
      "\n",
      "=== Generating Predictions ===\n",
      "\n",
      "Submission saved to submissions/solution_format_20250423_135441.csv\n",
      "\n",
      "=== Prediction Distribution ===\n",
      "\n",
      "Low       849\n",
      "High        4\n",
      "Medium      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Training Pipeline Complete ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from category_encoders import TargetEncoder, CatBoostEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import warnings\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directories\n",
    "def create_directories():\n",
    "    \"\"\"Create directories for saving models, features, results\"\"\"\n",
    "    directories = ['models', 'features', 'results', 'submissions']\n",
    "    \n",
    "    for directory in directories:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Created directory: {directory}\")\n",
    "\n",
    "# Get timestamp for unique filenames\n",
    "def get_timestamp():\n",
    "    \"\"\"Get a formatted timestamp for filenames\"\"\"\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ----------------- Step 1: Load and Prepare Data with Better Error Handling -----------------\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load datasets with enhanced validation and handling of edge cases.\"\"\"\n",
    "    try:\n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        \n",
    "        # Check for required columns\n",
    "        expected_cols = {'obs', 'job_title', 'job_posted_date', 'job_state', 'feature_1'}\n",
    "        if not expected_cols.issubset(train_df.columns):\n",
    "            raise ValueError(f\"Essential columns missing: {expected_cols - set(train_df.columns)}\")\n",
    "        \n",
    "        # Ensure train data has target column\n",
    "        if 'salary_category' not in train_df.columns:\n",
    "            raise ValueError(\"Training data is missing target column 'salary_category'\")\n",
    "        \n",
    "        # Check for consistent data types between train and test\n",
    "        for col in test_df.columns:\n",
    "            if col in train_df.columns and train_df[col].dtype != test_df[col].dtype:\n",
    "                print(f\"Warning: Column {col} has different types in train and test. Converting to consistent type.\")\n",
    "                # Try to make types consistent\n",
    "                if pd.api.types.is_numeric_dtype(train_df[col]):\n",
    "                    test_df[col] = pd.to_numeric(test_df[col], errors='coerce')\n",
    "                else:\n",
    "                    test_df[col] = test_df[col].astype(str)\n",
    "        \n",
    "        print(f\"Train data shape: {train_df.shape}\")\n",
    "        print(f\"Test data shape: {test_df.shape}\")\n",
    "        \n",
    "        return train_df, test_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# ----------------- Step 2: Enhanced Feature Engineering -----------------\n",
    "\n",
    "def engineer_features(train_df, test_df, timestamp, is_training=True):\n",
    "    \"\"\"Apply comprehensive feature engineering with better handling of edge cases.\"\"\"\n",
    "    \n",
    "    # Make copies to avoid modifying original\n",
    "    X_train = train_df.copy()\n",
    "    X_test = test_df.copy()\n",
    "    \n",
    "    # Extract target if training\n",
    "    if is_training and 'salary_category' in X_train.columns:\n",
    "        y = X_train['salary_category'].copy()\n",
    "        # Map categories to numerical values\n",
    "        label_mapping = {'High': 2, 'Medium': 1, 'Low': 0}\n",
    "        y = y.map(label_mapping)\n",
    "        X_train = X_train.drop(columns=['salary_category'])\n",
    "    else:\n",
    "        y = None\n",
    "    \n",
    "    # Remove obs column\n",
    "    if 'obs' in X_train.columns:\n",
    "        X_train = X_train.drop(columns=['obs'])\n",
    "    if 'obs' in X_test.columns:\n",
    "        test_obs = X_test['obs'].copy()\n",
    "        X_test = X_test.drop(columns=['obs'])\n",
    "    else:\n",
    "        test_obs = None\n",
    "    \n",
    "    # Define feature types\n",
    "    numerical_cols = ['feature_2', 'feature_9', 'feature_12']\n",
    "    categorical_cols = ['job_title', 'job_state', 'feature_1']\n",
    "    boolean_cols = ['feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', \n",
    "                   'feature_8', 'feature_10', 'feature_11']\n",
    "    job_desc_cols = [f'job_desc_{str(i).zfill(3)}' for i in range(1, 301)]\n",
    "    date_cols = ['job_posted_date']\n",
    "    \n",
    "    # ----- Handle missing values -----\n",
    "    \n",
    "    # For numerical columns\n",
    "    num_imputer = KNNImputer(n_neighbors=5)\n",
    "    for col in numerical_cols:\n",
    "        if col in X_train.columns:\n",
    "            # Convert to numeric first, handling non-numeric values\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "            \n",
    "            # Reshape for imputer\n",
    "            X_train_reshaped = X_train[col].values.reshape(-1, 1)\n",
    "            X_train[col] = num_imputer.fit_transform(X_train_reshaped).ravel()\n",
    "            \n",
    "            X_test_reshaped = X_test[col].values.reshape(-1, 1)\n",
    "            X_test[col] = num_imputer.transform(X_test_reshaped).ravel()\n",
    "    \n",
    "    # For categorical columns\n",
    "    for col in categorical_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].fillna('Unknown')\n",
    "            X_test[col] = X_test[col].fillna('Unknown')\n",
    "    \n",
    "    # For boolean columns\n",
    "    for col in boolean_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].fillna(0).astype(int)\n",
    "            X_test[col] = X_test[col].fillna(0).astype(int)\n",
    "    \n",
    "    # For job description columns - better handling of sparse feature matrices\n",
    "    for col in job_desc_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].fillna(0)\n",
    "            X_test[col] = X_test[col].fillna(0)\n",
    "    \n",
    "    # ----- Job Title Feature Extraction -----\n",
    "    \n",
    "    if 'job_title' in X_train.columns:\n",
    "        # Extract seniority level\n",
    "        X_train['is_senior'] = X_train['job_title'].str.contains(\n",
    "            'senior|sr|lead|principal|staff|architect|head', case=False).astype(int)\n",
    "        X_test['is_senior'] = X_test['job_title'].str.contains(\n",
    "            'senior|sr|lead|principal|staff|architect|head', case=False).astype(int)\n",
    "        \n",
    "        X_train['is_junior'] = X_train['job_title'].str.contains(\n",
    "            'junior|jr|associate|entry|intern', case=False).astype(int)\n",
    "        X_test['is_junior'] = X_test['job_title'].str.contains(\n",
    "            'junior|jr|associate|entry|intern', case=False).astype(int)\n",
    "        \n",
    "        # Extract role type\n",
    "        X_train['is_manager'] = X_train['job_title'].str.contains(\n",
    "            'manager|director|lead|head', case=False).astype(int)\n",
    "        X_test['is_manager'] = X_test['job_title'].str.contains(\n",
    "            'manager|director|lead|head', case=False).astype(int)\n",
    "        \n",
    "        X_train['is_developer'] = X_train['job_title'].str.contains(\n",
    "            'developer|engineer|programmer|coder', case=False).astype(int)\n",
    "        X_test['is_developer'] = X_test['job_title'].str.contains(\n",
    "            'developer|engineer|programmer|coder', case=False).astype(int)\n",
    "        \n",
    "        X_train['is_data'] = X_train['job_title'].str.contains(\n",
    "            'data|scientist|analyst|analytics', case=False).astype(int)\n",
    "        X_test['is_data'] = X_test['job_title'].str.contains(\n",
    "            'data|scientist|analyst|analytics', case=False).astype(int)\n",
    "        \n",
    "        # Target encoding for job title\n",
    "        if is_training:\n",
    "            title_encoder = TargetEncoder()\n",
    "            X_train['job_title_encoded'] = title_encoder.fit_transform(X_train[['job_title']], y)\n",
    "            # Save encoder for test predictions\n",
    "            joblib.dump(title_encoder, f'features/job_title_encoder_{timestamp}.joblib')\n",
    "        else:\n",
    "            # Find the most recent encoder file\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if 'job_title_encoder' in f])\n",
    "            if encoder_files:\n",
    "                title_encoder = joblib.load(f'features/{encoder_files[-1]}')\n",
    "                X_test['job_title_encoded'] = title_encoder.transform(X_test[['job_title']])\n",
    "            else:\n",
    "                print(\"Warning: Job title encoder not found. Using dummy values.\")\n",
    "                X_test['job_title_encoded'] = 0.5\n",
    "        \n",
    "        # Also use CatBoost encoder for job title\n",
    "        if is_training:\n",
    "            cb_encoder = CatBoostEncoder()\n",
    "            X_train['job_title_cb_encoded'] = cb_encoder.fit_transform(X_train[['job_title']], y)\n",
    "            joblib.dump(cb_encoder, f'features/job_title_cb_encoder_{timestamp}.joblib')\n",
    "        else:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if 'job_title_cb_encoder' in f])\n",
    "            if encoder_files:\n",
    "                cb_encoder = joblib.load(f'features/{encoder_files[-1]}')\n",
    "                X_test['job_title_cb_encoded'] = cb_encoder.transform(X_test[['job_title']])\n",
    "            else:\n",
    "                print(\"Warning: CatBoost encoder not found. Using dummy values.\")\n",
    "                X_test['job_title_cb_encoded'] = 0.5\n",
    "    \n",
    "    # ----- Date Feature Engineering -----\n",
    "    \n",
    "    if 'job_posted_date' in X_train.columns:\n",
    "        # Handle date format with robust error handling\n",
    "        def parse_date(date_str):\n",
    "            try:\n",
    "                if pd.isna(date_str):\n",
    "                    return pd.Timestamp('2020-01-01')\n",
    "                return pd.to_datetime(date_str, format='%Y/%m')\n",
    "            except:\n",
    "                return pd.Timestamp('2020-01-01')\n",
    "        \n",
    "        X_train['parsed_date'] = X_train['job_posted_date'].apply(parse_date)\n",
    "        X_test['parsed_date'] = X_test['job_posted_date'].apply(parse_date)\n",
    "        \n",
    "        # Extract year and month\n",
    "        X_train['job_posted_year'] = X_train['parsed_date'].dt.year\n",
    "        X_train['job_posted_month'] = X_train['parsed_date'].dt.month\n",
    "        X_test['job_posted_year'] = X_test['parsed_date'].dt.year\n",
    "        X_test['job_posted_month'] = X_test['parsed_date'].dt.month\n",
    "        \n",
    "        # Create cyclical features for month\n",
    "        X_train['month_sin'] = np.sin(2 * np.pi * X_train['job_posted_month']/12)\n",
    "        X_train['month_cos'] = np.cos(2 * np.pi * X_train['job_posted_month']/12)\n",
    "        X_test['month_sin'] = np.sin(2 * np.pi * X_test['job_posted_month']/12)\n",
    "        X_test['month_cos'] = np.cos(2 * np.pi * X_test['job_posted_month']/12)\n",
    "        \n",
    "        # Create quarter feature\n",
    "        X_train['job_quarter'] = ((X_train['job_posted_month'] - 1) // 3) + 1\n",
    "        X_test['job_quarter'] = ((X_test['job_posted_month'] - 1) // 3) + 1\n",
    "        \n",
    "        # Drop intermediate columns\n",
    "        X_train = X_train.drop(columns=['parsed_date', 'job_posted_date'])\n",
    "        X_test = X_test.drop(columns=['parsed_date', 'job_posted_date'])\n",
    "    \n",
    "    # ----- Geographic Features -----\n",
    "    \n",
    "    # Define regions\n",
    "    us_regions = {\n",
    "        'Northeast': ['ME', 'NH', 'VT', 'MA', 'RI', 'CT', 'NY', 'NJ', 'PA'],\n",
    "        'Midwest': ['OH', 'MI', 'IN', 'IL', 'WI', 'MN', 'IA', 'MO', 'ND', 'SD', 'NE', 'KS'],\n",
    "        'South': ['DE', 'MD', 'DC', 'VA', 'WV', 'NC', 'SC', 'GA', 'FL', 'KY', 'TN', 'AL', 'MS', 'AR', 'LA', 'OK', 'TX'],\n",
    "        'West': ['MT', 'ID', 'WY', 'CO', 'NM', 'AZ', 'UT', 'NV', 'CA', 'OR', 'WA', 'AK', 'HI']\n",
    "    }\n",
    "    \n",
    "    if 'job_state' in X_train.columns:\n",
    "        # Create region features\n",
    "        for region, states in us_regions.items():\n",
    "            X_train[f'region_{region}'] = X_train['job_state'].apply(lambda x: 1 if x in states else 0)\n",
    "            X_test[f'region_{region}'] = X_test['job_state'].apply(lambda x: 1 if x in states else 0)\n",
    "        \n",
    "        # One-hot encode job_state\n",
    "        if is_training:\n",
    "            state_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "            state_encoded = state_encoder.fit_transform(X_train[['job_state']])\n",
    "            joblib.dump(state_encoder, f'features/state_encoder_{timestamp}.joblib')\n",
    "        else:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if 'state_encoder' in f])\n",
    "            if encoder_files:\n",
    "                state_encoder = joblib.load(f'features/{encoder_files[-1]}')\n",
    "                state_encoded = state_encoder.transform(X_test[['job_state']])\n",
    "            else:\n",
    "                print(\"Warning: State encoder not found.\")\n",
    "                state_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "                state_encoded = state_encoder.fit_transform(X_test[['job_state']])\n",
    "        \n",
    "        state_cols = [f'state_{col}' for col in state_encoder.get_feature_names_out(['job_state'])]\n",
    "        if is_training:\n",
    "            X_train_state = pd.DataFrame(state_encoded, columns=state_cols, index=X_train.index)\n",
    "            X_train = pd.concat([X_train, X_train_state], axis=1)\n",
    "        else:\n",
    "            X_test_state = pd.DataFrame(state_encoded, columns=state_cols, index=X_test.index)\n",
    "            X_test = pd.concat([X_test, X_test_state], axis=1)\n",
    "    \n",
    "    # ----- Feature_1 Encoding -----\n",
    "    \n",
    "    if 'feature_1' in X_train.columns:\n",
    "        if is_training:\n",
    "            feat1_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "            feat1_encoded = feat1_encoder.fit_transform(X_train[['feature_1']])\n",
    "            joblib.dump(feat1_encoder, f'features/feat1_encoder_{timestamp}.joblib')\n",
    "        else:\n",
    "            encoder_files = sorted([f for f in os.listdir('features') if 'feat1_encoder' in f])\n",
    "            if encoder_files:\n",
    "                feat1_encoder = joblib.load(f'features/{encoder_files[-1]}')\n",
    "                feat1_encoded = feat1_encoder.transform(X_test[['feature_1']])\n",
    "            else:\n",
    "                print(\"Warning: Feature_1 encoder not found.\")\n",
    "                feat1_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "                feat1_encoded = feat1_encoder.fit_transform(X_test[['feature_1']])\n",
    "        \n",
    "        feat1_cols = [f'feat1_{col}' for col in feat1_encoder.get_feature_names_out(['feature_1'])]\n",
    "        if is_training:\n",
    "            X_train_feat1 = pd.DataFrame(feat1_encoded, columns=feat1_cols, index=X_train.index)\n",
    "            X_train = pd.concat([X_train, X_train_feat1], axis=1)\n",
    "        else:\n",
    "            X_test_feat1 = pd.DataFrame(feat1_encoded, columns=feat1_cols, index=X_test.index)\n",
    "            X_test = pd.concat([X_test, X_test_feat1], axis=1)\n",
    "    \n",
    "    # ----- Numerical Feature Transformations -----\n",
    "    \n",
    "    # Apply power transformer for skewed numerical features\n",
    "    if is_training:\n",
    "        for col in numerical_cols:\n",
    "            if col in X_train.columns:\n",
    "                power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "                X_train[col] = power_transformer.fit_transform(X_train[[col]])\n",
    "                joblib.dump(power_transformer, f'features/{col}_transformer_{timestamp}.joblib')\n",
    "    else:\n",
    "        for col in numerical_cols:\n",
    "            if col in X_test.columns:\n",
    "                transformer_files = sorted([f for f in os.listdir('features') if f'{col}_transformer' in f])\n",
    "                if transformer_files:\n",
    "                    power_transformer = joblib.load(f'features/{transformer_files[-1]}')\n",
    "                    X_test[col] = power_transformer.transform(X_test[[col]])\n",
    "                else:\n",
    "                    print(f\"Warning: Transformer for {col} not found.\")\n",
    "    \n",
    "    # Create polynomial features\n",
    "    for col in numerical_cols:\n",
    "        if col in X_train.columns:\n",
    "            X_train[f'{col}_squared'] = X_train[col] ** 2\n",
    "            X_train[f'{col}_cubed'] = X_train[col] ** 3\n",
    "            X_test[f'{col}_squared'] = X_test[col] ** 2\n",
    "            X_test[f'{col}_cubed'] = X_test[col] ** 3\n",
    "    \n",
    "    # Feature interactions between numerical features\n",
    "    for i, col1 in enumerate(numerical_cols):\n",
    "        for j, col2 in enumerate(numerical_cols):\n",
    "            if i < j and col1 in X_train.columns and col2 in X_train.columns:\n",
    "                X_train[f'{col1}_{col2}_interaction'] = X_train[col1] * X_train[col2]\n",
    "                X_test[f'{col1}_{col2}_interaction'] = X_test[col1] * X_test[col2]\n",
    "    \n",
    "    # ----- Boolean Feature Engineering -----\n",
    "    \n",
    "    # Sum of boolean features\n",
    "    if all(col in X_train.columns for col in boolean_cols):\n",
    "        X_train['boolean_sum'] = X_train[boolean_cols].sum(axis=1)\n",
    "        X_test['boolean_sum'] = X_test[boolean_cols].sum(axis=1)\n",
    "        \n",
    "        # Interactions between boolean features\n",
    "        for i, col1 in enumerate(boolean_cols):\n",
    "            for j, col2 in enumerate(boolean_cols):\n",
    "                if i < j:\n",
    "                    # AND interaction\n",
    "                    X_train[f'{col1}_{col2}_and'] = X_train[col1] & X_train[col2]\n",
    "                    X_test[f'{col1}_{col2}_and'] = X_test[col1] & X_test[col2]\n",
    "                    \n",
    "                    # OR interaction\n",
    "                    X_train[f'{col1}_{col2}_or'] = X_train[col1] | X_train[col2]\n",
    "                    X_test[f'{col1}_{col2}_or'] = X_test[col1] | X_test[col2]\n",
    "    \n",
    "    # ----- Job Description Feature Engineering -----\n",
    "    # Check if job_desc columns exist\n",
    "    if all(col in X_train.columns for col in job_desc_cols[:10]):  # Check first 10 columns\n",
    "        # Calculate aggregate statistics for job descriptions\n",
    "        X_train['job_desc_mean'] = X_train[job_desc_cols].mean(axis=1)\n",
    "        X_train['job_desc_std'] = X_train[job_desc_cols].std(axis=1)\n",
    "        X_train['job_desc_sum'] = X_train[job_desc_cols].sum(axis=1)\n",
    "        X_train['job_desc_min'] = X_train[job_desc_cols].min(axis=1)\n",
    "        X_train['job_desc_max'] = X_train[job_desc_cols].max(axis=1)\n",
    "        X_train['job_desc_nonzero'] = (X_train[job_desc_cols] != 0).sum(axis=1)\n",
    "        \n",
    "        X_test['job_desc_mean'] = X_test[job_desc_cols].mean(axis=1)\n",
    "        X_test['job_desc_std'] = X_test[job_desc_cols].std(axis=1)\n",
    "        X_test['job_desc_sum'] = X_test[job_desc_cols].sum(axis=1)\n",
    "        X_test['job_desc_min'] = X_test[job_desc_cols].min(axis=1)\n",
    "        X_test['job_desc_max'] = X_test[job_desc_cols].max(axis=1)\n",
    "        X_test['job_desc_nonzero'] = (X_test[job_desc_cols] != 0).sum(axis=1)\n",
    "        \n",
    "        # Advanced dimensionality reduction\n",
    "        if is_training:\n",
    "            # PCA for dense representation\n",
    "            pca = PCA(n_components=25)\n",
    "            job_desc_pca_train = pca.fit_transform(X_train[job_desc_cols])\n",
    "            joblib.dump(pca, f'features/job_desc_pca_{timestamp}.joblib')\n",
    "            \n",
    "            # SVD for sparse representation\n",
    "            svd = TruncatedSVD(n_components=25)\n",
    "            job_desc_svd_train = svd.fit_transform(X_train[job_desc_cols])\n",
    "            joblib.dump(svd, f'features/job_desc_svd_{timestamp}.joblib')\n",
    "            \n",
    "            # Add dimension reduction features to training data\n",
    "            for i in range(25):\n",
    "                X_train[f'pca_{i}'] = job_desc_pca_train[:, i]\n",
    "                X_train[f'svd_{i}'] = job_desc_svd_train[:, i]\n",
    "                \n",
    "            # Clustering on job descriptions\n",
    "            kmeans = KMeans(n_clusters=15, random_state=42, n_init=10)\n",
    "            X_train['job_desc_cluster'] = kmeans.fit_predict(job_desc_pca_train)\n",
    "            joblib.dump(kmeans, f'features/job_desc_kmeans_{timestamp}.joblib')\n",
    "            \n",
    "            # Create one-hot encoding for clusters\n",
    "            cluster_encoder = OneHotEncoder(sparse_output=False)\n",
    "            cluster_encoded_train = cluster_encoder.fit_transform(X_train[['job_desc_cluster']])\n",
    "            joblib.dump(cluster_encoder, f'features/cluster_encoder_{timestamp}.joblib')\n",
    "            \n",
    "            cluster_cols = [f'cluster_{i}' for i in range(cluster_encoded_train.shape[1])]\n",
    "            X_train_clusters = pd.DataFrame(cluster_encoded_train, columns=cluster_cols, index=X_train.index)\n",
    "            X_train = pd.concat([X_train, X_train_clusters], axis=1)\n",
    "            \n",
    "        else:\n",
    "            # Apply transformations to test data separately\n",
    "            pca_files = sorted([f for f in os.listdir('features') if 'job_desc_pca' in f])\n",
    "            svd_files = sorted([f for f in os.listdir('features') if 'job_desc_svd' in f])\n",
    "            kmeans_files = sorted([f for f in os.listdir('features') if 'job_desc_kmeans' in f])\n",
    "            cluster_files = sorted([f for f in os.listdir('features') if 'cluster_encoder' in f])\n",
    "            \n",
    "            if pca_files:\n",
    "                pca = joblib.load(f'features/{pca_files[-1]}')\n",
    "                job_desc_pca_test = pca.transform(X_test[job_desc_cols])\n",
    "            else:\n",
    "                print(\"Warning: PCA model not found.\")\n",
    "                job_desc_pca_test = np.zeros((X_test.shape[0], 25))\n",
    "            \n",
    "            if svd_files:\n",
    "                svd = joblib.load(f'features/{svd_files[-1]}')\n",
    "                job_desc_svd_test = svd.transform(X_test[job_desc_cols])\n",
    "            else:\n",
    "                print(\"Warning: SVD model not found.\")\n",
    "                job_desc_svd_test = np.zeros((X_test.shape[0], 25))\n",
    "            \n",
    "            # Add dimension reduction features to test data\n",
    "            for i in range(25):\n",
    "                X_test[f'pca_{i}'] = job_desc_pca_test[:, i]\n",
    "                X_test[f'svd_{i}'] = job_desc_svd_test[:, i]\n",
    "            \n",
    "            # Apply clustering to test data\n",
    "            if kmeans_files:\n",
    "                kmeans = joblib.load(f'features/{kmeans_files[-1]}')\n",
    "                X_test['job_desc_cluster'] = kmeans.predict(job_desc_pca_test)\n",
    "            else:\n",
    "                print(\"Warning: KMeans model not found.\")\n",
    "                X_test['job_desc_cluster'] = 0\n",
    "            \n",
    "            # Apply one-hot encoding to test clusters\n",
    "            if cluster_files:\n",
    "                cluster_encoder = joblib.load(f'features/{cluster_files[-1]}')\n",
    "                cluster_encoded_test = cluster_encoder.transform(X_test[['job_desc_cluster']])\n",
    "                \n",
    "                cluster_cols = [f'cluster_{i}' for i in range(cluster_encoded_test.shape[1])]\n",
    "                X_test_clusters = pd.DataFrame(cluster_encoded_test, columns=cluster_cols, index=X_test.index)\n",
    "                X_test = pd.concat([X_test, X_test_clusters], axis=1)\n",
    "        \n",
    "        # Drop original job description columns to reduce dimensionality\n",
    "        X_train = X_train.drop(columns=job_desc_cols)\n",
    "        X_test = X_test.drop(columns=job_desc_cols)\n",
    "\n",
    "    \n",
    "    # Drop original categorical columns now that they're encoded\n",
    "    cols_to_drop = ['job_title', 'job_state', 'feature_1']\n",
    "    X_train = X_train.drop(columns=[c for c in cols_to_drop if c in X_train.columns])\n",
    "    X_test = X_test.drop(columns=[c for c in cols_to_drop if c in X_test.columns])\n",
    "    \n",
    "    # Ensure all columns in test exist in train\n",
    "    for col in X_train.columns:\n",
    "        if col not in X_test.columns:\n",
    "            X_test[col] = 0\n",
    "    \n",
    "    # Ensure all columns in train exist in test\n",
    "    for col in X_test.columns:\n",
    "        if col not in X_train.columns:\n",
    "            X_train[col] = 0\n",
    "    \n",
    "    # Make sure columns are in the same order\n",
    "    X_test = X_test[X_train.columns]\n",
    "    \n",
    "    # Save feature data\n",
    "    if is_training:\n",
    "        X_train.to_csv(f'features/X_train_features_{timestamp}.csv', index=False)\n",
    "        if y is not None:\n",
    "            pd.Series(y).to_csv(f'features/y_train_{timestamp}.csv', index=False)\n",
    "    \n",
    "    # Return prepared datasets\n",
    "    if is_training:\n",
    "        return X_train, X_test, y, test_obs\n",
    "    else:\n",
    "        return X_train, X_test, None, test_obs\n",
    "\n",
    "# ----------------- Step 3: Advanced Model Building -----------------\n",
    "\n",
    "def optimize_models(X, y, timestamp):\n",
    "    \"\"\"Optimize multiple models with Optuna and proper cross-validation.\"\"\"\n",
    "    \n",
    "    # Initialize stratified k-fold\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    best_models = {}\n",
    "    \n",
    "    # Define models to optimize\n",
    "    models_to_optimize = {\n",
    "        'XGBoost': optimize_xgboost,\n",
    "        'CatBoost': optimize_catboost\n",
    "    }\n",
    "    \n",
    "    # Run optimization for each model\n",
    "    for model_name, optimizer_func in models_to_optimize.items():\n",
    "        print(f\"\\nOptimizing {model_name}...\")\n",
    "        best_model = optimizer_func(X, y, cv)\n",
    "        best_models[model_name] = best_model\n",
    "        \n",
    "        # Save the optimized model\n",
    "        joblib.dump(best_model, f'models/{model_name}_optimized_{timestamp}.joblib')\n",
    "        \n",
    "        # Quick validation on full dataset\n",
    "        cv_scores = cross_val_score(best_model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        print(f\"{model_name} CV Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        \n",
    "        # Save model performance\n",
    "        with open(f'results/model_performance_{timestamp}.txt', 'a') as f:\n",
    "            f.write(f\"{model_name} CV Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\\n\")\n",
    "    \n",
    "    return best_models\n",
    "\n",
    "def optimize_xgboost(X, y, cv):\n",
    "    \"\"\"Optimize XGBoost hyperparameters with Optuna.\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        model = XGBClassifier(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    print(f\"Best XGBoost Params: {study.best_params}\")\n",
    "    print(f\"Best XGBoost CV Accuracy: {study.best_value:.4f}\")\n",
    "    \n",
    "    return XGBClassifier(**study.best_params, n_jobs=-1)\n",
    "\n",
    "def optimize_catboost(X, y, cv):\n",
    "    \"\"\"Optimize CatBoost hyperparameters with Optuna.\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "            'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "            'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 10.0),\n",
    "            'random_seed': 42,\n",
    "            'thread_count': -1,\n",
    "            'verbose': False\n",
    "        }\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=30)  # Fewer trials since CatBoost is slower\n",
    "    print(f\"Best CatBoost Params: {study.best_params}\")\n",
    "    print(f\"Best CatBoost CV Accuracy: {study.best_value:.4f}\")\n",
    "    \n",
    "    return CatBoostClassifier(**study.best_params, thread_count=-1, verbose=False)\n",
    "\n",
    "def build_ensemble(best_models, X, y, timestamp):\n",
    "    \"\"\"Build advanced ensemble models using optimized base models.\"\"\"\n",
    "    \n",
    "    # Create a copy of best models to avoid modification\n",
    "    models = {name: model for name, model in best_models.items()}\n",
    "    \n",
    "    # Add additional models for diversity\n",
    "    models['RandomForest'] = RandomForestClassifier(\n",
    "        n_estimators=500, max_depth=None, min_samples_split=2, max_features='sqrt', \n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    models['GradientBoosting'] = GradientBoostingClassifier(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=6, subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train additional models\n",
    "    for name in ['RandomForest', 'GradientBoosting']:\n",
    "        models[name].fit(X, y)\n",
    "        joblib.dump(models[name], f'models/{name}_model_{timestamp}.joblib')\n",
    "    \n",
    "    # Create voting ensemble - fix naming conflict\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', models['RandomForest']),\n",
    "            ('gb', models['GradientBoosting']),\n",
    "            ('xgb', models['XGBoost']),\n",
    "            ('catb', models['CatBoost'])\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_clf.fit(X, y)\n",
    "    joblib.dump(voting_clf, f'models/voting_ensemble_{timestamp}.joblib')\n",
    "    \n",
    "    # Create stacking ensemble\n",
    "    stacking_estimators = [\n",
    "        ('rf', models['RandomForest']),\n",
    "        ('gb', models['GradientBoosting']),\n",
    "        ('xgb', models['XGBoost']),\n",
    "        ('catb', models['CatBoost'])\n",
    "    ]\n",
    "    \n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=stacking_estimators,\n",
    "        final_estimator=RandomForestClassifier(random_state=42),\n",
    "        cv=5\n",
    "    )\n",
    "    stacking_clf.fit(X, y)\n",
    "    joblib.dump(stacking_clf, f'models/stacking_ensemble_{timestamp}.joblib')\n",
    "    \n",
    "    # Create a meta-ensemble combining models - fix naming conflict\n",
    "    final_ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('vote', voting_clf),\n",
    "            ('stack', stacking_clf),\n",
    "            ('xgb', models['XGBoost']),\n",
    "            ('catb', models['CatBoost'])\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    final_ensemble.fit(X, y)\n",
    "    \n",
    "    # Save the final ensemble model\n",
    "    joblib.dump(final_ensemble, f'models/final_ensemble_{timestamp}.joblib')\n",
    "    \n",
    "    return final_ensemble\n",
    "\n",
    "# ----------------- Step 4: Training and Prediction -----------------\n",
    "\n",
    "def run_training_pipeline():\n",
    "    \"\"\"Run the complete training pipeline with proper file management\"\"\"\n",
    "    \n",
    "    # Create timestamp for unique filenames\n",
    "    timestamp = get_timestamp()\n",
    "    print(f\"Run timestamp: {timestamp}\")\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    create_directories()\n",
    "    \n",
    "    print(\"\\n=== Starting Training Pipeline ===\\n\")\n",
    "    \n",
    "    # Create a log file for this run\n",
    "    with open(f'results/training_log_{timestamp}.txt', 'w') as log_file:\n",
    "        log_file.write(f\"Training Run: {timestamp}\\n\\n\")\n",
    "        log_file.write(\"=== Starting Training Pipeline ===\\n\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    train_df, test_df = load_data()\n",
    "    \n",
    "    # Feature engineering\n",
    "    print(\"\\n=== Applying Feature Engineering ===\\n\")\n",
    "    X_train, X_test, y_train, test_obs = engineer_features(train_df, test_df, timestamp)\n",
    "    \n",
    "    print(f\"Final training data shape: {X_train.shape}\")\n",
    "    print(f\"Final test data shape: {X_test.shape}\")\n",
    "    \n",
    "    # Save shapes to log\n",
    "    with open(f'results/training_log_{timestamp}.txt', 'a') as log_file:\n",
    "        log_file.write(f\"Training data shape: {X_train.shape}\\n\")\n",
    "        log_file.write(f\"Test data shape: {X_test.shape}\\n\\n\")\n",
    "    \n",
    "    # Handle class imbalance with advanced resampling\n",
    "    print(\"\\n=== Handling Class Imbalance ===\\n\")\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"Class distribution after resampling: {np.bincount(y_resampled)}\")\n",
    "    \n",
    "    # Save resampling info to log\n",
    "    with open(f'results/training_log_{timestamp}.txt', 'a') as log_file:\n",
    "        log_file.write(f\"Class distribution after resampling: {np.bincount(y_resampled)}\\n\\n\")\n",
    "    \n",
    "    # Optimize models\n",
    "    print(\"\\n=== Optimizing Base Models ===\\n\")\n",
    "    best_models = optimize_models(X_resampled, y_resampled, timestamp)\n",
    "    \n",
    "    # Build ensemble\n",
    "    print(\"\\n=== Building Ensemble Model ===\\n\")\n",
    "    final_model = build_ensemble(best_models, X_resampled, y_resampled, timestamp)\n",
    "    \n",
    "    # Save model for future use\n",
    "    joblib.dump(final_model, f'models/final_salary_model_{timestamp}.joblib')\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"\\n=== Generating Predictions ===\\n\")\n",
    "    predictions = final_model.predict(X_test)\n",
    "    \n",
    "    # Map predictions back to categories\n",
    "    reverse_mapping = {2: 'High', 1: 'Medium', 0: 'Low'}\n",
    "    predictions_labels = np.array([reverse_mapping[p] for p in predictions])\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'obs': test_obs,\n",
    "        'salary_category': predictions_labels\n",
    "    })\n",
    "    \n",
    "    # Save submission\n",
    "    submission_path = f'submissions/solution_format_{timestamp}.csv'\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "    \n",
    "    # Report prediction distribution\n",
    "    print(\"\\n=== Prediction Distribution ===\\n\")\n",
    "    pred_distribution = pd.Series(predictions_labels).value_counts()\n",
    "    print(pred_distribution)\n",
    "    \n",
    "    # Save results to log\n",
    "    with open(f'results/training_log_{timestamp}.txt', 'a') as log_file:\n",
    "        log_file.write(\"=== Prediction Distribution ===\\n\")\n",
    "        log_file.write(f\"{pred_distribution.to_string()}\\n\\n\")\n",
    "        log_file.write(f\"Submission saved to {submission_path}\\n\")\n",
    "    \n",
    "    # Create a symlink to the latest model\n",
    "    latest_model_path = 'models/latest_model.joblib'\n",
    "    if os.path.exists(latest_model_path):\n",
    "        os.remove(latest_model_path)\n",
    "    if os.name == 'nt':  # Windows\n",
    "        # Windows doesn't support symbolic links easily, so we'll copy the file\n",
    "        import shutil\n",
    "        shutil.copy2(f'models/final_salary_model_{timestamp}.joblib', latest_model_path)\n",
    "    else:\n",
    "        # For Unix-based systems\n",
    "        os.symlink(f'final_salary_model_{timestamp}.joblib', latest_model_path)\n",
    "    \n",
    "    print(\"\\n=== Training Pipeline Complete ===\\n\")\n",
    "    return final_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
